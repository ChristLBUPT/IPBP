{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "using modified version of Apple OpenELM\n"
     ]
    }
   ],
   "source": [
    "#papermill_description=IMPORT_DEPENDENCIES\n",
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "%load_ext autoreload\n",
    "%load_ext line_profiler\n",
    "%autoreload 0\n",
    "# %autoreload 3\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import LoraConfig, get_peft_model, LoraModel\n",
    "from modules import OpenELMModel\n",
    "from modules import LlamaTokenizer, LlamaModel, BertModel\n",
    "from dep_model import DependencyParser, Hparam\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from gpustat import GPUStatCollection\n",
    "from tqdm import tqdm, trange\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Literal, Tuple, List, Union, Any\n",
    "import os\n",
    "from os import path as osp\n",
    "import json\n",
    "import math\n",
    "from matplotlib import (\n",
    "    pyplot as plt,\n",
    "    markers\n",
    ")\n",
    "import numpy as np\n",
    "from subprocess import Popen, PIPE\n",
    "import pickle as pkl\n",
    "from util import eisner\n",
    "import re\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Extract Attention Features\n",
    "### 1. Load Models and Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "LLAMA_MODEL_NAME: str = 'open_llama_7b'\n",
    "TRAIN_DATA_DIR: str = './data/train.conllu'\n",
    "DATA_PROPORTION = 1\n",
    "TRANSPOSE = False # !!remember to change the cache file name `CACHE_DIR` if you change this option!!\n",
    "CACHE_DIR = '<specify cache dir>'\n",
    "FORCE_RECACHE = False # pos, height and label attention samples are cached in '${HOME}/attn-sample-cache.pt' to accelerate reading\n",
    "# default behavior is directly jump to dependency tree reconstruction (assume attn_samples, kde_estims and mi_estims are already saved to disk)\n",
    "LOAD_ATTN_SAMPLES = True # when set to True, will skip extracting attention weights and load from disk\n",
    "LOAD_KDE_ESTIMS = True # when set to True, will skip calculating KDE estimations (f(a_i|y)) and load from disk\n",
    "LOAD_MI = True # when set to True, will skip calculating MI estimations and load from disk\n",
    "BREAK_AFTER_EXTRACT = False # when set to True, will only extract attention weights and save to disk, and then interrupt\n",
    "BREAK_AFTER_KDE = False # when set to True, after calculating KDE estimations(f(a_i|t=t_i)) and save to disk, and then interrupt\n",
    "BREAK_AFTER_MI = False # when set to True, after calculating MI estimations and save to disk, and then interrupt\n",
    "SAVE_DURING_EXTRACTING = True\n",
    "USE_MEMMAP = False # when extracting, save negative samples to disk using memmap\n",
    "METADATA_ONLY = False # when extracting, don't save anything except matadata\n",
    "SAVE_EVERY = 1024 # this option will only be valid if SAVE_DURING_EXTRACTING or USE_MEMMAP is set to True\n",
    "KDE_NUM_SLICES = 8\n",
    "LOAD_BASELINES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_ATTN_SAMPLES and SAVE_DURING_EXTRACTING and not BREAK_AFTER_EXTRACT:\n",
    "    raise NotImplementedError(f'Error, if you process attention sample gathering and set `SAVE_DURING_EXTRACTING` to True, BREAK_AFTER_EXTRACT must be set to True, '\n",
    "                              '\\nand next time, execute with LOAD_ATTN_SAMPLES=True, '\n",
    "                              'since the attention samples are saved during extracting and not in memory.')\n",
    "assert int(SAVE_DURING_EXTRACTING) + int(METADATA_ONLY) + int(USE_MEMMAP) <= 1, 'Error, only one of `SAVE_DURING_EXTRACTING`, `METADATA_ONLY` and `USE_MEMMAP` can be set to True.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=LOAD_MODEL\n",
    "# warning: these code are dependent on the environment, please modify them according to your own environment\n",
    "pretrained_path = '../pretrained-models/'\n",
    "LLAMA_PATH = osp.join(pretrained_path, LLAMA_MODEL_NAME)\n",
    "print(f'initializing LLAMA tokenizer and model from `{LLAMA_PATH}`...')\n",
    "# config_dir_name = 'openllama' if 'llama' in LLAMA_MODEL_NAME else 'bert' if 'bert' in LLAMA_MODEL_NAME \n",
    "if 'llama' in LLAMA_MODEL_NAME:\n",
    "    config_dir_name = 'openllama'\n",
    "elif 'bert' in LLAMA_MODEL_NAME:\n",
    "    config_dir_name = 'bert'\n",
    "else:\n",
    "    raise ValueError(f'Error, invalid model name {LLAMA_MODEL_NAME} to choose from `openllama` or `bert`')\n",
    "dep_parser_llama = DependencyParser(\n",
    "    json.load(open(f\"./configs/{config_dir_name}/sample_labelmap.json\")), \n",
    "    Hparam(**json.load(open(f\"./configs/{config_dir_name}/sample_hpara.json\"))), \n",
    "    LLAMA_PATH, device_to_place='cuda:0'\n",
    ") \n",
    "model_llama, tok_llama = dep_parser_llama.llm if dep_parser_llama.llm is not None else dep_parser_llama.bert, dep_parser_llama.tokenizer\n",
    "print(model_llama.device, model_llama.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract Attentions over the Dataset and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=LOAD_DATA\n",
    "# data_instances = readfile('./data/train.conllu', 'train')\n",
    "# lens = [len(each[0]) for each in data_instances[:10]]\n",
    "train_data_llama = dep_parser_llama.load_data(TRAIN_DATA_DIR)\n",
    "train_data_llama = train_data_llama[:round(len(train_data_llama) * DATA_PROPORTION)]\n",
    "dl_llama = DataLoader(train_data_llama, collate_fn=lambda x: dep_parser_llama.feature2input('cuda:0', dep_parser_llama.convert_examples_to_features(x)))\n",
    "def formalize_pickle_file_name(original_save_name: str, save_idx: int = None, num_saves: int = None):\n",
    "    file_name, file_ext = original_save_name.rsplit('.', maxsplit=1)\n",
    "    file_name = f\"{file_name}_{str(DATA_PROPORTION).replace('.', '_')}{'_transpose' if TRANSPOSE else ''}\"\n",
    "    if save_idx is not None and num_saves is not None:\n",
    "        # pad save_idx with 0, making it have the save length as num_saves\n",
    "        file_name = f\"{file_name}_{str(save_idx).zfill(len(str(num_saves)))}of{num_saves}\"\n",
    "    return f'{file_name}.{file_ext}'\n",
    "\n",
    "def save_attn_samples(samples, path: str):\n",
    "    print(f'saving to {path}...')\n",
    "    torch.save(samples, path)\n",
    "\n",
    "if 'dev' in TRAIN_DATA_DIR:\n",
    "    kde_save_pth = osp.join(osp.split(osp.realpath(LLAMA_PATH))[0], 'kde', osp.split(LLAMA_PATH)[1] + '_dev',)\n",
    "else:\n",
    "    kde_save_pth = osp.join(osp.split(osp.realpath(LLAMA_PATH))[0], 'kde', osp.split(LLAMA_PATH)[1],)\n",
    "os.makedirs(kde_save_pth, exist_ok=True)\n",
    "print(f'When running, attention samples will be saved to `{kde_save_pth}`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=EXTRACT_ATTN_FEATURES\n",
    "from explaination import adj_matrix_to_heights, get_shape, print_listlike, FeatureList, concatenate_attn_features\n",
    "\n",
    "# test model output dim\n",
    "def extract_attn_features(\n",
    "    tok: Union[LlamaTokenizer, BertTokenizer], \n",
    "    model: Union[LlamaModel, BertModel], \n",
    "    dataloader: DataLoader, \n",
    "    labelmap: Dict[str, int], \n",
    "    num_samples: int = 100, \n",
    "    silent: bool = True, \n",
    "    transpose: bool = False, \n",
    "    save_during_extracting: bool = False,\n",
    "    save_every: int = -1,\n",
    "    metadata_only: bool = False,\n",
    "    use_memmap: bool = False,\n",
    ") -> Tuple[\n",
    "        Tuple[Union[List[Tensor], FeatureList], Union[List[Tensor], FeatureList]], Dict[str, Union[List[Tensor], FeatureList]]\n",
    "    ]:\n",
    "    \"\"\"\n",
    "    returns \n",
    "        (\n",
    "            pos_features: List[Tensor[n_arcs_of_sample_i, feature_dim(n_layers * n_heads)]], \n",
    "            neg_features: List[Tensor[sample_i_seq_len ^ 2 - n_arcs_of_sample_i, feature_dim]]\n",
    "        ), label_features: Dict[str(label_name of rel j), List[Tensor[samp_i_n_rel_j_arcs, feature_dim]]]\n",
    "    \"\"\"\n",
    "    tok.pad_token = tok.eos_token\n",
    "    torch.backends.cuda.enable_flash_sdp(True)\n",
    "    torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "    # if concat:\n",
    "    #     pos_features, neg_features = FeatureList(), FeatureList()\n",
    "    #     height_features, label_features = defaultdict(FeatureList), defaultdict(FeatureList)\n",
    "    # else:\n",
    "    pos_features, neg_features = [], []\n",
    "    height_features = defaultdict(list)\n",
    "    label_features = defaultdict(list)\n",
    "    min_attn_score, max_attn_score = 1145141919810, -1145141919810\n",
    "    if num_samples == -1:\n",
    "        num_samples = len(dataloader)\n",
    "    if save_during_extracting:\n",
    "        save_idx = 0\n",
    "        num_saves = math.ceil(num_samples / save_every)\n",
    "        n_pos_samples, n_neg_samples = 0, 0\n",
    "    if metadata_only:\n",
    "        n_pos_samples, n_neg_samples = 0, 0\n",
    "    if use_memmap:\n",
    "        # load metadata\n",
    "        with open(osp.join(kde_save_pth, formalize_pickle_file_name('metadata.json'))) as f:\n",
    "            metadata = json.load(f)\n",
    "            n_pos_samples, n_neg_samples = metadata['n_pos_samples'], metadata['n_neg_samples']\n",
    "        # initialize memmap for neg_features_concat\n",
    "        neg_features_shape=(n_neg_samples, model_llama.config.num_hidden_layers * model_llama.config.num_attention_heads)\n",
    "        print(f'creating memmap file (shape: {neg_features_shape})...')\n",
    "        neg_features_memmap = np.memmap(osp.join(kde_save_pth, formalize_pickle_file_name('neg_attn_samples.pkl')), dtype='float32', mode='w+', shape=neg_features_shape)\n",
    "        neg_features_memmap_ptr = 0\n",
    "    \n",
    "    id2label = {label_idx: label_name for label_name, label_idx in labelmap.items()}\n",
    "    for data_idx, (input_ids, input_attention_mask, label_mask, eval_mask, \\\n",
    "        arcs, rels, word_ids, pos_ids, ngram_ids, \\\n",
    "        ngram_positions, segment_ids, valid_ids) in enumerate(tqdm(dataloader, total=num_samples, desc='extracting attentions', ncols=100)):\n",
    "\n",
    "        echo = not silent and data_idx == 0\n",
    "\n",
    "        if echo:\n",
    "            print(input_ids.shape, label_mask.shape, valid_ids.shape, arcs.shape, rels.shape)\n",
    "        B, S = input_ids.shape\n",
    "        w2s = [] # the idx at position i is whole-word i's last subword's idx\n",
    "        for subword_idx, each in enumerate(valid_ids[0].tolist()):\n",
    "            if each == 1:\n",
    "                w2s.append(subword_idx)\n",
    "\n",
    "        arc_adj_matrix = torch.zeros(S, S).to('cuda:0') # 1 for `have arc`, 0 for `no arc` (marking at whole-word's last subword)\n",
    "        label_adj_matrix = torch.zeros(S, S).to('cuda:0') # arc[i][j]'s relation type (marking at whole-word's last subword)\n",
    "        # print_listlike([*enumerate(tok.convert_ids_to_tokens(input_ids[0]))])\n",
    "        for word_idx, head_idx in enumerate(arcs[0]):\n",
    "            if head_idx != -1:\n",
    "                arc_adj_matrix[w2s[word_idx]][w2s[head_idx]] = 1\n",
    "                label_adj_matrix[w2s[word_idx]][w2s[head_idx]] = rels[0][word_idx]\n",
    "                # print(input_ids[0][w2s[word_idx]], input_ids[0][w2s[head_idx]])\n",
    "                dependant_pos_id, head_pos_id = w2s[word_idx], w2s[head_idx]\n",
    "                dependant_token_id, head_token_id = input_ids[0][dependant_pos_id].item(), input_ids[0][head_pos_id].item()\n",
    "                # print(f\"[{dependant_pos_id}]{tok.convert_ids_to_tokens(dependant_token_id)} -> [{head_pos_id}]{tok.convert_ids_to_tokens(head_token_id)} ({id2label[rels[0][word_idx].item()]})\")\n",
    "                # print([*enumerate(tok_llama.convert_ids_to_tokens(input_ids[0]))])\n",
    "\n",
    "        height_adj_matrix = adj_matrix_to_heights(arc_adj_matrix)\n",
    "\n",
    "        if metadata_only:\n",
    "            n_pos_samples += arc_adj_matrix.sum().item()\n",
    "            n_neg_samples += (1 - arc_adj_matrix).sum().item()\n",
    "            if data_idx >= num_samples - 1:\n",
    "                break\n",
    "\n",
    "            continue\n",
    "\n",
    "        if transpose:\n",
    "            arc_adj_matrix, height_adj_matrix, label_adj_matrix = arc_adj_matrix.T, height_adj_matrix.T, label_adj_matrix.T\n",
    "        if isinstance(model, LlamaModel):\n",
    "            res = model.forward(\n",
    "                input_ids=input_ids, attention_mask=input_attention_mask,\n",
    "                output_hidden_states=True,\n",
    "                output_attentions=False,\n",
    "                output_attention_queries=True\n",
    "            )\n",
    "            key_values, queries = res.past_key_values, res.queries\n",
    "            # kv: [num_layers, 2(k and v), batch_size, num_heads, sequence_length, head_dim], q: [num_layers, batch_size, num_heads, sequence_length, head_dim]\n",
    "            if echo:\n",
    "                print('past_key_values shape:', get_shape(res.past_key_values))\n",
    "                # print(key_values)\n",
    "                print('queries shape:', get_shape(res.queries))\n",
    "                # print(queries)\n",
    "\n",
    "            attn_scores = () # [num_layers, 1(batch_size), num_heads, seq_len, seq_len]\n",
    "            for layer_idx in range(len(model.layers)):\n",
    "                k = key_values[layer_idx][0]\n",
    "                q = queries[layer_idx]\n",
    "                if q.shape[-3] != k.shape[-3]: # num_q_heads == num_k_heads * n_groups\n",
    "                    n_groups = q.shape[-3] / k.shape[-3]\n",
    "                    assert n_groups == int(n_groups)\n",
    "                    n_groups = int(n_groups)\n",
    "                else:\n",
    "                    n_groups = 1\n",
    "                \n",
    "                k = torch.repeat_interleave(k, n_groups, -3)\n",
    "                this_attn_score = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(res.last_hidden_state.shape[-1]) \n",
    "                # scaled dot-product attention, this_attn_score: [batch_size(1), num_heads, sequence_length, sequence_length]\n",
    "                attn_scores += (this_attn_score,)\n",
    "            \n",
    "            if echo:\n",
    "                print('attn_scores shape:', get_shape(attn_scores))\n",
    "            attn_scores = torch.cat(attn_scores, dim=1).squeeze(0) # [num_heads * num_layers, sequence_length, sequence_length], batch_size(1) is squeezed\n",
    "        else:\n",
    "            res = model.forward( \n",
    "                input_ids=input_ids, attention_mask=input_attention_mask,\n",
    "                output_raw_attentions=True\n",
    "            )\n",
    "            attn_scores = torch.cat(res.attentions) # cat along the first dim (originally for batch size, now is `1`), resulting in [num_layers, num_heads, sequence_length, sequence_length]\n",
    "            L, H, S, S = attn_scores.shape\n",
    "            attn_scores = attn_scores.reshape(L * H, S, S)\n",
    "            \n",
    "        if echo:\n",
    "            print('attn_score after squeezing:', get_shape(attn_scores))\n",
    "        attn_features = attn_scores.permute(1, 2, 0) # [sequence_length, sequence_length, num_heads * num_layers]\n",
    "        # if use_memmap:\n",
    "        if attn_features.min() < min_attn_score:\n",
    "            min_attn_score = attn_features.min().item()\n",
    "        if attn_features.max() > max_attn_score:\n",
    "            max_attn_score = attn_features.max().item()\n",
    "        # add flatten attention features\n",
    "        pos_features.append(attn_features[(arc_adj_matrix == 1).cpu()].cpu()) \n",
    "        # [[sentence1_num_arcs, num_heads * num_layers], [sentence2_num_arcs, num_heads * num_layers], ... ]\n",
    "        neg_features.append(attn_features[(arc_adj_matrix == 0).cpu()].cpu()) \n",
    "        # [[sent1_seq_len * sent1_seq_len - sent1_num_arcs, num_heads * num_layers], [sent2_seq_len * sent2_seq_len - sent2_num_arcs, num_heads * num_layers]]\n",
    "        for label_idx in labelmap.values():\n",
    "            label_features[label_idx].append(attn_features[(label_adj_matrix == label_idx).cpu()].cpu())\n",
    "            # {rel1_label_name: [[samp1_num_rel1_samples, num_heads * num_layers]], [[samp2_num_rel1_samples, num_heads * num_layers]]}\n",
    "        \n",
    "        for height in range(1, int(height_adj_matrix.max().item() + 1)):\n",
    "            height_features[height].append(attn_features[(height_adj_matrix == height).cpu()].cpu())\n",
    "\n",
    "        # save during\n",
    "        if save_during_extracting and (data_idx != 0 and data_idx % save_every == 0 or data_idx == num_samples - 1):\n",
    "            print(f'saving attention samples at {data_idx} (save idx {save_idx})...')\n",
    "            (pos_features_concat, neg_features_concat), height_features_concat, label_features_concat, label_ids = concatenate_attn_features(\n",
    "                (pos_features, neg_features), height_features, label_features, concat_across_labels=False)\n",
    "\n",
    "            save_attn_samples(pos_features_concat, osp.join(kde_save_pth, formalize_pickle_file_name(f'pos_attn_samples.pkl', save_idx, num_saves)))\n",
    "            save_attn_samples(neg_features_concat, osp.join(kde_save_pth, formalize_pickle_file_name('neg_attn_samples.pkl', save_idx, num_saves)))\n",
    "            save_attn_samples(height_features_concat, osp.join(kde_save_pth, formalize_pickle_file_name('height_attn_samples.pkl', save_idx, num_saves)))\n",
    "            save_attn_samples(label_features_concat, osp.join(kde_save_pth, formalize_pickle_file_name('label_attn_samples.pkl', save_idx, num_saves)))\n",
    "            n_pos_samples += len(pos_features_concat)\n",
    "            n_neg_samples += len(neg_features_concat)\n",
    "            pos_features, neg_features = [], []\n",
    "            height_features = defaultdict(list)\n",
    "            label_features = defaultdict(list)\n",
    "            save_idx += 1\n",
    "        \n",
    "        if use_memmap and (data_idx != 0 and data_idx % save_every == 0 or data_idx == num_samples - 1):\n",
    "            neg_features_concat = torch.cat(neg_features, dim=0)\n",
    "            neg_features_memmap[neg_features_memmap_ptr: neg_features_memmap_ptr + neg_features_concat.shape[0]] = neg_features_concat.float().numpy()\n",
    "            print(f\"writing to memmap from {neg_features_memmap_ptr} to {neg_features_memmap_ptr + neg_features_concat.shape[0]}\")\n",
    "            neg_features_memmap_ptr += neg_features_concat.shape[0]\n",
    "            neg_features = []\n",
    "\n",
    "        if data_idx >= num_samples - 1:\n",
    "            break\n",
    "    # end for data_idx, (input_ids, input_attention_mask, label_mask, eval_mask, arcs, rels, word_ids, pos_ids, ngram_ids, ngram_positions, segment_ids, valid_ids) in enumerate(dataloader)\n",
    "\n",
    "    if save_during_extracting:\n",
    "        with open(osp.join(kde_save_pth, 'labelmap.json'), 'w') as f:\n",
    "            json.dump(labelmap, f)\n",
    "        with open(osp.join(kde_save_pth, formalize_pickle_file_name('metadata.json')), 'w') as f:\n",
    "            json.dump({\n",
    "                'n_pos_samples': n_pos_samples,\n",
    "                'n_neg_samples': n_neg_samples,\n",
    "                'min_attn_score': min_attn_score,\n",
    "                'max_attn_score': max_attn_score,\n",
    "            }, f)\n",
    "        # with open(osp.join(kde_save_pth, 'label_id2name.json'), 'w') as f:\n",
    "        #     json.dump({label_id: label_name for label_name, label_id in dep_parser_llama.labelmap.items()}, f)\n",
    "        label_id2name = {label_id: label_name for label_name, label_id in labelmap.items()}\n",
    "        with open(osp.join(kde_save_pth, 'label_names.txt'), 'w') as f:\n",
    "            label_names = [label_id2name[label_id] for label_id in labelmap.values()]\n",
    "            json.dump(label_names, f)\n",
    "    \n",
    "    if metadata_only:\n",
    "        with open(osp.join(kde_save_pth, formalize_pickle_file_name('metadata.json')), 'w') as f:\n",
    "            json.dump({\n",
    "                'n_pos_samples': int(n_pos_samples),\n",
    "                'n_neg_samples': int(n_neg_samples),\n",
    "            }, f)\n",
    "    \n",
    "    if use_memmap:\n",
    "        neg_features_memmap.flush()\n",
    "        with open(osp.join(kde_save_pth, formalize_pickle_file_name('metadata.json')), 'w') as f:\n",
    "            json.dump({\n",
    "                'n_pos_samples': n_pos_samples,\n",
    "                'n_neg_samples': n_neg_samples,\n",
    "                'min_attn_score': min_attn_score,\n",
    "                'max_attn_score': max_attn_score,\n",
    "            }, f)\n",
    "\n",
    "    # if save_during_extracting is set to True, the return value will be empty lists and dicts, if not, return the UNconcatenated (chunked by sentence) features\n",
    "    return (pos_features, neg_features), height_features, label_features\n",
    "\n",
    "if not LOAD_ATTN_SAMPLES:\n",
    "    with torch.no_grad():\n",
    "        arc_attn_features, height_attn_features, label_attn_features = extract_attn_features(\n",
    "            tok=tok_llama, model=model_llama, dataloader=dl_llama, labelmap=dep_parser_llama.labelmap, num_samples=-1, silent=False, transpose=TRANSPOSE, save_during_extracting=SAVE_DURING_EXTRACTING, save_every=SAVE_EVERY, metadata_only=METADATA_ONLY, use_memmap=USE_MEMMAP\n",
    "        )\n",
    "# result_test(tok_llama, model_llama, dl_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_ATTN_SAMPLES and not SAVE_DURING_EXTRACTING:\n",
    "    print(sum([each.shape[0] for each in arc_attn_features[0]]), sum([each.shape[0] for each in arc_attn_features[1]]))\n",
    "    print(sum([sum([each.shape[0] for each in height_features]) for height_features in height_attn_features.values()]))\n",
    "    print(sum([sum([each.shape[0] for each in label_features]) for label_features in label_attn_features.values()]))\n",
    "# print(sum([each.shape[0] for each in label_attn_features]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Make Attention Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Do Kernel Density Estimation, Calculate MI and Infer Dependency Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Save & Load attention features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=SAVE_CONCATENATED_FEATURES\n",
    "import os.path as osp\n",
    "import os\n",
    "\n",
    "\n",
    "KDE_DEVICE='cuda:0'\n",
    "\n",
    "if not LOAD_ATTN_SAMPLES and not METADATA_ONLY and not SAVE_DURING_EXTRACTING:\n",
    "    print(f'concatenating features')\n",
    "    # if CONCAT_DURING_EXTRACTING:\n",
    "    #     pos_features, neg_features = arc_attn_features[0].item, arc_attn_features[1].item\n",
    "    #     height_features = [each.item for each in height_attn_features.values()]\n",
    "    #     label_features = [each.item for each in label_attn_features.values()]\n",
    "    #     label_ids = [*label_attn_features.keys()]\n",
    "\n",
    "    # else:\n",
    "\n",
    "    (pos_features, neg_features), height_features, label_features, label_ids = concatenate_attn_features(arc_attn_features, height_attn_features, label_attn_features, concat_across_labels=False)\n",
    "    # pos|neg_features: [num_(pos|neg)_arcs, num_features(num_layers * num_heads)], height_features: [heights(tuple), num_arcs_of_this_height, num_features]\n",
    "    print(f'{pos_features.numel():,} pos attn-feature samples, {neg_features.numel():,} neg attn-feature samples')\n",
    "    print(f'{sum([this_height_feature.numel() for this_height_feature in height_features]):,} height attn-feature samples')\n",
    "    print(f'saving features...')\n",
    "    # transpose = ''\n",
    "\n",
    "    save_attn_samples(pos_features, osp.join(kde_save_pth, formalize_pickle_file_name(f'pos_attn_samples.pkl')))\n",
    "    if not USE_MEMMAP:\n",
    "        save_attn_samples(neg_features, osp.join(kde_save_pth, formalize_pickle_file_name('neg_attn_samples.pkl')))\n",
    "    save_attn_samples(height_features, osp.join(kde_save_pth, formalize_pickle_file_name('height_attn_samples.pkl')))\n",
    "    save_attn_samples(label_features, osp.join(kde_save_pth, formalize_pickle_file_name('label_attn_samples.pkl')))\n",
    "    with open(osp.join(kde_save_pth, 'labelmap.json'), 'w') as f:\n",
    "        labelmap = dep_parser_llama.labelmap\n",
    "        json.dump(labelmap, f)\n",
    "    if not USE_MEMMAP:\n",
    "        with open(osp.join(kde_save_pth, formalize_pickle_file_name('metadata.json')), 'w') as f:\n",
    "            json.dump({\n",
    "                'n_pos_samples': len(pos_features),\n",
    "                'n_neg_samples': len(neg_features),\n",
    "            }, f)\n",
    "    # with open(osp.join(kde_save_pth, 'label_id2name.json'), 'w') as f:\n",
    "    #     json.dump({label_id: label_name for label_name, label_id in dep_parser_llama.labelmap.items()}, f)\n",
    "    label_id2name = {label_id: label_name for label_name, label_id in dep_parser_llama.labelmap.items()}\n",
    "    with open(osp.join(kde_save_pth, 'label_names.txt'), 'w') as f:\n",
    "        label_names = [label_id2name[label_id] for label_id in label_ids]\n",
    "        json.dump(label_names, f)\n",
    "\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "if BREAK_AFTER_EXTRACT:\n",
    "    raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=LOAD_CONCATENATED_FEATURES\n",
    "if LOAD_ATTN_SAMPLES and not SAVE_DURING_EXTRACTING:\n",
    "    pos_features: Tensor = torch.load(osp.join(kde_save_pth, formalize_pickle_file_name('pos_attn_samples.pkl')), map_location='cpu')\n",
    "    if not LOAD_KDE_ESTIMS: \n",
    "        # since neg_features is only used by KDE estimation of f(a_i|t=t_i) \n",
    "        # if loads both attention samples and kde estims, then `neg_features`, which is very large, will not be loaded\n",
    "        if USE_MEMMAP: \n",
    "            with open(osp.join(kde_save_pth, formalize_pickle_file_name('metadata.json'))) as f:\n",
    "                metadata = json.load(f)\n",
    "                min_attn_feature_val, max_attn_feature_val = metadata['min_attn_score'], metadata['max_attn_score']\n",
    "                total_n_neg_samples = metadata['n_neg_samples']\n",
    "\n",
    "            neg_features: np.ndarray = np.memmap(osp.join(kde_save_pth, formalize_pickle_file_name('neg_attn_samples.pkl')), shape=(total_n_neg_samples, pos_features.shape[1]), dtype='float32', mode='r', )\n",
    "        else:\n",
    "            neg_features: Tensor = torch.load(osp.join(kde_save_pth, formalize_pickle_file_name('neg_attn_samples.pkl')), map_location='cpu')\n",
    "    height_features: Tuple[Tensor] = torch.load(osp.join(kde_save_pth, formalize_pickle_file_name('height_attn_samples.pkl')), map_location='cpu')\n",
    "    label_features: Tuple[Tensor] = torch.load(osp.join(kde_save_pth, formalize_pickle_file_name('label_attn_samples.pkl')), map_location='cpu')\n",
    "    labelmap: Dict[str, int] = json.load(open(osp.join(kde_save_pth, 'labelmap.json')))\n",
    "    # label_id2name = json.load(open(osp.join(kde_save_pth, 'label_id2name.json')))\n",
    "    label_names: List[int] = json.load(open(osp.join(kde_save_pth, 'label_names.txt')))\n",
    "    # if USE_MEMMAP:\n",
    "if LOAD_ATTN_SAMPLES and SAVE_DURING_EXTRACTING:\n",
    "    pickle_file_names = [*filter(lambda x: re.match(r'.*_attn_samples.*_(\\d+)of(\\d+).pkl', x), os.listdir(kde_save_pth))]\n",
    "    possible_total_num_files = set([re.search(r'.*_(\\d+)of(\\d+)', each).group(2) for each in pickle_file_names])\n",
    "    if len(possible_total_num_files) != 1:\n",
    "        raise RuntimeError(f\"found multiple possible total number of attn-sample save chunks: {', '.join(possible_total_num_files)}\")\n",
    "\n",
    "    labelmap: Dict[str, int] = json.load(open(osp.join(kde_save_pth, 'labelmap.json')))\n",
    "    # label_id2name = json.load(open(osp.join(kde_save_pth, 'label_id2name.json')))\n",
    "    label_names: List[int] = json.load(open(osp.join(kde_save_pth, 'label_names.txt')))\n",
    "    def add_to_pos_features(features, this_features):\n",
    "        return this_features if features is None else torch.cat([features, this_features], dim=0)\n",
    "    # def add_to_preallocated_features(features, start_idx, this_features):\n",
    "    #     features[start_idx: start_idx + this_features.shape[0]] = this_features\n",
    "    def add_to_label_features(features, this_features):\n",
    "        if len(features) == 0:\n",
    "            return this_features\n",
    "        else:\n",
    "            return [torch.cat([feature, this_feature], dim=0) for feature, this_feature in zip(features, this_features)]\n",
    "    pos_features, neg_features = None, None\n",
    "    height_features, label_features = [], []\n",
    "    total_num_files = int(possible_total_num_files.pop())\n",
    "    if LOAD_KDE_ESTIMS and not FORCE_RECACHE and osp.exists(CACHE_DIR):\n",
    "        pos_features, height_features, label_features = torch.load(CACHE_DIR)\n",
    "\n",
    "    else:\n",
    "        with open(osp.join(kde_save_pth, formalize_pickle_file_name('metadata.json')), 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "            total_n_neg_samples = metadata['n_neg_samples']\n",
    "            min_attn_feature_val, max_attn_feature_val = metadata['min_attn_score'], metadata['max_attn_score']\n",
    "            current_neg_sample_ptr = 0\n",
    "\n",
    "        for current_num_save in (chunked_load_pbar := trange(total_num_files, desc='loading attn sample chunks...')):\n",
    "            # these lines are annotated since I want to load neg_features during ESTIMATE_KDE\n",
    "            # if not LOAD_KDE_ESTIMS:\n",
    "            #     # since neg_features is only used by KDE estimation of f(a_i|t=t_i) \n",
    "            #     # if both attention samples and kde estims are loaded, the time consumption will be incredibly high\n",
    "            #     this_neg_features: Tensor = torch.load(osp.join(kde_save_pth, formalize_pickle_file_name('neg_attn_samples.pkl', current_num_save, total_num_files)), map_location='cpu')\n",
    "            #     if neg_features is None:\n",
    "            #         neg_features = torch.empty((total_n_neg_samples, this_neg_features.shape[1]))\n",
    "            #         chunked_load_pbar.set_description('loading attention samples...')\n",
    "            #     neg_features[current_neg_sample_ptr: current_neg_sample_ptr + this_neg_features.shape[0]] = this_neg_features\n",
    "            #     current_neg_sample_ptr += this_neg_features.shape[0]\n",
    "            \n",
    "            this_pos_features: Tensor = torch.load(osp.join(kde_save_pth, formalize_pickle_file_name('pos_attn_samples.pkl', current_num_save, total_num_files)), map_location='cpu')\n",
    "            pos_features: Tensor = add_to_pos_features(pos_features, this_pos_features)\n",
    "            this_height_features: Tuple[Tensor] = torch.load(osp.join(kde_save_pth, formalize_pickle_file_name('height_attn_samples.pkl', current_num_save, total_num_files)), map_location='cpu')\n",
    "            height_features = add_to_label_features(height_features, this_height_features)\n",
    "            this_label_features: Tuple[Tensor] = torch.load(osp.join(kde_save_pth, formalize_pickle_file_name('label_attn_samples.pkl', current_num_save, total_num_files)), map_location='cpu')\n",
    "            label_features = add_to_label_features(label_features, this_label_features)\n",
    "        torch.save([pos_features, height_features, label_features], CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Estimate attention conditional probability ($f(a_i|y)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1 mask out dependency labels with no attention samples corresponding to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-dim attn-vector occurred at [0]<UNK>\n",
      "0-dim attn-vector occurred at [46]<s>\n"
     ]
    }
   ],
   "source": [
    "#papermill_description=MASK_ZERO_DIM_FEATURES\n",
    "masked_label_indices = [] # NOTE: indices corresponding to the position in label_features (concatenated, Type: List[Tensor[num_samples, num_attn_features]])\n",
    "for idx, label_name in enumerate(label_names):\n",
    "    # n_samples = len(label_attn_features[label_idx])\n",
    "    # print(n_samples)\n",
    "    size = label_features[idx].shape\n",
    "    # except IndexError:\n",
    "    #     print(f'Error occurred when processing [{idx}]{label_idx}, features:')\n",
    "    #     print(label_features[idx])\n",
    "    if size[0] == 0:\n",
    "        print(f'0-dim attn-vector occurred at [{idx}]{label_name}')\n",
    "        # print(size, label_attn_features[labelmap[label_name]])\n",
    "        masked_label_indices.append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2 calculate attention conditional probabilities ($f(a_i|y=0), f(a_i|y=1), f(a_i|l=l_0)$) using KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=ESTIMATE_KDE\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from explaination import integral_torch_cuda, estimate_kde_torch\n",
    "from copy import deepcopy\n",
    "\n",
    "def estimate_kde_torch_sliced(x: Tensor, samples: Tensor, num_slices: int = 1):\n",
    "    \"\"\"split across x, and estimate KDE for each slice, then concatenate them, in order to save GPU memory\"\"\"\n",
    "    x_slices = torch.split(x, math.ceil(x.shape[0] / num_slices), dim=0)\n",
    "    for idx, x_slice in enumerate(x_slices):\n",
    "        if idx == 0:\n",
    "            kde_estim = estimate_kde_torch(x_slice, samples, normalize=False)\n",
    "        else:\n",
    "            kde_estim = torch.cat([kde_estim, estimate_kde_torch(x_slice, samples, normalize=False)])\n",
    "\n",
    "    \n",
    "    cdf_max = integral_torch_cuda(x, kde_estim)\n",
    "    return kde_estim / cdf_max\n",
    "\n",
    "if not LOAD_KDE_ESTIMS:\n",
    "    model_llama = model_llama.cpu() \n",
    "    print(f'offload model_llama to cpu to save GPU memory before doing KDE estimations')\n",
    "    n_feature_dims = pos_features.shape[-1]\n",
    "    pos_kde_estims, neg_kde_estims = [], []\n",
    "    label_kde_estims = defaultdict(list)\n",
    "\n",
    "    # TODO: if neg_features is a memmap, then this operation will be very slow, implement max and min during extracting and save to metadata\n",
    "    # if not USE_MEMMAP:\n",
    "        # max_attn_feature_val = max(pos_features.max().item(), neg_features.max().item())\n",
    "        # min_attn_feature_val = min(pos_features.min().item(), neg_features.min().item())\n",
    "\n",
    "    print(f'feature range: [{min_attn_feature_val}, {max_attn_feature_val}]')\n",
    "\n",
    "    x = torch.cat([torch.arange(round(min_attn_feature_val, 1) - 0.2, 0, 0.1), torch.arange(0, 1, 0.01), torch.arange(1, max_attn_feature_val + 0.2)]).to(KDE_DEVICE)\n",
    "    neg_shard_size = 128\n",
    "    neg_shard_starts = range(0, n_feature_dims, neg_shard_size)\n",
    "    current_shard_start = 0\n",
    "    for feat_idx in trange(n_feature_dims, desc=f\"estimating kde for each head\"): # iterate over features\n",
    "        # pos_kde_estims: [f(a_1|y=1), f(a_2|y=1), f(a_3|y=1), ... ]\n",
    "        pos_kde_estims.append(estimate_kde_torch_sliced(x, pos_features[:, feat_idx].view(-1).to(KDE_DEVICE), num_slices=KDE_NUM_SLICES).cpu())\n",
    "        # neg_kde_estims: [f(a_1|y=0), f(a_2|y=0), f(a_3|y=0), ... ]\n",
    "        # if not USE_MEMMAP:\n",
    "        #     neg_kde_estims.append(estimate_kde_torch_sliced(x, neg_features[:, feat_idx].view(-1).to(KDE_DEVICE), num_slices=KDE_NUM_SLICES).cpu())\n",
    "        if SAVE_DURING_EXTRACTING: \n",
    "            if feat_idx in neg_shard_starts:\n",
    "                current_shard_start = feat_idx\n",
    "                neg_shard = torch.empty((total_n_neg_samples, neg_shard_size), dtype=pos_features.dtype)\n",
    "                current_neg_sample_ptr = 0\n",
    "                for current_num_save in trange(total_num_files, desc=f'loading neg tensor ({current_shard_start} to {current_shard_start + neg_shard_size})...', leave=False):\n",
    "                    this_neg_features: Tensor = torch.load(osp.join(kde_save_pth, formalize_pickle_file_name('neg_attn_samples.pkl', current_num_save, total_num_files)), map_location='cpu')\n",
    "                    neg_shard[current_neg_sample_ptr: current_neg_sample_ptr + this_neg_features.shape[0]] = this_neg_features[:, current_shard_start: current_shard_start + neg_shard_size]\n",
    "                    current_neg_sample_ptr += this_neg_features.shape[0]\n",
    "                    \n",
    "            neg_kde_estims.append(estimate_kde_torch_sliced(x, neg_shard[:, feat_idx - current_shard_start].view(-1).to(KDE_DEVICE), num_slices=KDE_NUM_SLICES).cpu())\n",
    "        else:\n",
    "            raise NotImplementedError('Error, attn sample saving methods other than `SAVE_DURING_EXTRACTING` are not implemented yet')\n",
    "        for idx, label_name in enumerate(label_names):\n",
    "            if idx not in masked_label_indices:\n",
    "                label_kde_estims[label_name].append(estimate_kde_torch_sliced(x, label_features[idx][:, feat_idx].view(-1).to(KDE_DEVICE), num_slices=KDE_NUM_SLICES).cpu())\n",
    "                # label_kde_estims: {'label_name_1': [f(a_1|l=1), f(a_2|l=1), ...], 'label_name_m': [f(a_1|l=m), f(a_2|l=m), ...]}\n",
    "    \n",
    "    # if USE_MEMMAP:\n",
    "    #     chunk_size = 128\n",
    "    #     pbar = trange(n_feature_dims, desc='estimating KDE for neg_features', ncols=100)\n",
    "    #     for feat_start in range(0, neg_features.shape[1], chunk_size):\n",
    "    #         this_neg_features = torch.from_numpy(neg_features[:, feat_start: feat_start + chunk_size])\n",
    "    #         for feat_idx in range(feat_start, min(feat_start + chunk_size, neg_features.shape[1])):\n",
    "    #             neg_kde_estims.append(estimate_kde_torch_sliced(x.cpu(), this_neg_features[:, feat_idx - feat_start].view(-1), num_slices=KDE_NUM_SLICES).cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of attention heads: $h$, number of dependency labels: $m$\n",
    "\n",
    "pos_kde_estims: $[f(a_1|y=1), f(a_2|y=1), ..., f(a_h|y=1)]$\n",
    "\n",
    "neg_kde_estims: $[f(a_1|y=0), f(a_2|y=0), ..., f(a_h|y=0)]$\n",
    "\n",
    "label_kde_estims: \n",
    "$$\n",
    "\\textrm{label}_1: [f(a_1|l=1), f(a_2|l=1), ..., f(a_h|l=1)],\\\\...\\\\\n",
    "\\textrm{label}_m: [f(a_1|l=m), f(a_2|l=m), ..., f(a_h|l=m)]\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.3 Save/Load KDE estimated probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=SAVE_KDE_ESTIMS\n",
    "# data_proportion = '_0_2'\n",
    "# transpose = '_transpose'\n",
    "if not LOAD_KDE_ESTIMS:\n",
    "    pkl.dump([each.cpu() for each in pos_kde_estims], open(osp.join(kde_save_pth, formalize_pickle_file_name('pos_attn_conditional.pkl')), 'wb'))\n",
    "    pkl.dump([each.cpu() for each in neg_kde_estims], open(osp.join(kde_save_pth, formalize_pickle_file_name('neg_attn_conditional.pkl')), 'wb'))\n",
    "    for label_name in label_kde_estims: # move \n",
    "        for idx in range(len(label_kde_estims[label_name])):\n",
    "            label_kde_estims[label_name][idx] = label_kde_estims[label_name][idx].cpu()\n",
    "    pkl.dump(label_kde_estims, open(osp.join(kde_save_pth, formalize_pickle_file_name('label_attn_conditional.pkl')), 'wb'))\n",
    "    pkl.dump(x.cpu(), open(osp.join(kde_save_pth, formalize_pickle_file_name('x.pkl')), 'wb'))\n",
    "    if BREAK_AFTER_KDE:\n",
    "        raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=LOAD_KDE_ESTIMS\n",
    "from os import path as osp\n",
    "\n",
    "if LOAD_KDE_ESTIMS:\n",
    "    pos_kde_estims: List[Tensor] = pkl.load(open(osp.join(kde_save_pth, formalize_pickle_file_name('pos_attn_conditional.pkl')), 'rb')) # f(a_i|y=1)\n",
    "    neg_kde_estims: List[Tensor] = pkl.load(open(osp.join(kde_save_pth, formalize_pickle_file_name('neg_attn_conditional.pkl')), 'rb')) # f(a_i|y=0)\n",
    "    label_kde_estims: Dict[str, List[Tensor]] = pkl.load(open(osp.join(kde_save_pth, formalize_pickle_file_name('label_attn_conditional.pkl')), 'rb')) # {'label_name(l)': [f(a_i|y=l)]}\n",
    "    x: Tensor = pkl.load(open(osp.join(kde_save_pth, formalize_pickle_file_name('x.pkl')), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "dict_keys(['prep', 'det', 'nn', 'num', 'pobj', 'punct', 'poss', 'possessive', 'amod', 'nsubj', 'appos', 'dobj', 'dep', 'cc', 'conj', 'nsubjpass', 'partmod', 'auxpass', 'advmod', 'root', 'ccomp', 'aux', 'cop', 'xcomp', 'quantmod', 'tmod', 'neg', 'infmod', 'rcmod', 'pcomp', 'mark', 'advcl', 'predet', 'csubj', 'mwe', 'parataxis', 'npadvmod', 'number', 'acomp', 'prt', 'iobj', 'preconj', 'expl', 'discourse', 'csubjpass'])\n",
      "[0, 46]\n",
      "[1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024]\n",
      "[device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu')]\n"
     ]
    }
   ],
   "source": [
    "# print(label_kde_estims)\n",
    "# label_kde_estims.pop('<s>')\n",
    "print(len(label_kde_estims)) \n",
    "print(label_kde_estims.keys()) \n",
    "print(masked_label_indices)\n",
    "print([len(each) for each in label_kde_estims.values()])\n",
    "print([each[0].device for each in label_kde_estims.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Calculate label joint probability ($f(a_i,y)$), marginal probability ($p(y=y_m)$) and label conditional probability ($f(y|a_i)$ or $f(l|a_i)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1024/1024 [00:00<00:00, 1154.13it/s]\n"
     ]
    }
   ],
   "source": [
    "#papermill_description=EST_JOINT_CONDITIONAL_PROBAB\n",
    "with open(osp.join(kde_save_pth, formalize_pickle_file_name('metadata.json')), 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "n_pos_samples, n_neg_samples = metadata['n_pos_samples'], metadata['n_neg_samples']\n",
    "n_samples = n_neg_samples + n_pos_samples\n",
    "n_samples_per_label = [label_features[idx].shape[0] for idx in range(len(label_features)) if idx not in masked_label_indices]\n",
    "assert sum(n_samples_per_label) == n_pos_samples, \\\n",
    "    \"the sum of all samples of each label from `label_features` must be equal to the number of positive samples: \"\\\n",
    "    f\"{sum(n_samples_per_label)} != {n_pos_samples}\"\n",
    "n_samples_per_label += [n_neg_samples]\n",
    "# print(n_pos_samples, n_neg_samples)\n",
    "pos_probab, neg_probab = n_pos_samples / n_samples, n_neg_samples / n_samples # p(y=1), p(y=0)\n",
    "label_probabilities = [n_samples / sum(n_samples_per_label) for n_samples in n_samples_per_label] # p(l=0,1,2,...,num_labels) with last probability for `no arc`\n",
    "joint_probab_pos: List[Tensor] = [] # p(a_0, y=0), p(a_1, y=0), ..., p(a_h, y=0): List[n_heads], of Tensor[n_x]\n",
    "joint_probab_neg: List[Tensor] = [] # p(a_0, y=1), p(a_1, y=1), ..., p(a_h, y=1): List[n_heads], of Tensor[n_x]\n",
    "joint_probab_label: List[List[Tensor]] = [] # List[n_heads, n_labels] of Tensor[n_x]\n",
    "# p(l,a_i): [\n",
    "#   [p(l=1,a_0), p(l=2,a_0), p(l=3,a_0), ...], \n",
    "#   [p(l=1,a_1), p(l=2,a_1), p(l=3,a_1), ...], \n",
    "#  ]\n",
    "# joint_probab_label_neg: List[List[Tensor]] = [] # Tensor[n_heads, n_labels, n_x]\n",
    "# # p(^l,a_i): [\n",
    "# #   [p(l!=1,a_0)[n_x], p(l!=2,a_0)[n_x], p(l!=3,a_0)[n_x], ...], \n",
    "# #   [p(l!=1,a_1)[n_x], p(l!=2,a_1)[n_x], p(l!=3,a_1)[n_x], ...], \n",
    "# #  ]\n",
    "marginal_probab_attn: List[Tensor] = [] # p(a_i): List[n_heads] of Tensor[n_x]\n",
    "marginal_probab_attn_from_label: List[Tensor] = [] # p(a_i): List[n_heads] of Tensor[n_x]\n",
    "marginal_probab_attn_pos_from_label: List[Tensor] = [] # p(a_i): List[n_heads] of Tensor[n_x]\n",
    "conditional_probab_pos = [] # p(y=0|a_0), p(y=0|a_1), ... p(y=0|a_h): List[n_heads], of Tensor[n_x]\n",
    "conditional_probab_neg = [] # p(y=1|a_0), p(y=1|a_1), ... p(y=1|a_h): List[n_heads], of Tensor[n_x]\n",
    "conditional_probab_label = [] # List[n_heads, n_labels] of Tensor[n_x]\n",
    "# p(l|a_i): [\n",
    "#   [p(l=1|a_0), p(l=2|a_0), p(l=3|a_0), ...], \n",
    "#   [p(l=1|a_1), p(l=2|a_1), p(l=3|a_1), ...], \n",
    "#  ]\n",
    "# conditional_probab_label_neg = [] # List[n_heads, n_labels] of Tensor[n_x]\n",
    "# # p(^l|a_i)\n",
    "\n",
    "for attn_feat_idx in trange(len(pos_kde_estims)): # attn_feat_idx (i)\n",
    "    joint_probab_pos.append(pos_probab * pos_kde_estims[attn_feat_idx]) # p(a_i,y=1) = p(a_i|y=1) * p(y=1)\n",
    "    joint_probab_neg.append(neg_probab * neg_kde_estims[attn_feat_idx]) # p(a_i,y=0) = p(a_i|y=0) * p(y=0)\n",
    "    joint_probab_label.append([])\n",
    "    # appended: [p(a_i, l=0), p(a_i, l=1), ... p(a_i, l=m)] = \n",
    "    #       [p(a_i|l=0) * p(l=0), p(a_i|l=1) * p(l=1), ... p(a_i|l=m) * p(l=m)]\n",
    "    for label_idx, (label_name, label_estim) in enumerate(label_kde_estims.items()):\n",
    "        joint_probab_label[-1].append(label_probabilities[label_idx] * label_estim[attn_feat_idx])\n",
    "    joint_probab_label[-1].append(label_probabilities[-1] * neg_kde_estims[attn_feat_idx])\n",
    "    marginal_probab_attn.append(joint_probab_pos[-1] + joint_probab_neg[-1]) # p(a_i) = p(a_i,y=1) + p(a_i,y=0)\n",
    "    # print(len(joint_probab_label[-1]), [type(each) for each in joint_probab_label[-1]])\n",
    "    # print(sum(joint_probab_label[-1]))\n",
    "    marginal_probab_attn_from_label.append(sum(joint_probab_label[-1])) # p(a_i) = p(a_i,l=0) + p(a_i,l=1) + ... + p(a_i,l=m) \n",
    "    marginal_probab_attn_pos_from_label.append(sum(joint_probab_label[-1][:-1])) # (excluding the negative label, WARNING: NOT corresponding to a valid probabalistic distribution, should be normalized)\n",
    "    conditional_probab_pos.append((joint_probab_pos[-1] / marginal_probab_attn[-1]).masked_fill(marginal_probab_attn[-1] == 0, 0)) # p(y=1|a_i) = p(a_i,y=1) / p(a_i)\n",
    "    conditional_probab_neg.append((joint_probab_neg[-1] / marginal_probab_attn[-1]).masked_fill(marginal_probab_attn[-1] == 0, 0)) # p(y=0|a_i) = p(a_i,y=0) / p(a_i)\n",
    "    conditional_probab_label.append([])\n",
    "    for label_idx, this_joint_probab_label in enumerate(joint_probab_label[-1]): # this_joint_probab_label: p(a_i, l=label_idx or no_arc)\n",
    "        conditional_probab_label[-1].append(this_joint_probab_label / marginal_probab_attn_from_label[-1]) # appended p(l=label_idx or no_arc | a_i)\n",
    "\n",
    "joint_probab_label_stacked = torch.stack([torch.stack(each) for each in joint_probab_label]) # p(a_i, l=label_idx) List[n_heads, n_labels] of Tensor[n_x] -> Tensor[n_heads, n_labels, n_x]\n",
    "_, n_labels, _ = joint_probab_label_stacked.shape\n",
    "\n",
    "# for attn_feat_idx in trange(len(pos_kde_estims)):\n",
    "#     joint_probab_label_neg.append([])\n",
    "#     conditional_probab_label_neg.append([])\n",
    "#     for label_idx in range(joint_probab_label_stacked.shape[1]):\n",
    "#         joint_probab_label_neg[-1].append(\n",
    "#             joint_probab_label_stacked[attn_feat_idx, torch.arange(n_labels) != label_idx, :].sum(dim=0)\n",
    "#         )\n",
    "#         # print(joint_probab_label_stacked[attn_feat_idx, torch.arange(n_labels) != label_idx, :].shape)\n",
    "#         # print(joint_probab_label_neg[-1][-1].shape , marginal_probab_attn_from_label[attn_feat_idx].shape)\n",
    "#         conditional_probab_label_neg[-1].append(\n",
    "#             joint_probab_label_neg[-1][-1] / marginal_probab_attn_from_label[attn_feat_idx]\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the marginal probabilities integrate to 1 (and )\n",
    "# for this_marginal_probab_attn, this_marginal_probab_attn_from_label in zip(marginal_probab_attn, marginal_probab_attn_from_label):\n",
    "#     print(integral_torch_cuda(x, this_marginal_probab_attn.cuda()))\n",
    "#     print(integral_torch_cuda(x, this_marginal_probab_attn_from_label.cuda()))\n",
    "\n",
    "# for attn_feat_idx in range(len(conditional_probab_label)):\n",
    "#     # for label_idx in range(len(joint_probab_label_neg[attn_feat_idx])):\n",
    "#     this_conditional_probab_label = torch.stack(conditional_probab_label[attn_feat_idx])\n",
    "#     print(this_conditional_probab_label.sum(0))\n",
    "# print(marginal_probab_attn[0].shape, marginal_probab_attn_from_label[0].shape)\n",
    "# plt.plot(x.cpu(), marginal_probab_attn[0].cpu())\n",
    "# plt.plot(x.cpu(), marginal_probab_attn_from_label[0].cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "joint_probab_pos: $p(a_1, y=1), p(a_2, y=1), \\cdots, p(a_h, y=1) $\n",
    "\n",
    "joint_probab_neg: $p(a_1, y=0), p(a_2, y=0), \\cdots, p(a_h, y=0) $\n",
    "\n",
    "joint_probab_label: \n",
    "$$ \\begin{matrix}\n",
    "& label_0 & label_1 && label_m \\\\\n",
    "{\\textrm{head}_1} & p(a_1, l=0)& p(a_1, l=1)& \\cdots& p(a_1, l=m) \\\\&\\vdots&\\ddots&&\\vdots\\\\\n",
    "{\\textrm{head}_h} & p(a_h, l=0)& p(a_h, l=1)& \\cdots& p(a_h, l=m)\n",
    "\\end{matrix} $$\n",
    "\n",
    "**marginal**_probab_attn, marginal_probab_attn_from_label: $f(a_1), f(a_2), \\cdots, f(a_h)$\n",
    "\n",
    "conditional_probab_pos: $p(y=1|a_1), (y=1|a_2), \\cdots, p(y=1|a_h)$\n",
    "\n",
    "conditional_probab_neg: $p(y=0|a_1), (y=0|a_2), \\cdots, p(y=0|a_h)$\n",
    "\n",
    "conditional_probab_label: \n",
    "$$ \\begin{matrix}\n",
    "& label_0 & label_1 && label_m \\\\\n",
    "{\\textrm{head}_1} & p( l=0 | a_1)& p( l=1|a_1)& \\cdots& p( l=m|a_1) \\\\&\\vdots&\\ddots&&\\vdots\\\\\n",
    "{\\textrm{head}_h} & p( l=0 | a_h)& p( l=1 | a_h)& \\cdots& p( l=m | a_h)\n",
    "\\end{matrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=SAVE_JOINT_CONDITIONAL_PROBAB\n",
    "pkl.dump([each.cpu() for each in conditional_probab_pos], open(osp.join(kde_save_pth, formalize_pickle_file_name('pos_arc_conditional.pkl')), 'wb'))\n",
    "pkl.dump([each.cpu() for each in conditional_probab_neg], open(osp.join(kde_save_pth, formalize_pickle_file_name('neg_arc_conditional.pkl')), 'wb'))\n",
    "pkl.dump([each.cpu() for each in joint_probab_pos], open(osp.join(kde_save_pth, formalize_pickle_file_name('joint_pos.pkl')), 'wb'))\n",
    "pkl.dump([each.cpu() for each in joint_probab_neg], open(osp.join(kde_save_pth, formalize_pickle_file_name('joint_neg.pkl')), 'wb'))\n",
    "for i in range(len(conditional_probab_label)):\n",
    "    for j in range(len(conditional_probab_label[i])):\n",
    "        conditional_probab_label[i][j] = conditional_probab_label[i][j].cpu()\n",
    "        joint_probab_label[i][j] = joint_probab_label[i][j].cpu()\n",
    "pkl.dump(conditional_probab_label, open(osp.join(kde_save_pth, formalize_pickle_file_name('label_conditional.pkl')), 'wb'))\n",
    "pkl.dump(joint_probab_label, open(osp.join(kde_save_pth, formalize_pickle_file_name('joint_label.pkl')), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=SAVE_JOINT_CONDITIONAL_PROBAB\n",
    "# conditional_probab_pos: List[Tensor] = pkl.load(open(osp.join(kde_save_pth, formalize_pickle_file_name('pos_arc_conditional.pkl')), 'rb'))\n",
    "# conditional_probab_neg: List[Tensor] = pkl.load(open(osp.join(kde_save_pth, formalize_pickle_file_name('neg_arc_conditional.pkl')), 'rb'))\n",
    "# conditional_probab_label: List[List[Tensor]] = pkl.load(open(osp.join(kde_save_pth, formalize_pickle_file_name('label_conditional.pkl')), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=MOVE_JOINT_PROBAB_TO_CPU\n",
    "conditional_probab_pos = [each.cpu() for each in conditional_probab_pos]\n",
    "conditional_probab_neg = [each.cpu() for each in conditional_probab_neg]\n",
    "for i in range(len(conditional_probab_label)):\n",
    "    for j in range(len(conditional_probab_label[i])):\n",
    "        conditional_probab_label[i][j] = conditional_probab_label[i][j].cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Estimate Mutual Information\n",
    "formula\n",
    "$$\n",
    "\\sum_{m=0}^{n_{\\rm{labels}}} {\n",
    "    \\int_{attn_{\\rm{min}}}^{attn_{\\rm{max}}} {\n",
    "        f(a_i, y=y_m) (\\textrm{joint\\textunderscore probab\\textunderscore pos|neg|label})\n",
    "        \\log{ \\frac{f(a_i,y=y_m)}{f(a_i)p(y=y_m)} }\n",
    "        \\rm{d} a_i\n",
    "    }\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_probabilities: [46(A)]\n",
      "marginal_probab_attn_from_label: [1024(A), [154](T)]\n",
      "joint_probab_label: [1024(A), 46(A), [154](T)]\n"
     ]
    }
   ],
   "source": [
    "print(\"label_probabilities:\", get_shape(label_probabilities))\n",
    "print(\"marginal_probab_attn_from_label:\", get_shape(marginal_probab_attn_from_label))\n",
    "print(\"joint_probab_label:\", get_shape(joint_probab_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos_probab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexplaination\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m integral_torch_cuda, inference_by_func, estimate_mi\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdebugpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mbreakpoint\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m breakpoint_dbp\n\u001b[0;32m----> 4\u001b[0m arc_probabilities \u001b[38;5;241m=\u001b[39m [\u001b[43mpos_probab\u001b[49m, neg_probab] \u001b[38;5;66;03m# p(y=y_m)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m joint_probabilities_arc \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mzip\u001b[39m(joint_probab_pos, joint_probab_neg)] \u001b[38;5;66;03m# [[f(a_0, y=0), f(a_0, y=1)], [f(a_1, y=0), f(a_1, y=1)], ... ]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# raise KeyboardInterrupt\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pos_probab' is not defined"
     ]
    }
   ],
   "source": [
    "#papermill_description=ESTIMATE_MI\n",
    "from explaination import integral_torch_cuda, inference_by_func, estimate_mi\n",
    "# from debugpy import breakpoint as breakpoint_dbp\n",
    "\n",
    "def calculate_entropy(probabilities):\n",
    "    return sum([-each * math.log2(each) for each in probabilities])\n",
    "\n",
    "arc_probabilities = [pos_probab, neg_probab] # p(y=y_m)\n",
    "joint_probabilities_arc = [*zip(joint_probab_pos, joint_probab_neg)] # [[f(a_0, y=0), f(a_0, y=1)], [f(a_1, y=0), f(a_1, y=1)], ... ]\n",
    "\n",
    "# raise KeyboardInterrupt\n",
    "pos_scaler = sum(label_probabilities[:-1])\n",
    "label_probabilities_pos = (torch.tensor(label_probabilities[:-1]) / pos_scaler).tolist() # [n_labels - 1]\n",
    "pos_joint_probab_label_stacked = joint_probab_label_stacked[:, :-1, :] / pos_scaler # [n_attn_features, n_labels - 1, n_x]\n",
    "marginal_probab_attn_pos_from_label_stacked = torch.stack(marginal_probab_attn_pos_from_label) / pos_scaler\n",
    "\n",
    "if not LOAD_MI:\n",
    "    binary_label_mi: List[List[float]] = [] # List[num_labels, num_heads]\n",
    "    binary_label_mi_pos: List[List[float]] = [] # List[num_labels, num_heads]\n",
    "\n",
    "    # joint_probab_label_stacked = torch.stack([torch.stack(each) for each in joint_probab_label])\n",
    "    # print(joint_probab_label_stacked.shape)\n",
    "    # print(joint_probab_label_stacked[:, 0, :].shape)\n",
    "    # print(joint_probab_label_stacked[:, torch.arange(joint_probab_label_stacked.size(1)) != 0, :].shape)\n",
    "    # print(get_shape(joint_probab_label_stacked))\n",
    "    mi_intermediate_dir = osp.join(kde_save_pth, f\"mi_intermediate_{str(DATA_PROPORTION).replace('.', '_')}{'_transpose' if TRANSPOSE else ''}\")\n",
    "    os.makedirs(mi_intermediate_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    for label_idx in trange(len(label_probabilities), desc='estimating for each label type'):\n",
    "        label_mi_intermediate_dir = osp.join(mi_intermediate_dir, f'{label_idx:02d}')\n",
    "        os.makedirs(label_mi_intermediate_dir, exist_ok=True)\n",
    "        n_attn_features, n_labels, n_x = joint_probab_label_stacked.shape\n",
    "\n",
    "        binary_label_mi.append(\n",
    "            estimate_mi(\n",
    "                x, [label_probabilities[label_idx], 1 - label_probabilities[label_idx]], marginal_probab_attn_from_label, \n",
    "                torch.cat([\n",
    "                    joint_probab_label_stacked[:, label_idx, :].view(n_attn_features, 1, n_x), \n",
    "                    joint_probab_label_stacked[:, torch.arange(n_labels) != label_idx, :].sum(1).view(n_attn_features, 1, n_x)\n",
    "                ], dim=1), # Tensor[n_heads, 2, n_x]\n",
    "                KDE_DEVICE, \n",
    "                # intermediate_save_dir=label_mi_intermediate_dir, \n",
    "            )\n",
    "        )\n",
    "\n",
    "        if label_idx != (len(label_probabilities) - 1):\n",
    "            binary_label_mi_pos.append(\n",
    "                estimate_mi(\n",
    "                    x, [label_probabilities_pos[label_idx], 1 - label_probabilities_pos[label_idx]], marginal_probab_attn_pos_from_label_stacked, \n",
    "                    torch.cat([\n",
    "                        _label_pos_joint_probab_label_stacked := pos_joint_probab_label_stacked[:, label_idx, :].view(n_attn_features, 1, n_x), \n",
    "                        _others_pos_joint_probab_label_stacked := pos_joint_probab_label_stacked[:, torch.arange(n_labels - 1) != label_idx, :].sum(1).view(n_attn_features, 1, n_x)\n",
    "                    ], dim=1), # Tensor[n_heads, 2, n_x]\n",
    "                    KDE_DEVICE, \n",
    "                    # intermediate_save_dir=label_mi_intermediate_dir, \n",
    "                )\n",
    "            )\n",
    "            mape = ((_label_pos_joint_probab_label_stacked + _others_pos_joint_probab_label_stacked).squeeze(1) - marginal_probab_attn_pos_from_label_stacked).abs() \\\n",
    "                / marginal_probab_attn_pos_from_label_stacked\n",
    "            mape.masked_fill_(mape.isnan(), 0)\n",
    "            this_label_entropy = calculate_entropy([label_probabilities_pos[label_idx], 1 - label_probabilities_pos[label_idx]])\n",
    "            if ((this_binary_label_mi_pos_proportion := (Tensor(binary_label_mi_pos[-1]) / this_label_entropy)) > 1).any():\n",
    "                breakpoint()\n",
    "            else:\n",
    "                print(f\"avg. pos binary mi proportion: {this_binary_label_mi_pos_proportion.mean().item()}, max: {this_binary_label_mi_pos_proportion.max().item()}\" )\n",
    "\n",
    "\n",
    "    arc_mi = estimate_mi(x, arc_probabilities, marginal_probab_attn, joint_probabilities_arc, KDE_DEVICE)\n",
    "    label_mi = estimate_mi(x, label_probabilities, marginal_probab_attn_from_label, joint_probab_label, KDE_DEVICE)\n",
    "    label_mi_pos = estimate_mi(x, label_probabilities_pos, \n",
    "        marginal_probab_attn_pos_from_label_stacked, \n",
    "        pos_joint_probab_label_stacked, KDE_DEVICE\n",
    "    )\n",
    "\n",
    "    with open(osp.join(kde_save_pth, formalize_pickle_file_name('mi.json')), 'w') as f:\n",
    "        json.dump({'arc_mi': arc_mi, 'label_mi': label_mi, 'binary_label_mi': binary_label_mi, 'pos_label_mi': label_mi_pos, 'pos_binary_label_mi': binary_label_mi_pos}, f)\n",
    "else:\n",
    "    with open(osp.join(kde_save_pth, formalize_pickle_file_name('mi.json')), 'r') as f:\n",
    "        mi_json = json.load(f)\n",
    "    arc_mi, label_mi, binary_label_mi, label_mi_pos, binary_label_mi_pos = \\\n",
    "        mi_json['arc_mi'], mi_json['label_mi'], mi_json['binary_label_mi'], mi_json['pos_label_mi'], mi_json['pos_binary_label_mi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2717388420678617\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(sum(label_mi_pos) / len(label_mi_pos))\n",
    "print(TRANSPOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum arc entropy proportion: 0.7371160386615214, maximum label entropy proportion: 0.4858623397666946\n",
      "avg. of max entropy proportion for each label 0.500809713974314\n"
     ]
    }
   ],
   "source": [
    "#papermill_description=CALCULATE_VAR_ENTROPY\n",
    "unmasked_label_names = [each for idx, each in enumerate(label_names) if idx not in masked_label_indices]\n",
    "arc_entropy = calculate_entropy(arc_probabilities)\n",
    "arc_entropy_proportions = [each / arc_entropy for each in arc_mi]\n",
    "label_entropy = calculate_entropy(label_probabilities)\n",
    "label_entropy_proportions = [each / label_entropy for each in label_mi]\n",
    "label_binary_entropies = [calculate_entropy([each, 1 - each]) for each in label_probabilities]\n",
    "label_binary_entropies_pos = [calculate_entropy([each, 1 - each]) for each in label_probabilities_pos]\n",
    "max_label_binary_mis = [max(each) for each in binary_label_mi]\n",
    "max_label_binary_mis_pos = [max(each) for each in binary_label_mi_pos]\n",
    "max_label_binary_head_indices = [np.argmax(each) for each in binary_label_mi]\n",
    "max_label_binary_head_indices_pos = [np.argmax(each) for each in binary_label_mi_pos]\n",
    "label_max_binary_entropy_proportions = []\n",
    "for idx, (e1, e2) in enumerate(zip(label_binary_entropies, max_label_binary_mis)):\n",
    "    label_max_binary_entropy_proportions.append(e2 / e1)\n",
    "    # print(f'[{idx}]{unmasked_label_names[idx]}: {e2 / e1}')\n",
    "    if e2 / e1 > 1:\n",
    "        print(f\"[{idx}]{unmasked_label_names[idx]} MI({e2}) greater than entropy({e1}) at head {max_label_binary_head_indices[idx]}\")\n",
    "\n",
    "label_max_binary_entropy_proportions_pos = []\n",
    "for idx, (e1, e2) in enumerate(zip(label_binary_entropies_pos, max_label_binary_mis_pos)):\n",
    "    label_max_binary_entropy_proportions_pos.append(e2 / e1)\n",
    "    # print(f'[{idx}]{unmasked_label_names[idx]}: {e2 / e1}')\n",
    "    if e2 / e1 > 1:\n",
    "        print(f\"[{idx}]{unmasked_label_names[idx]} MI({e2}) greater than entropy({e1}) at head {max_label_binary_head_indices[idx]}\")\n",
    "\n",
    "print(f'maximum arc entropy proportion: {max(arc_entropy_proportions)}, maximum label entropy proportion: {max(label_entropy_proportions)}')\n",
    "print(f'avg. of max entropy proportion for each label {sum(label_max_binary_entropy_proportions) / len(label_max_binary_entropy_proportions)}')\n",
    "print(f'avg. of max pos entropy proportion for each label {sum(label_max_binary_entropy_proportions_pos) / len(label_max_binary_entropy_proportions_pos)}')\n",
    "# labels without no_arc 0.21166560621232114 0.19102649549393663 0.24943423602262535 \n",
    "# labels with no_arc 0.21166560621232114 0.16670989650835305 0.23851857726097217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "[(0, 'prep'), (1, 'det'), (2, 'nn'), (3, 'num'), (4, 'pobj'), (5, 'punct'), (6, 'poss'), (7, 'possessive'), (8, 'amod'), (9, 'nsubj'), (10, 'appos'), (11, 'dobj'), (12, 'dep'), (13, 'cc'), (14, 'conj'), (15, 'nsubjpass'), (16, 'partmod'), (17, 'auxpass'), (18, 'advmod'), (19, 'root'), (20, 'ccomp'), (21, 'aux'), (22, 'cop'), (23, 'xcomp'), (24, 'quantmod'), (25, 'tmod'), (26, 'neg'), (27, 'infmod'), (28, 'rcmod'), (29, 'pcomp'), (30, 'mark'), (31, 'advcl'), (32, 'predet'), (33, 'csubj'), (34, 'mwe'), (35, 'parataxis'), (36, 'npadvmod'), (37, 'number'), (38, 'acomp'), (39, 'prt'), (40, 'iobj'), (41, 'preconj'), (42, 'expl'), (43, 'discourse'), (44, 'csubjpass')]\n"
     ]
    }
   ],
   "source": [
    "print(len(unmasked_label_names))\n",
    "print([*enumerate(unmasked_label_names)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Reconstruction Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high MI heads for each label type with threshold 0.4\n",
      "('prep', 582) ('det', 636) ('nn', 395) ('num', 7) ('pobj', 575) ('punct', 690) ('poss', 5) ('possessive', 61) ('amod', 322) ('nsubj', 302)\n",
      "('appos', 6) ('dobj', 216) ('dep', 5) ('cc', 23) ('conj', 92) ('nsubjpass', 5) ('partmod', 5) ('auxpass', 20) ('advmod', 6) ('root', 883)\n",
      "('ccomp', 30) ('aux', 227) ('cop', 8) ('xcomp', 29) ('quantmod', 8) ('tmod', 5) ('neg', 6) ('infmod', 9) ('rcmod', 13) ('pcomp', 7)\n",
      "('mark', 8) ('advcl', 5) ('predet', 5) ('csubj', 5) ('mwe', 5) ('parataxis', 5) ('npadvmod', 6) ('number', 5) ('acomp', 5) ('prt', 22)\n",
      "('iobj', 5) ('preconj', 6) ('expl', 16) ('discourse', 5) ('csubjpass', 5)\n",
      "heads responsible for no_arc: 1024\n",
      "total 6310 heads (including 5286 label heads)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 50\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# mass = 0.05\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# mass_high_mi_heads = get_high_mi_heads_mass(binary_label_mi, mass)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# print(f\"high MI heads for each label type with mass {mass}\")\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# print_listlike([(label_name, len(each)) for label_name, each in zip(unmasked_label_names, mass_high_mi_heads)])\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# print(f'heads responsible for no_arc: {(no_arc_high_mi_heads := len(mass_high_mi_heads[-1]))}')\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# print(f\"total {(total_num_high_mi_heads := sum([len(each) for each in mass_high_mi_heads]))} heads (including {total_num_high_mi_heads - no_arc_high_mi_heads} label heads)\")\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m BREAK_AFTER_MI:\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#papermill_description=GET_HIGH_MI_HEADS\n",
    "def get_high_mi_heads_threshold(binary_label_mi: List[List[float]], label_binary_entropies: List[float], threshold: float = 0.2, topk: int = 5):\n",
    "    label_high_mi_heads = []\n",
    "    for label_idx, this_binary_label_mi in enumerate(binary_label_mi):\n",
    "        sorted_mi_heads_and_mis = sorted(enumerate(this_binary_label_mi), key=lambda x: x[1], reverse=True)\n",
    "        # print(f\"label [{label_idx}]{unmasked_label_names[label_idx]}: {[(idx, round(each, 4), round(each / label_binary_entropies[label_idx], 4)) for idx, each in  high_mi_heads_and_mis]}\")\n",
    "        topk_mi_heads = set([idx for idx, mi in sorted_mi_heads_and_mis[:topk]])\n",
    "        high_mi_proportion_heads = set([idx for idx, mi in sorted_mi_heads_and_mis if mi / label_binary_entropies[label_idx] > threshold])\n",
    "        label_high_mi_heads.append(topk_mi_heads.union(high_mi_proportion_heads))\n",
    "        # for each in high_mi_heads:\n",
    "    return label_high_mi_heads\n",
    "\n",
    "def get_high_mi_heads_threshold_mix(binary_label_mi: List[List[float]], label_binary_entropies: List[float], \n",
    "    binary_label_mi_pos: List[List[float]], label_binary_entropies_pos: List[float], alpha: float = 0.5, threshold: float = 0.2, last_threshold: float = 0.2):\n",
    "    label_high_mi_heads = []\n",
    "    binary_label_mi, label_binary_entropies, binary_label_mi_pos, label_binary_entropies_pos = \\\n",
    "        torch.Tensor(binary_label_mi), torch.Tensor(label_binary_entropies), torch.Tensor(binary_label_mi_pos), torch.Tensor(label_binary_entropies_pos)\n",
    "    for label_idx in range(len(binary_label_mi)):\n",
    "        if label_idx != (len(binary_label_mi) - 1):\n",
    "            this_mixed_mi = binary_label_mi_pos[label_idx] * (1 - alpha) + binary_label_mi[label_idx] * alpha\n",
    "            this_mixed_mi_proportions = (binary_label_mi_pos[label_idx] / label_binary_entropies_pos[label_idx]) * (1 - alpha) + (binary_label_mi[label_idx] / label_binary_entropies[label_idx]) * alpha\n",
    "            # breakpoint()\n",
    "        else:\n",
    "            this_mixed_mi = binary_label_mi[label_idx]\n",
    "            this_mixed_mi_proportions = binary_label_mi[label_idx] / label_binary_entropies[label_idx]\n",
    "        # breakpoint()\n",
    "        # print(f'label {label_idx} avg. mi proportion {this_mixed_mi_proportions.mean().item()}')\n",
    "        sorted_heads_and_proportions = sorted(enumerate(this_mixed_mi_proportions.tolist()), key=lambda x: x[1], reverse=True)\n",
    "        # print(f\"label [{label_idx}]{unmasked_label_names[label_idx]}: {[(idx, round(each, 4), round(each / label_binary_entropies[label_idx], 4)) for idx, each in  high_mi_heads_and_mis]}\")\n",
    "        topk_mi_heads = set([idx for idx, mi in sorted_heads_and_proportions[:5]])\n",
    "        high_mi_proportion_heads = set([idx for idx, mi_proportion in sorted_heads_and_proportions if mi_proportion > (threshold if label_idx != (len(binary_label_mi) - 1) else last_threshold)])\n",
    "        label_high_mi_heads.append(topk_mi_heads.union(high_mi_proportion_heads))\n",
    "        # for each in high_mi_heads:\n",
    "    return label_high_mi_heads\n",
    "\n",
    "def get_high_mi_heads_mass(binary_label_mi: List[List[float]], mass_proportion: float = 0.2):\n",
    "    label_high_mi_heads = []\n",
    "    for label_idx, this_binary_label_mi in enumerate(binary_label_mi):\n",
    "        current_mass = 0\n",
    "        mass_max = sum(this_binary_label_mi) * mass_proportion\n",
    "        sorted_mi_heads_and_mis = sorted(enumerate(this_binary_label_mi), key=lambda x: x[1], reverse=True)\n",
    "        this_label_high_mi_heads = []\n",
    "        for idx, mi in sorted_mi_heads_and_mis:\n",
    "            current_mass += mi\n",
    "            this_label_high_mi_heads.append(idx)\n",
    "            if current_mass > mass_max:\n",
    "                break\n",
    "        label_high_mi_heads.append(this_label_high_mi_heads)\n",
    "    \n",
    "    return label_high_mi_heads\n",
    "        \n",
    "# print_listlike([(label_name, len(each)) for label_name, each in zip(unmasked_label_names, label_high_mi_heads)])\n",
    "# threshold:\n",
    "# ('prep', 26) ('det', 20) ('nn', 6) ('num', 5) ('pobj', 61) ('punct', 5) ('poss', 5) ('possessive', 67) ('amod', 26) ('nsubj', 9)\n",
    "# ('appos', 5) ('dobj', 39) ('dep', 5) ('cc', 18) ('conj', 5) ('nsubjpass', 5) ('partmod', 5) ('auxpass', 36) ('advmod', 5) ('root', 968)\n",
    "# ('ccomp', 5) ('aux', 46) ('cop', 5) ('xcomp', 5) ('quantmod', 5) ('tmod', 5) ('neg', 10) ('infmod', 5) ('rcmod', 5) ('pcomp', 5)\n",
    "# ('mark', 5) ('advcl', 5) ('predet', 5) ('csubj', 5) ('mwe', 5) ('parataxis', 5) ('npadvmod', 5) ('number', 17) ('acomp', 5) ('prt', 43)\n",
    "# ('iobj', 22) ('preconj', 5) ('expl', 5) ('discourse', 5) ('csubjpass', 5)\n",
    "# threshold = 0.46 if TRANSPOSE else 0.2\n",
    "# threshold_high_mi_heads = get_high_mi_heads_threshold(binary_label_mi, label_binary_entropies, threshold)\n",
    "# t1_threshold = (Tensor(binary_label_mi) / Tensor(label_binary_entropies).unsqueeze(-1)).flatten().sort(descending=True).values[1700].item()\n",
    "# t2_threshold = (Tensor(binary_label_mi_pos) / Tensor(label_binary_entropies_pos).unsqueeze(-1)).flatten().sort(descending=True).values[1700].item()\n",
    "# for alpha in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "# t1_threshold = 0.133 if TRANSPOSE else 0.2\n",
    "# t2_threshold = 0.133 if TRANSPOSE else 0.2\n",
    "# alpha = 0.5\n",
    "# mixed_threshold = alpha * t1_threshold + (1 - alpha) * t2_threshold\n",
    "# # mixed_threshold = 0.35 if TRANSPOSE else 0.16\n",
    "# t2_threshold = 0.6 #if TRANSPOSE else 0.16\n",
    "_SEARCH_MODE = 'posneg'# if '_SEARCH_MODE' not in os.environ else os.environ['SEARCH_MODE']# 'mixed' or 'posneg'\n",
    "\n",
    "alpha = 0.5\n",
    "threshold_l, threshold_r = 0.1, 0.2\n",
    "threshold_m = (threshold_l + threshold_r) / 2\n",
    "target_num_heads = 2000\n",
    "total_num_high_mi_heads = -1\n",
    "while not abs(total_num_high_mi_heads - target_num_heads) < 5:\n",
    "    if _SEARCH_MODE == 'mixed':\n",
    "        threshold_high_mi_heads = get_high_mi_heads_threshold_mix(binary_label_mi, label_binary_entropies, binary_label_mi_pos, label_binary_entropies_pos, alpha=alpha, threshold=threshold_m, last_threshold=threshold_m)\n",
    "    elif _SEARCH_MODE == 'posneg':\n",
    "        threshold_high_mi_heads = get_high_mi_heads_threshold(binary_label_mi, label_binary_entropies, threshold_m)\n",
    "    # threshold_high_mi_heads = get_high_mi_heads_threshold(binary_label_mi, threshold)\n",
    "    print(f\"trying on threshold {threshold_m}, alpha {alpha}, search mode {_SEARCH_MODE}\")\n",
    "    # print_listlike([(label_name, len(each)) for label_name, each in zip(unmasked_label_names, threshold_high_mi_heads)])\n",
    "    total_num_high_mi_heads = sum([len(each) for each in threshold_high_mi_heads])\n",
    "    no_arc_high_mi_heads = len(threshold_high_mi_heads[-1])\n",
    "    # print(f'heads responsible for no_arc: {(no_arc_high_mi_heads := len(threshold_high_mi_heads[-1]))}')\n",
    "    print(f\"total {total_num_high_mi_heads} ({total_num_high_mi_heads - no_arc_high_mi_heads} + {no_arc_high_mi_heads}) heads\")\n",
    "    if total_num_high_mi_heads < target_num_heads:\n",
    "        threshold_r = threshold_m\n",
    "    else:\n",
    "        threshold_l = threshold_m\n",
    "    \n",
    "    threshold_m = (threshold_l + threshold_r) / 2\n",
    "\n",
    "    # mass = 0.05\n",
    "    # mass_high_mi_heads = get_high_mi_heads_mass(binary_label_mi, mass)\n",
    "    # print(f\"high MI heads for each label type with mass {mass}\")\n",
    "    # print_listlike([(label_name, len(each)) for label_name, each in zip(unmasked_label_names, mass_high_mi_heads)])\n",
    "    # print(f'heads responsible for no_arc: {(no_arc_high_mi_heads := len(mass_high_mi_heads[-1]))}')\n",
    "    # print(f\"total {(total_num_high_mi_heads := sum([len(each) for each in mass_high_mi_heads]))} heads (including {total_num_high_mi_heads - no_arc_high_mi_heads} label heads)\")\n",
    "if BREAK_AFTER_MI:\n",
    "    raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "threshold: 0.6899999976158142, last appended value: 313: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 52.16it/s]\n"
     ]
    }
   ],
   "source": [
    "_PLOT = False\n",
    "if _PLOT:\n",
    "    from matplotlib import pyplot as plt\n",
    "    available_thresholds = torch.arange(0.4, 0.7, 0.01)\n",
    "    n_heads = []\n",
    "    for threshold in (pbar := tqdm(available_thresholds)):\n",
    "        n_heads.append(sum([len(each) for each in get_high_mi_heads_threshold(binary_label_mi, threshold.item())]))\n",
    "        pbar.set_description(f\"threshold: {threshold.item()}, last appended value: {n_heads[-1]}\")\n",
    "    plt.plot(available_thresholds, n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4000, 0.4100, 0.4200, 0.4300, 0.4400, 0.4500, 0.4600, 0.4800, 0.5000,\n",
       "        0.5400, 0.5800, 0.6200, 0.6600, 0.7000])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cat([torch.arange(0.4, 0.46, 0.01), torch.arange(0.46, 0.5, 0.02), torch.arange(0.5, 0.7001, 0.04)]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "baseline"
    ]
   },
   "outputs": [],
   "source": [
    "#papermill_description=LOAD_BASELINES\n",
    "from explaination_baselines import SimpleMLP, IndependentMLP\n",
    "def mask_baseline_matrix(baseline_matrix: Tensor, masked_label_indices: List[int]):\n",
    "    baseline_num_labels = baseline_matrix.shape[0]\n",
    "    baseline_unmasked_label_index = torch.ones(baseline_num_labels).bool()\n",
    "    for idx in range(baseline_num_labels):\n",
    "        if idx in masked_label_indices:\n",
    "            baseline_unmasked_label_index[idx] = False\n",
    "    return baseline_matrix[baseline_unmasked_label_index]\n",
    "\n",
    "load_baselines = False\n",
    "if LOAD_BASELINES:\n",
    "    mean_values_matrix = torch.load(osp.join(kde_save_pth, 'baselines', 'total_mean_values.pt'))\n",
    "    pl_matrix = torch.load(osp.join(kde_save_pth, 'baselines', 'probless_matrix.pt'))\n",
    "    iou_matrix = torch.load(osp.join(kde_save_pth, 'baselines', 'iou_matrix_dynamic_0.995.pt'))\n",
    "    v_information_matrix = torch.load(osp.join(kde_save_pth, 'baselines', 'v_information_h_2_4_leaky_relu_0.0_0.0_balanced.pt')).transpose(-2, -1)\n",
    "    variational_family_state_dict = torch.load(osp.join(kde_save_pth, 'baselines', 'variational_family_mlp_h_2_4_leaky_relu_0.0_0.0_balanced.pt'))\n",
    "    variational_family = IndependentMLP(pos_features.shape[-1], [2, 4], variational_family_state_dict['final_layer'].shape[-1], 'leaky_relu')\n",
    "    variational_family.load_state_dict(variational_family_state_dict)\n",
    "    v_information_matrix /= 47054848 \n",
    "    v_information_matrix = torch.cat([v_information_matrix[2:], v_information_matrix[0].unsqueeze(0)], dim=0)\n",
    "    bounded_v_information_matrix = (torch.ones_like(v_information_matrix) * v_information_matrix[v_information_matrix < 1e12].max() - v_information_matrix).to(pl_matrix.device)\n",
    "    binary_entropies_matrix = torch.Tensor(label_binary_entropies).to(v_information_matrix.device).unsqueeze(-1).expand_as(v_information_matrix)\n",
    "    # v_information_matrix = binary_entropies_matrix - v_information_matrix\n",
    "    elasticnet_state_dict = torch.load(osp.join(kde_save_pth, 'baselines', 'mlp_1e-05_1e-05', 'best_model.pt'), map_location='cpu')\n",
    "    elasticnet_matrix = elasticnet_state_dict['mlp.weight']\n",
    "    # vinf_state_dict = torch.load(osp.join(kde_save_pth, 'baselines'))\n",
    "    deepmlp_state_dict = torch.load(osp.join(kde_save_pth, 'baselines', 'mlp_h_512_128_0.0_0.0.pt'), map_location='cpu')\n",
    "    deepmlp = SimpleMLP(pos_features.shape[-1], [512, 128], deepmlp_state_dict['mlp.2.weight'].shape[0])\n",
    "    deepmlp.load_state_dict(deepmlp_state_dict)\n",
    "    # elasticnet = SimpleMLP(pos_features.shape[-1], [], elasticnet_matrix.shape[0],).to(torch.bfloat16)\n",
    "    # elasticnet.load_state_dict(elasticnet_state_dict)\n",
    "    # vinf = IndependentMLP(pos_features.shape[-1], [], len(unmasked_label_names) + 1, 'leaky_relu')\n",
    "\n",
    "    pl_matrix = mask_baseline_matrix(pl_matrix, masked_label_indices)\n",
    "    iou_matrix = mask_baseline_matrix(iou_matrix, masked_label_indices)\n",
    "    elasticnet_matrix = torch.cat([elasticnet_matrix[2:], elasticnet_matrix[0].unsqueeze(0)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "baseline"
    ]
   },
   "outputs": [],
   "source": [
    "#papermill_description=GET_BASELINE_HEADS\n",
    "# print(Tensor(binary_label_mi).shape)\n",
    "# print(pl_matrix.shape)\n",
    "def get_num_heads(high_corr_head_list):\n",
    "    return sum([len(each) for each in high_corr_head_list])\n",
    "\n",
    "def select_baseline_heads_by_headnum(baseline_matrix: Tensor, high_mi_heads: List[List[int]]):\n",
    "    baseline_high_mi_heads = []\n",
    "    for label_idx, each in enumerate(high_mi_heads):\n",
    "        this_num_heads = len(each)\n",
    "        index_sorted_label_baseline_matrix = sorted([*enumerate(baseline_matrix[label_idx].tolist())], reverse=True, key=lambda x: x[1])\n",
    "        baseline_high_mi_heads.append([each[0] for each in index_sorted_label_baseline_matrix[:this_num_heads]])\n",
    "    return baseline_high_mi_heads\n",
    "\n",
    "def select_baseline_heads_by_threshold(baseline_matrix: Tensor, threshold: float, force_positive: bool = True):\n",
    "    # total_headnum = get_num_heads(high_mi_heads)\n",
    "    # current_threshold = baseline_matrix.flatten().sort(descending=True)[total_headnum].item()\n",
    "    # for _ in range(50): # maximum 50 iters\n",
    "    target_heads = []\n",
    "    for label_idx, this_baseline_corrs in enumerate(baseline_matrix):\n",
    "        # label_high_mi_heads.append([])\n",
    "        sorted_heads_and_corrs = sorted(enumerate(this_baseline_corrs), key=lambda x: x[1], reverse=True)\n",
    "        # print(f\"label [{label_idx}]{unmasked_label_names[label_idx]}: {[(idx, round(each, 4), round(each / label_binary_entropies[label_idx], 4)) for idx, each in  high_mi_heads_and_mis]}\")\n",
    "        if force_positive:\n",
    "            topk_mi_heads = set([idx for idx, corr in sorted_heads_and_corrs[:5] if corr > 0])\n",
    "            high_mi_proportion_heads = set([idx for idx, corr in sorted_heads_and_corrs if corr > threshold and corr > 0])\n",
    "        else:\n",
    "            topk_mi_heads = set([idx for idx, corr in sorted_heads_and_corrs[:5]])\n",
    "            high_mi_proportion_heads = set([idx for idx, corr in sorted_heads_and_corrs if corr > threshold])\n",
    "        \n",
    "        target_heads.append(topk_mi_heads.union(high_mi_proportion_heads))\n",
    "        # if get_num_heads(target_heads) < total_headnum:\n",
    "    return target_heads\n",
    "\n",
    "def get_baseline_grid(baseline_matrix: Tensor, head_proportion: float, num_grid_sets: int):\n",
    "    assert num_grid_sets % 2 == 1, f'only odd number of grid sets are supported, got {num_grid_sets}'\n",
    "    sorted_corr_values = sorted(baseline_matrix.flatten().tolist(), reverse=True)\n",
    "    total_num_heads = baseline_matrix.numel()\n",
    "    middle_threshold = sorted_corr_values[int(head_proportion * total_num_heads)]\n",
    "    loose_threshold = sorted_corr_values[int(3 * head_proportion * total_num_heads)]\n",
    "    max_value = max(sorted_corr_values)\n",
    "    num_grids_per_side = (num_grid_sets - 1) // 2\n",
    "\n",
    "    thresholds = torch.linspace(loose_threshold, middle_threshold, num_grids_per_side + 1).tolist()[:-1] \\\n",
    "        + [middle_threshold] + torch.linspace(middle_threshold, max_value, num_grids_per_side + 2).tolist()[1:-1]\n",
    "    head_sets, num_heads = [], []\n",
    "    for threshold in thresholds:\n",
    "        head_sets.append(select_baseline_heads_by_threshold(baseline_matrix, threshold))\n",
    "        num_heads.append(get_num_heads(head_sets[-1]))\n",
    "\n",
    "    return thresholds, head_sets, num_heads\n",
    "    # sets, thresholds, numheads = [initial_set], [start_threshold], [get_num_heads(initial_set)]\n",
    "    # return [*zip(sets, thresholds, numheads)]\n",
    "# print(get_num_heads(pl_high_mi_heads))\n",
    "# get baselines grid test grids\n",
    "matrix_to_try = bounded_v_information_matrix\n",
    "_GET_THRESHOLD = False\n",
    "if LOAD_BASELINES and _GET_THRESHOLD:\n",
    "    threshold_l, threshold_r = -100, 100\n",
    "    threshold_m = (threshold_l + threshold_r) / 2\n",
    "    target_num_heads = 2000\n",
    "    total_num_high_mi_heads = -1\n",
    "    while not abs(total_num_high_mi_heads - target_num_heads) < 5:\n",
    "        baseline_threshold_high_mi_heads = select_baseline_heads_by_threshold(matrix_to_try, threshold_m, force_positive=False)\n",
    "        total_num_high_mi_heads = get_num_heads(baseline_threshold_high_mi_heads)\n",
    "        print(f\"trying on threshold {threshold_m}\")\n",
    "        print(f\"total {total_num_high_mi_heads} heads (including {len(baseline_threshold_high_mi_heads[-1])} neg heads)\")\n",
    "        if total_num_high_mi_heads < target_num_heads:\n",
    "            threshold_r = threshold_m\n",
    "        else:\n",
    "            threshold_l = threshold_m\n",
    "        \n",
    "        threshold_m = (threshold_l + threshold_r) / 2\n",
    "\n",
    "    # num_high_mi_heads = get_num_heads(threshold_high_mi_heads)\n",
    "    # start_head_proportion = num_high_mi_heads / pl_matrix.numel()\n",
    "    # print(num_high_mi_heads, start_head_proportion)\n",
    "    # pl_thresholds, pl_head_sets, pl_num_heads = get_baseline_grid(pl_matrix, start_head_proportion, 5)\n",
    "    # iou_thresholds, iou_head_sets, iou_num_heads = get_baseline_grid(iou_matrix, start_head_proportion, 5)\n",
    "    # elasticnet_thresholds, elasticnet_head_sets, elasticnet_num_heads = get_baseline_grid(elasticnet_matrix, start_head_proportion, 5)\n",
    "\n",
    "# pl_high_mi_heads = select_baseline_heads_by_mi_heads(pl_matrix, threshold_high_mi_heads)\n",
    "# iou_high_mi_heads = select_baseline_heads_by_mi_heads(iou_matrix, threshold_high_mi_heads)\n",
    "# elasticnet_high_mi_heads = select_baseline_heads_by_mi_heads(elasticnet_matrix, threshold_high_mi_heads)\n",
    "# random_high_mi_heads = select_baseline_heads_by_mi_heads(torch.rand_like(pl_matrix), threshold_high_mi_heads)\n",
    "# pl_high_mi_heads = []\n",
    "# for label_idx, each in enumerate(threshold_high_mi_heads):\n",
    "#     this_num_heads = len(each)\n",
    "#     index_sorted_label_pl_matrix = sorted([*enumerate(pl_matrix[label_idx].tolist())], reverse=True, key=lambda x: x[1])\n",
    "#     pl_high_mi_heads.append([each[0] for each in index_sorted_label_pl_matrix[:this_num_heads]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=INFERENCE_BY_ATTN_FEATURES\n",
    "%autoreload 2\n",
    "import pdb\n",
    "from explaination import batched_inference_by_func\n",
    "from explaination import move_to_device\n",
    "# MODEL_DEVICE = model_llama.device\n",
    "def get_least_used_gpu():\n",
    "    result = GPUStatCollection.new_query()\n",
    "    spare = []\n",
    "    for idx, each in enumerate(result):\n",
    "        spare += [each.memory_total - each.memory_used]\n",
    "    \n",
    "    return spare.index(max(spare))\n",
    "\n",
    "def move_to_device(device: Union[int, str], *tensors):\n",
    "    return [(each.to(device) if each is not None else each) for each in tensors]\n",
    "\n",
    "\n",
    "MODEL_DEVICE = f'cuda:{get_least_used_gpu()}'\n",
    "\n",
    "val_data_llama = dep_parser_llama.load_data('./data/dev.conllu')\n",
    "val_data_llama = sorted(val_data_llama, key=lambda instance: len(instance.head), reverse=True)\n",
    "\n",
    "dl_val_llama = DataLoader(val_data_llama, collate_fn=lambda x: dep_parser_llama.feature2input(MODEL_DEVICE, dep_parser_llama.convert_examples_to_features(x)), batch_size=1)\n",
    "\n",
    "@torch.no_grad\n",
    "def infer_by_attn_features(\n",
    "    tok: Union[LlamaTokenizer, BertTokenizer], model: Union[LlamaModel, BertModel], dataloader: DataLoader, labelmap_convert: Dict[int, int],\n",
    "    x: Tensor,\n",
    "    conditional_probabs: Union[List[Tuple[Tensor, Tensor]], Tensor], \n",
    "    mutual_informations: List[float],\n",
    "    label_conditional_probabs: Union[List[List[Tensor]], Tensor] = None,\n",
    "    label_mutual_informations: List[List[float]] = None,\n",
    "    num_samples: int = 100, with_labels: bool = False, silent: bool = True, use_weighted_mi: bool = True, high_mi_heads: List[List[int]] = None, \n",
    "    no_pbar: bool = False, transpose: bool = False,\n",
    "    include_neg_possibilities: bool = False,\n",
    "    infer_method: Literal['kde', 'score', 'mlp', 'variational_family'] = 'kde',\n",
    "    infer_model: torch.nn.Module = None,\n",
    "    \n",
    ") -> Tuple[\n",
    "        Tuple[List[Tensor], List[Tensor]], Dict[str, List[Tensor]]\n",
    "    ]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tok: tokenizer\n",
    "        model: model\n",
    "        dataloader: DataLoader\n",
    "        labelmap_convert (Dict[int, int]): labelmap converting the labelmap of `label_conditional_probabs` to the labelmap of the dataset\n",
    "        x (Tensor[n_x]): the domain of defination of conditional_probabs\n",
    "        conditional_probabs: `List[n_heads, 2] of Tensor[n_x]` or `Tensor[n_heads, 2, n_x]`:\n",
    "            [head1: [pos_conditional_probab_on_head_1, neg_conditional_probab_on_head_1],\n",
    "            ...\n",
    "            [headh: [pos_conditional_probab_on_head_h, neg_conditional_probab_on_head_h],\n",
    "        mutual_informations: `List[n_heads]` [mi_1, mi_2, ...]\n",
    "        label_conditional_probabs: `List[n_heads, n_labels] of Tensor[n_x]` or `Tensor[n_heads, n_labels, n_x]`: \n",
    "        [\n",
    "            head1: [label1_conditional_probab_on_head_1, label2_conditional_probab_on_head_1, ...], \n",
    "            ...,\n",
    "            headh: [label1_conditional_probab_on_head_h, label2_conditional_probab_on_head_h, ...]\n",
    "        ]\n",
    "        label_mutual_informations: `List[n_labels, n_heads]`: [\n",
    "        label1_mi_for_all_heads, \n",
    "        label2_mi_for_all_heads, \n",
    "        ...]\n",
    "        num_samples: number of samples to extract\n",
    "        with_labels: whether to extract label features\n",
    "        silent: whether to print debug info\n",
    "        mi_threshold: threshold for mutual information to consider, used when inferring by arcs\n",
    "        high_mi_heads: `List[n_labels]` of List[(int)]\n",
    "            heads with high mutual information to consider, used when inferring by labels (`with_labels=True`)\n",
    "    returns \n",
    "    \"\"\"\n",
    "    tok.pad_token = tok.eos_token\n",
    "    torch.backends.cuda.enable_flash_sdp(True)\n",
    "    torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "    num_heads = len(mutual_informations)\n",
    "    # assert `conditional_probabs` have shape [n_heads, 2, n_x]\n",
    "    if isinstance(conditional_probabs, Tensor):\n",
    "        assert len(conditional_probabs.shape) == 3 and conditional_probabs.shape[1] == 2, f\"conditional_probabs must be in shape [n_heads, 2, n_x]\"\n",
    "    else:\n",
    "        assert len(conditional_probabs) == num_heads, f\"conditional_probabs must have the same n_heads (=len(mutual_informations({num_heads})))\"\n",
    "        assert all([len(each) == 2 for each in conditional_probabs]), \"conditional_probabs must be in shape [n_heads, 2, n_x]\"\n",
    "        for head_idx, (pos_conditional_probab, neg_conditional_probab) in enumerate(conditional_probabs):\n",
    "            assert len(pos_conditional_probab.shape) == 1 and len(neg_conditional_probab.shape) == 1, f\"pos_conditional_probab and neg_conditional_probab must be in shape [n_x]\"\n",
    "        # stack conditional_probabs to Tensor[n_heads, 2, n_x]\n",
    "        conditional_probabs = torch.stack([torch.stack([pos_conditional_probab, neg_conditional_probab]) for pos_conditional_probab, neg_conditional_probab in conditional_probabs])\n",
    "    # assert `label_conditional_probabs` has shape [n_heads, n_labels, n_x]\n",
    "    if with_labels:\n",
    "        assert label_mutual_informations is not None and label_conditional_probabs is not None, \\\n",
    "            \"label_mutual_informations and label_conditional_probabs must be provided if with_labels is True\"\n",
    "        num_labels = len(label_mutual_informations)\n",
    "        if isinstance(label_conditional_probabs, Tensor):\n",
    "            assert len(label_conditional_probabs.shape) == 3 and label_conditional_probabs.shape[1] == num_labels, \\\n",
    "                f\"label_conditional_probabs must be in shape [n_heads, {num_labels}(=len(label_mutual_informations), n_x]\"\n",
    "        else:\n",
    "            assert len(label_conditional_probabs) == len(mutual_informations), \"label_conditional_probabs must have the same length as mutual_informations\"\n",
    "            for head_idx, label_conditional_probab in enumerate(label_conditional_probabs):\n",
    "                assert len(label_conditional_probab) == len(label_mutual_informations), f\"each entry of label_conditional_probabs must have the same length as num_labels\"\n",
    "                for label_idx, label_probab in enumerate(label_conditional_probab):\n",
    "                    assert len(label_probab.shape) == 1, f\"label_probab must be in shape [n_x]\"\n",
    "            # stack label_conditional_probabs to Tensor[n_heads, num_labels, n_x]\n",
    "            label_conditional_probabs = torch.stack([torch.stack(label_conditional_probab) for label_conditional_probab in label_conditional_probabs])\n",
    "    \n",
    "    if high_mi_heads is not None:\n",
    "        if with_labels:\n",
    "            num_labels = len(label_mutual_informations)\n",
    "            assert len(high_mi_heads) == num_labels, \"high_mi_heads must have the same length as label_mutual_informations\"\n",
    "            # print(f'high_mi_heads: {high_mi_heads} will supress mi_threshold ({mi_threshold})')\n",
    "    \n",
    "    # if infer_method == 'model' and not include_neg_possibilities:\n",
    "    #     raise AssertionError(\"inference method of `model` requires include_neg_possibilities to be True\")\n",
    "    \n",
    "    labelmap_reverse_convert = {v: k for k, v in labelmap_convert.items()} # convert the labelmap of the dataset to the labelmap of `label_conditional_probabs`\n",
    "\n",
    "    if num_samples == -1:\n",
    "        num_samples = len(dataloader)\n",
    "    n_corrects, correct_labels, gt_correct_labels, totals = 0, 0, 0, 0\n",
    "    label_corrects, label_totals = defaultdict(int), defaultdict(int)\n",
    "    matrix_corrects, matrix_totals = 0, 0\n",
    "\n",
    "    high_mi_heads_flatten = []\n",
    "    label_indicator = []\n",
    "    selected_conditional_probabs = []\n",
    "    weights = [0] * (num_labels if include_neg_possibilities else (num_labels - 1))\n",
    "    weights_flatten = []\n",
    "    label_conditional_probabs_T = label_conditional_probabs.transpose(0, 1)\n",
    "    for label_idx, label_high_mi_heads in enumerate(high_mi_heads if include_neg_possibilities else high_mi_heads[:-1]):\n",
    "        high_mi_heads_flatten.extend(list(label_high_mi_heads))\n",
    "        label_indicator.extend([label_idx] * len(label_high_mi_heads))\n",
    "        weights[label_idx] += sum([label_mutual_informations[label_idx][head_idx] for head_idx in label_high_mi_heads])\n",
    "        weights_flatten.extend([label_mutual_informations[label_idx][head_idx] for head_idx in label_high_mi_heads])\n",
    "        selected_conditional_probabs.append(label_conditional_probabs_T[label_idx][torch.tensor(list(label_high_mi_heads))])\n",
    "\n",
    "    high_mi_heads_flatten = torch.LongTensor(high_mi_heads_flatten) # [sum(label1_hi_mi_head, label2_hi_mi_head, ...)]\n",
    "    label_indicator = torch.LongTensor(label_indicator) # [0, 0, 0, 0, 0, ... 1, 1, 1, 1, 1, ...]\n",
    "    weights_flatten = torch.Tensor(weights_flatten) # [sum(label1_hi_mi_head, label2_hi_mi_head, ...)]\n",
    "    \n",
    "    selected_conditional_probabs = torch.cat(selected_conditional_probabs, dim=0).to(MODEL_DEVICE) # Tensor[sum(label1_hi_mi_head, label2_hi_mi_head, ...), n_x]\n",
    "\n",
    "    num_high_mi_heads = selected_conditional_probabs.shape[0]\n",
    "    expanded_x = x.unsqueeze(0).expand(num_high_mi_heads, -1).to(MODEL_DEVICE) # \n",
    "\n",
    "\n",
    "    model_cpu = deepcopy(model.cpu())\n",
    "    model_cuda = model.to(MODEL_DEVICE)\n",
    "    arcs_to_save, labels_to_save, results_to_save, probab_labels_to_save, attn_scores_to_save  = [], [], [], [], []\n",
    "\n",
    "    for data_idx, (input_ids, input_attention_mask, label_mask, eval_mask, \\\n",
    "        arcs, rels, word_ids, pos_ids, ngram_ids, \\\n",
    "        ngram_positions, segment_ids, valid_ids) in enumerate(pbar := (dataloader if no_pbar else tqdm(dataloader, total=num_samples, desc='extracting attentions', mininterval=2))):\n",
    "\n",
    "        echo = not silent and data_idx == 0\n",
    "        if echo:\n",
    "            print(input_ids.shape, label_mask.shape, valid_ids.shape, arcs.shape, rels.shape)\n",
    "        if input_ids.shape[-1] > (128 if infer_method != 'variational_family' else 100): # move long samples to cpu to save GPU memory\n",
    "            model = model_cpu\n",
    "            input_ids, input_attention_mask, label_mask, eval_mask, arcs, rels, word_ids, pos_ids, ngram_ids, ngram_positions, segment_ids, valid_ids = \\\n",
    "                move_to_device('cpu', input_ids, input_attention_mask, label_mask, eval_mask, arcs, rels, word_ids, pos_ids, ngram_ids, ngram_positions, segment_ids, valid_ids)\n",
    "        elif model.device != MODEL_DEVICE:\n",
    "            model = model_cuda\n",
    "\n",
    "        batch_size, S = input_ids.shape\n",
    "        sequence_lengths = input_attention_mask.sum(1).tolist()\n",
    "        text_lengths = [valid_ids[i, :sequence_lengths[i]].sum().item() for i in range(batch_size)]\n",
    "        w2s = [] # the idx at position i is whole-word i's last subword's idx\n",
    "        for sample_idx, valid_id in enumerate(valid_ids):\n",
    "            w2s.append([])\n",
    "            for subword_idx, each in enumerate(valid_id.tolist()):\n",
    "                if each == 1:\n",
    "                    w2s[-1].append(subword_idx)\n",
    "\n",
    "        arc_adj_matrix = torch.zeros(batch_size, S, S).to(model.device) # 1 for `have arc`, 0 for `no arc` (marking at whole-word's last subword)\n",
    "        label_adj_matrix = torch.zeros(batch_size, S, S).to(model.device) # arc[i][j]'s relation type (marking at whole-word's last subword)\n",
    "        for sample_idx in range(batch_size):\n",
    "            for word_idx, head_idx in enumerate(arcs[sample_idx]):\n",
    "                if head_idx != -1:\n",
    "                    arc_adj_matrix[sample_idx][w2s[sample_idx][word_idx]][w2s[sample_idx][head_idx]] = 1\n",
    "                    label_adj_matrix[sample_idx][w2s[sample_idx][word_idx]][w2s[sample_idx][head_idx]] = rels[sample_idx][word_idx]\n",
    "\n",
    "        if isinstance(model, LlamaModel):\n",
    "            res = model.forward(\n",
    "                input_ids=input_ids, attention_mask=input_attention_mask,\n",
    "                output_hidden_states=True,\n",
    "                output_attention_queries=True\n",
    "            )\n",
    "            key_values, queries = res.past_key_values, res.queries\n",
    "            # kv: [num_layers, 2(k and v), batch_size, num_heads, sequence_length, head_dim], q: [num_layers, batch_size, num_heads, sequence_length, head_dim]\n",
    "            if echo:\n",
    "                print('past_key_values shape:', get_shape(res.past_key_values))\n",
    "                print('queries shape:', get_shape(res.queries))\n",
    "\n",
    "            attn_scores = () # [num_layers, 1(batch_size), num_heads, seq_len, seq_len]\n",
    "            for layer_idx in range(len(model.layers)):\n",
    "                k = key_values[layer_idx][0] # [batch_size, num_heads, sequence_length, head_dim]\n",
    "                q = queries[layer_idx] # [batch_size, num_heads, sequence_length, head_dim]\n",
    "                if q.shape[-3] != k.shape[-3]: # num_q_heads == num_k_heads * n_groups\n",
    "                    n_groups = q.shape[-3] / k.shape[-3]\n",
    "                    assert n_groups == int(n_groups)\n",
    "                    n_groups = int(n_groups)\n",
    "                else:\n",
    "                    n_groups = 1\n",
    "                \n",
    "                k = torch.repeat_interleave(k, n_groups, -3) \n",
    "                # [batch_size, num_heads, sequence_length, head_dim] -> [batch_size, num_heads * num_groups, sequence_length, head_dim]\n",
    "                this_attn_score = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(res.last_hidden_state.shape[-1]) # scaled dot-product attention\n",
    "                # [batch_size, num_heads, sequence_length, sequence_length]\n",
    "                if infer_method == 'score':\n",
    "                    this_attn_score = this_attn_score.softmax(dim=-2 if transpose else -1)\n",
    "                attn_scores += (this_attn_score.transpose(-2, -1) if transpose else this_attn_score,)\n",
    "                # if infer_method == 'score':\n",
    "                #     attn_scores[-1] = torch.nn.functional.softmax(attn_scores[-1], dim=-1)\n",
    "                # breakpoint()\n",
    "            attn_scores = torch.cat(attn_scores, dim=1) # [batch_size, num_heads * num_layers, sequence_length, sequence_length]\n",
    "\n",
    "        else:\n",
    "            res = model.forward(\n",
    "                input_ids=input_ids, attention_mask=input_attention_mask,\n",
    "                output_raw_attentions=True\n",
    "            )\n",
    "            attn_scores = torch.stack(res.attentions) # [num_layers, batch_size, num_heads, seq_len, seq_len]\n",
    "            attn_scores = attn_scores.transpose(0, 1) # [batch_size, num_layers, num_heads, seq_len, seq_len]\n",
    "            B, L, H, S, _ = attn_scores.shape\n",
    "            attn_scores = attn_scores.reshape(B, L * H, S, S)\n",
    "            if transpose:\n",
    "                attn_scores = attn_scores.transpose(-2, -1)\n",
    "        \n",
    "        if echo:\n",
    "            print('attn_scores shape:', get_shape(attn_scores))\n",
    "        # mask attn scores\n",
    "        for sample_idx in range(batch_size):\n",
    "            attn_scores[sample_idx, :, sequence_lengths[sample_idx]:, sequence_lengths[sample_idx]:] = -torch.inf\n",
    "        # if echo:\n",
    "        #     print('attn_score after squeezing:', get_shape(attn_scores))\n",
    "        # attn_features = attn_scores.permute(1, 2, 0) # [sequence_length, sequence_length, num_heads * num_layers]\n",
    "        # attn_features = attn_scores\n",
    "        # probab = torch.zeros_like(attn_scores).to(model.device)\n",
    "\n",
    "        # if with_labels:\n",
    "        #     probab_labels = torch.zeros(batch_size, S, S, num_labels - 1).to(model.device)\n",
    "\n",
    "        B, H, S, _ = attn_scores.shape\n",
    "        if infer_method in ['kde', 'score', 'variational_family']:\n",
    "            if infer_method == 'variational_family':\n",
    "                infer_model = infer_model.to(model.device)\n",
    "                assert B == 1, f'only support batch_size == 1, got `{B}`'\n",
    "                v_probab = infer_model(attn_scores.view(B * H, S * S).transpose(-2, -1).float()).squeeze(-2).permute(1, 0, 2).view(H, S, S, -1).sigmoid() # H, S, S, n_labels\n",
    "                v_probab = torch.cat([v_probab[:, :, :, 2:], v_probab[:, :, :, 0].unsqueeze(-1)], dim=-1)\n",
    "                # breakpoint()\n",
    "                selected_v_all_probab = torch.index_select(v_probab, 0, high_mi_heads_flatten.to(model.device)).view(num_high_mi_heads, S * S, -1)\n",
    "                selected_v_probab = torch.gather(selected_v_all_probab, -1, label_indicator.to(model.device).unsqueeze(-1).unsqueeze(-1).expand(num_high_mi_heads, S * S, 1)).squeeze(-1).unsqueeze(0)\n",
    "            else:\n",
    "                label_high_mi_head_attn_scores = torch.index_select(attn_scores, 1, high_mi_heads_flatten.to(model.device),).view(batch_size, num_high_mi_heads, S * S)\n",
    "\n",
    "            if infer_method == 'kde':\n",
    "                batched_conditional_probabs = torch.log2(batched_inference_by_func(\n",
    "                    label_high_mi_head_attn_scores.to(model.device), \n",
    "                    expanded_x.to(model.device).unsqueeze(0).expand(batch_size, *expanded_x.shape), \n",
    "                    selected_conditional_probabs.to(model.device).unsqueeze(0).expand(batch_size, *selected_conditional_probabs.shape)))\n",
    "            elif infer_method == 'score': # replace the original conditional probab with \n",
    "                batched_conditional_probabs = torch.log2(label_high_mi_head_attn_scores.float())\n",
    "            elif infer_method == 'variational_family':\n",
    "                batched_conditional_probabs = torch.log2(selected_v_probab.float())\n",
    "\n",
    "        \n",
    "            batched_conditional_probabs *= weights_flatten.to(model.device).unsqueeze(0).unsqueeze(-1).expand(batch_size, -1, S * S)\n",
    "\n",
    "            probab_labels = torch.zeros(batch_size, num_labels if include_neg_possibilities else (num_labels - 1), S * S).to(model.device)\n",
    "\n",
    "            # print(batched_conditional_probabs.shape, probab_labels.shape, max(label_indicator))\n",
    "            probab_labels.scatter_reduce_(1, label_indicator.to(model.device).unsqueeze(0).unsqueeze(-1).expand(batch_size, -1, S * S), batched_conditional_probabs.to(model.device), 'sum')\n",
    "            probab_labels = probab_labels.transpose(-1, -2).reshape(batch_size, S, S, num_labels if include_neg_possibilities else (num_labels - 1))\n",
    "        elif infer_method == 'mlp':\n",
    "            assert batch_size == 1, f'only support batch_size == 1, got `{batch_size}`'\n",
    "            infer_model = infer_model.to(model.device).to(model.dtype)\n",
    "            probab_labels = infer_model(attn_scores.view(H, S * S).transpose(-2, -1)).view(1, S, S, -1)\n",
    "            probab_labels = torch.cat([probab_labels[:, :, :, 2:], probab_labels[:, :, :, 0].unsqueeze(-1)], dim=-1).softmax(dim=-1)\n",
    "        else:\n",
    "            raise NotImplementedError(f'`infer_method` {infer_method} is not implemented')\n",
    "\n",
    "        # elif infer_method == 'score':\n",
    "        #     probab_labels = torch.zeros(batch_size, num_labels if include_neg_possibilities else (num_labels - 1), S * S).to(model.device)\n",
    "        #     label_high_mi_head_attn_scores = torch.index_select(attn_scores, 1, high_mi_heads_flatten.to(model.device),).view(batch_size, num_high_mi_heads, S * S)\n",
    "        #     probab_labels.scatter_reduce_(1, label_indicator.to(model.device).unsqueeze(0).unsqueeze(-1).expand(batch_size, -1, S * S), label_high_mi_head_attn_scores.to(model.device), 'sum')\n",
    "        #     probab_labels = probab_labels.transpose(-1, -2).reshape(batch_size, S, S, num_labels if include_neg_possibilities else (num_labels - 1))\n",
    "        \n",
    "        # elif infer_method == 'model':\n",
    "        #     if next(infer_model.parameters()).device != attn_scores.device:\n",
    "        #         infer_model = infer_model.to(attn_scores.device)\n",
    "        #     probab_labels = infer_model(attn_scores.view(batch_size, S * S, -1)).view(batch_size, S, S, num_labels + 1)\n",
    "        #     probab_labels = torch.cat([probab_labels[:, :, :, 2:], probab_labels[:, :, :, 0].unsqueeze(-1)], dim=-1)\n",
    "        #     probab_labels = torch.nn.functional.softmax(probab_labels, dim=-1)\n",
    "\n",
    "        # for feature_idx in range(attn_scores.shape[1]):\n",
    "        #     if with_labels:\n",
    "        #         for label_idx, label_probab in enumerate(label_conditional_probabs[feature_idx]):\n",
    "        #             if label_idx == num_labels - 1: # the last label stands for no_arc\n",
    "        #                 continue\n",
    "        #             if feature_idx not in high_mi_heads[label_idx]:\n",
    "        #                 continue\n",
    "        #             weight = label_mutual_informations[label_idx][feature_idx] if use_weighted_mi else 1.0  # Weight by MI if specified\n",
    "        #             for sample_idx in range(batch_size):\n",
    "        #                 probab_labels[sample_idx, :, :, label_idx] += weight * torch.log2(\n",
    "        #                     inference_by_func(\n",
    "        #                         attn_scores[sample_idx, feature_idx].flatten().to(model.device), \n",
    "        #                         x.to(model.device), label_probab.flatten().to(model.device)\n",
    "        #                     ).view(S, S), \n",
    "        #                 )\n",
    "        #             weights[label_idx] += weight\n",
    "        #             # neg_probab_labels[:, :, label_idx] += weight * torch.log2(\n",
    "        #             #     inference_by_func(\n",
    "        #             #         attn_scores[feature_idx].flatten().to(MODEL_DEVICE), x.to(MODEL_DEVICE), label_probab.flatten().to(MODEL_DEVICE)\n",
    "        #             #     ).view(S, S)\n",
    "        #             # )\n",
    "        #         # normalize probab_labels\n",
    "        #     else: # only condition on arcs\n",
    "        #         raise NotImplementedError\n",
    "        #         if mutual_informations[feature_idx] < mi_threshold:\n",
    "        #             continue\n",
    "        #         probab += mutual_informations[feature_idx] * torch.log2(\n",
    "        #             inference_by_func(\n",
    "        #                 attn_scores[feature_idx].flatten().to(model.device), x.to(model.device), conditional_probabs[feature_idx][0].flatten().to(model.device)\n",
    "        #             ).view(*attn_scores.shape[1:])\n",
    "        #         )\n",
    "        # end for feature_idx\n",
    "        label_mask[:, 0] = 0\n",
    "        # print(valid_ids.device)\n",
    "        if with_labels:\n",
    "            for label_idx in range(num_labels if include_neg_possibilities else (num_labels - 1)):\n",
    "                if infer_method != 'mlp':\n",
    "                    probab_labels[:, :, :, label_idx] /= weights[label_idx]\n",
    "            # probab_labels.exp_()\n",
    "            # BEGIN: multiply with other negative probabilities\n",
    "            if include_neg_possibilities and infer_method != 'mlp':\n",
    "                probab_labels_neg = torch.log2(1 - probab_labels.exp())\n",
    "                probab_labels_neg_summation = probab_labels_neg.sum(dim=-1, keepdim=True).expand_as(probab_labels_neg).clone()\n",
    "                probab_labels_neg_summation -= probab_labels_neg\n",
    "                probab_labels += probab_labels_neg_summation\n",
    "            # END: multiply with other negative probabilities\n",
    "            \n",
    "            probab_labels_pooled, labels_with_max_probab = (probab_labels[:, :, :, :-1] if include_neg_possibilities else probab_labels).max(dim=-1, )\n",
    "            # probab_labels_pooled.exp_() # [batch_size, n_tokens, n_tokens]\n",
    "            for sample_idx in range(batch_size):\n",
    "                length_mask = (torch.arange(S, device=input_ids.device) < sequence_lengths[sample_idx])\n",
    "                this_probab_labels = probab_labels[sample_idx][(valid_ids[sample_idx] == 1) & length_mask][:, (valid_ids[sample_idx] == 1) & length_mask] # [n_words, n_words, n_labels - 1]\n",
    "                this_probab_labels_pooled = probab_labels_pooled[sample_idx][(valid_ids[sample_idx] == 1) & length_mask][:, (valid_ids[sample_idx] == 1) & length_mask] # [n_words, n_words]\n",
    "                # [n_tokens, n_tokens] -> [n_words, n_words]\n",
    "                this_labels_with_max_probab = labels_with_max_probab[sample_idx][(valid_ids[sample_idx] == 1) & length_mask][:, (valid_ids[sample_idx] == 1) & length_mask]\n",
    "\n",
    "                result = eisner(this_probab_labels_pooled.unsqueeze(0), label_mask[sample_idx].unsqueeze(0)) # [1, n_words]\n",
    "                this_arcs = arcs[sample_idx][:text_lengths[sample_idx]]\n",
    "                arcs_to_save.append(this_arcs.cpu())\n",
    "                results_to_save.append(result[0].cpu())\n",
    "                correct_mask = (result[0] == this_arcs)[eval_mask[sample_idx][:text_lengths[sample_idx]]]\n",
    "                n_corrects += correct_mask.sum().item()\n",
    "                totals += correct_mask.shape[0]\n",
    "                # totals += eval_mask[sample_idx][:text_lengths[sample_idx]].sum()\n",
    "                result_labels = Tensor([labelmap_convert[this_labels_with_max_probab[token_idx][head_idx].item()] \\\n",
    "                    for token_idx, head_idx in enumerate(result[0].tolist())]).to(result.device,) # [S]\n",
    "                gt_labels = rels[sample_idx, :text_lengths[sample_idx]] # [S]\n",
    "                correct_labels += (result_labels == gt_labels)[eval_mask[sample_idx][:text_lengths[sample_idx]]][correct_mask].sum().item()\n",
    "                gt_labels_converted = [(labelmap_reverse_convert[label] if label in labelmap_reverse_convert else -1) for label in gt_labels.tolist()] # gt_labels' corresponding unmasked label indices\n",
    "                labels_to_save.append(gt_labels_converted)\n",
    "                gt_arcs_predicted_labels = Tensor([labelmap_convert[this_labels_with_max_probab[token_idx][head_idx].item()] \\\n",
    "                    for token_idx, head_idx in enumerate(this_arcs.tolist())]).to(result.device,) # [S]\n",
    "                gt_arcs_probabilities = torch.stack([this_probab_labels[token_idx][head_idx] \\\n",
    "                    for token_idx, head_idx in enumerate(this_arcs.tolist())]).to(result.device,)\n",
    "                probab_labels_to_save.append(this_probab_labels.cpu())\n",
    "                gt_correct_label_mask = (gt_arcs_predicted_labels == gt_labels)\n",
    "                gt_correct_labels += gt_correct_label_mask[eval_mask[sample_idx, :text_lengths[sample_idx]]].sum().item()\n",
    "                for label_idx in range(gt_labels.max().item() + 1):\n",
    "                    label_corrects[label_idx] += gt_correct_label_mask[gt_labels == label_idx].sum().item()\n",
    "                    label_totals[label_idx] += (gt_labels == label_idx).sum().item()\n",
    "                \n",
    "                # matrix level (including negative samples) ! WARNING, those code are not guaranteed to run when `batch_size > 1`\n",
    "                gt_matrix = torch.ones_like(this_labels_with_max_probab) * (label_conditional_probabs.shape[1] - 1)\n",
    "\n",
    "                for idx in range(len(gt_labels_converted)):\n",
    "                    if this_arcs[idx] != -1:\n",
    "                        gt_matrix[idx][this_arcs[idx]] = gt_labels_converted[idx]\n",
    "                gt_matrix_mask = (gt_matrix != -1)\n",
    "                matrix_correct_mask = (gt_matrix == this_labels_with_max_probab)[gt_matrix_mask]\n",
    "                matrix_corrects += matrix_correct_mask.sum().item()\n",
    "                matrix_totals += matrix_correct_mask.numel()\n",
    "                # print(result)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            result = eisner(probab[valid_ids[0] == 1][:, valid_ids[0] == 1].unsqueeze(0), label_mask)\n",
    "        # print(result, result.shape)\n",
    "        # print(arcs, arcs.shape)\n",
    "        # print(eval_mask, eval_mask.shape)\n",
    "        if data_idx >= num_samples - 1:\n",
    "            break\n",
    "\n",
    "        current_result = {\"metrics\": {\"UAS\": round(n_corrects / totals * 100, 2), \"LAS\": round(correct_labels / totals * 100, 2), \"GTLAS\": round(gt_correct_labels / totals * 100, 2), \"MLAS\": round(matrix_corrects / matrix_totals * 100, 2)}, \n",
    "                          \"label_corrects\": label_corrects, \"label_totals\": label_totals, \"arcs\": arcs_to_save, \"labels\": labels_to_save, \"results\": results_to_save, \"probab_labels\": probab_labels_to_save}\n",
    "        if not no_pbar:\n",
    "            pbar.set_postfix({**current_result['metrics'], 'len': input_ids.shape[-1]})\n",
    "\n",
    "    return current_result\n",
    "\n",
    "# if \"CUDA_LAUNCH_BLOCKING\" in os.environ:\n",
    "#     os.environ.pop(\"CUDA_LAUNCH_BLOCKING\")\n",
    "torch.cuda.empty_cache()\n",
    "# results = infer_by_attn_features(tok_llama, model_llama.to(MODEL_DEVICE), dl_val_llama, {unmasked_label_idx: dep_parser_llama.labelmap[label_name] for unmasked_label_idx, label_name in enumerate(unmasked_label_names)}, x, [*zip(conditional_probab_pos, conditional_probab_neg)], arc_mi, conditional_probab_label, binary_label_mi, -1, with_labels=True, high_mi_heads=threshold_high_mi_heads, no_pbar=False)\n",
    "# %lprun -f infer_by_attn_features \n",
    "_INFER_MODE = 'control'\n",
    "_BASELINE_NAME = 'v_information'\n",
    "print(f'{_INFER_MODE =}')\n",
    "os.makedirs('./inference_results', exist_ok=True)\n",
    "if _INFER_MODE == 'mixed_grid':\n",
    "    # for mixed_threshold in [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5] if TRANSPOSE else [0.1, 0.13, 0.16, ]:\n",
    "    # mixed_threshold = 0.133 if TRANSPOSE else 0.15\n",
    "    # last_threshold = 0.133 if TRANSPOSE else 0.15\n",
    "    threshold = 0.133 if TRANSPOSE else 0.1432\n",
    "    # alpha = 0.5\n",
    "    results = []\n",
    "    for alpha in [0.5]:#[0.8, 0.7]:#, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]:\n",
    "        print(f'doing experiment on {alpha=}, {threshold=}')\n",
    "        threshold_high_mi_heads = get_high_mi_heads_threshold_mix(binary_label_mi, label_binary_entropies, binary_label_mi_pos, label_binary_entropies_pos, alpha=alpha, threshold=threshold, last_threshold=threshold)\n",
    "        print(f'total {sum([len(each) for each in threshold_high_mi_heads])} heads (including {len(threshold_high_mi_heads[-1])} neg heads)')\n",
    "        inference_result = infer_by_attn_features(tok_llama, model_llama.to(MODEL_DEVICE), dl_val_llama, {unmasked_label_idx: dep_parser_llama.labelmap[label_name] for unmasked_label_idx, label_name in enumerate(unmasked_label_names)}, x, [*zip(conditional_probab_pos, conditional_probab_neg)], arc_mi, conditional_probab_label, (torch.Tensor(binary_label_mi[:-1]) * alpha + torch.Tensor(binary_label_mi_pos) * (1 - alpha)).tolist() + [binary_label_mi[-1]], -1, with_labels=True, high_mi_heads=threshold_high_mi_heads, no_pbar=False, transpose=TRANSPOSE, include_neg_possibilities=True)\n",
    "        with open(f\"./inference_results/mixed_alpha_{alpha}_t1_{threshold}_t2_{threshold}{'_transpose' if TRANSPOSE else ''}.pkl\", 'wb') as f:\n",
    "            pkl.dump({k: v for k, v in inference_result.items() if k in ['metrics', 'label_corrects', 'label_totals']}, f)\n",
    "        labelmap_pth = f\"./inference_results/sample_labelmap.json\"\n",
    "        if not osp.exists(labelmap_pth):\n",
    "            with open(labelmap_pth, 'w') as f:\n",
    "                json.dump(dep_parser_llama.labelmap, f)\n",
    "        results.append(inference_result)\n",
    "elif _INFER_MODE == 'control':\n",
    "    n_labels_from_mi, n_heads_from_mi = len(binary_label_mi), len(binary_label_mi[0])\n",
    "    control_heads = [rd.choices(range(n_heads_from_mi), k=2000 // n_labels_from_mi)]\n",
    "    inference_result = infer_by_attn_features(tok_llama, model_llama.to(MODEL_DEVICE), dl_val_llama, {unmasked_label_idx: dep_parser_llama.labelmap[label_name] for unmasked_label_idx, label_name in enumerate(unmasked_label_names)}, x, [*zip(conditional_probab_pos, conditional_probab_neg)], arc_mi, conditional_probab_label, torch.ones_like(torch.Tensor(binary_label_mi)), -1, with_labels=True, high_mi_heads=control_heads, no_pbar=False, transpose=TRANSPOSE, include_neg_possibilities=True)\n",
    "elif _INFER_MODE == 'posneg_single':\n",
    "    threshold_high_mi_heads = get_high_mi_heads_threshold(binary_label_mi, label_binary_entropies, 0.15 if TRANSPOSE else 0.1734)\n",
    "    print(f'total {sum([len(each) for each in threshold_high_mi_heads])} heads (including {len(threshold_high_mi_heads[-1])} neg heads)')\n",
    "    inference_result = infer_by_attn_features(tok_llama, model_llama.to(MODEL_DEVICE), dl_val_llama, {unmasked_label_idx: dep_parser_llama.labelmap[label_name] for unmasked_label_idx, label_name in enumerate(unmasked_label_names)}, x, [*zip(conditional_probab_pos, conditional_probab_neg)], arc_mi, conditional_probab_label, binary_label_mi, -1, with_labels=True, high_mi_heads=threshold_high_mi_heads, no_pbar=False, transpose=TRANSPOSE, include_neg_possibilities=True)\n",
    "# elif _INFER_MODE == 'posneg_threshold_grid':\n",
    "#     metrics = []\n",
    "#     threshold_grid = torch.cat([torch.arange(0.4, 0.46, 0.01), torch.arange(0.46, 0.5, 0.02), torch.arange(0.5, 0.7001, 0.04)]).tolist() if TRANSPOSE else [0.2]\n",
    "#     for tidx, threshold in enumerate(threshold_grid):\n",
    "#         for include_neg_possibilities in [False, True]:\n",
    "#             print(f'testing on threshold: {threshold}, transpose: {TRANSPOSE}, include neg: {include_neg_possibilities} ({tidx * 2 + int(include_neg_possibilities) + 1} / {len(threshold_grid)})')\n",
    "#             label_high_mi_heads = get_high_mi_heads_threshold(binary_label_mi, threshold)\n",
    "#             # print(f'testing on mass: {0.05}, transpose: {TRANSPOSE}')\n",
    "#             # label_high_mi_heads = get_high_mi_heads_mass(binary_label_mi, 0.05)\n",
    "#             result_save_pth = osp.join(kde_save_pth, 'inference_results', f\"{'transpose' if TRANSPOSE else 'original'}_{threshold}.pt\")\n",
    "#             print(f\"total {sum([len(each) for each in label_high_mi_heads])} heads\")\n",
    "#             print(f\"inference results will be saved to {result_save_pth}\")\n",
    "#             inference_result = infer_by_attn_features(tok_llama, model_llama.to(MODEL_DEVICE), dl_val_llama, {unmasked_label_idx: dep_parser_llama.labelmap[label_name] for unmasked_label_idx, label_name in enumerate(unmasked_label_names)}, x, [*zip(conditional_probab_pos, conditional_probab_neg)], arc_mi, conditional_probab_label, binary_label_mi, -1, with_labels=True, high_mi_heads=label_high_mi_heads, no_pbar=False, transpose=TRANSPOSE, include_neg_possibilities=include_neg_possibilities)\n",
    "#             print('result:', inference_result['metrics'])\n",
    "#             metrics.append(inference_result['metrics'])\n",
    "    # with open(result_save_pth, 'wb') as f:\n",
    "    #     torch.save(inference_result, f)\n",
    "# elif _INFER_MODE == 'posneg_mass_grid':\n",
    "#     for mass in [0.01, 0.02, 0.03, 0.05, 0.08, 0.1, 0.2][::-1]:\n",
    "#         print('testing on method `mass`, mass:', mass)\n",
    "#         label_high_mi_heads = get_high_mi_heads_mass(binary_label_mi, mass)\n",
    "#         print('result:', infer_by_attn_features(tok_llama, model_llama.to(MODEL_DEVICE), dl_val_llama, {unmasked_label_idx: dep_parser_llama.labelmap[label_name] for unmasked_label_idx, label_name in enumerate(unmasked_label_names)}, x, [*zip(conditional_probab_pos, conditional_probab_neg)], arc_mi, conditional_probab_label, binary_label_mi, -1, with_labels=True, high_mi_heads=label_high_mi_heads, no_pbar=False))\n",
    "elif _INFER_MODE == 'head_selection_baseline':\n",
    "    baseline_name2matrix = {'probeless': pl_matrix, 'iou': iou_matrix, 'elasticnet': elasticnet_matrix, 'v_information': bounded_v_information_matrix}\n",
    "    baseline_name2threshold = {'probeless': 20.7031, 'iou': 0.00488758, 'elasticnet': 0.062377844005823135, 'v_information': 4.406166076660156}\n",
    "    print(f'doing baseline inference on {_BASELINE_NAME}, threshold: {baseline_name2threshold[_BASELINE_NAME]}')\n",
    "    baseline_high_mi_heads = select_baseline_heads_by_threshold(baseline_name2matrix[_BASELINE_NAME], baseline_name2threshold[_BASELINE_NAME])\n",
    "    print(f'total {sum([len(each) for each in baseline_high_mi_heads])} heads (including {len(baseline_high_mi_heads[-1])} neg heads)')\n",
    "    inference_result = infer_by_attn_features(tok_llama, model_llama.to(MODEL_DEVICE), dl_val_llama, {unmasked_label_idx: dep_parser_llama.labelmap[label_name] for unmasked_label_idx, label_name in enumerate(unmasked_label_names)}, x, [*zip(conditional_probab_pos, conditional_probab_neg)], arc_mi, conditional_probab_label, baseline_name2matrix[_BASELINE_NAME], -1, with_labels=True, high_mi_heads=baseline_high_mi_heads, no_pbar=False, include_neg_possibilities=True)\n",
    "\n",
    "elif _INFER_MODE == 'baseline_infer_by_mlp':\n",
    "    infer_by_attn_features(tok_llama, model_llama.to(MODEL_DEVICE), dl_val_llama, {unmasked_label_idx: dep_parser_llama.labelmap[label_name] for unmasked_label_idx, label_name in enumerate(unmasked_label_names)}, x, [*zip(conditional_probab_pos, conditional_probab_neg)], arc_mi, conditional_probab_label, binary_label_mi, -1, with_labels=True, high_mi_heads=threshold_high_mi_heads, no_pbar=False, include_neg_possibilities=True, infer_method='mlp', infer_model=deepmlp)\n",
    "elif _INFER_MODE == 'baseline_infer_by_variational_family':\n",
    "    infer_by_attn_features(tok_llama, model_llama.to(MODEL_DEVICE), dl_val_llama, {unmasked_label_idx: dep_parser_llama.labelmap[label_name] for unmasked_label_idx, label_name in enumerate(unmasked_label_names)}, x, [*zip(conditional_probab_pos, conditional_probab_neg)], arc_mi, conditional_probab_label, binary_label_mi, -1, with_labels=True, high_mi_heads=threshold_high_mi_heads, no_pbar=False, include_neg_possibilities=True, infer_method='variational_family', infer_model=variational_family)\n",
    "    \n",
    "elif _INFER_MODE == 'baseline_infer_by_scores':\n",
    "    for topk in [6, 7, 8]:\n",
    "        threshold_high_mi_heads = get_high_mi_heads_threshold(binary_label_mi, label_binary_entropies, 114514, topk=topk)\n",
    "        print(f'topk: {topk}, total {sum([len(each) for each in threshold_high_mi_heads])} heads (including {len(threshold_high_mi_heads[-1])} neg heads)')\n",
    "        infer_by_attn_features(tok_llama, model_llama.to(MODEL_DEVICE), dl_val_llama, {unmasked_label_idx: dep_parser_llama.labelmap[label_name] for unmasked_label_idx, label_name in enumerate(unmasked_label_names)}, x, [*zip(conditional_probab_pos, conditional_probab_neg)], arc_mi, conditional_probab_label, binary_label_mi, -1, with_labels=True, high_mi_heads=threshold_high_mi_heads, no_pbar=False, include_neg_possibilities=True, infer_method='score')\n",
    "    # print('result (pl):', infer_by_attn_features(tok_llama, model_llama.to(MODEL_DEVICE), dl_val_llama, {unmasked_label_idx: dep_parser_llama.labelmap[label_name] for unmasked_label_idx, label_name in enumerate(unmasked_label_names)}, x, [*zip(conditional_probab_pos, conditional_probab_neg)], arc_mi, conditional_probab_label, pl_matrix, -1, with_labels=True, high_mi_heads=pl_high_mi_heads, no_pbar=False))\n",
    "    # print('result (iou):', infer_by_attn_features(tok_llama, model_llama.to(MODEL_DEVICE), dl_val_llama, {unmasked_label_idx: dep_parser_llama.labelmap[label_name] for unmasked_label_idx, label_name in enumerate(unmasked_label_names)}, x, [*zip(conditional_probab_pos, conditional_probab_neg)], arc_mi, conditional_probab_label, iou_matrix, -1, with_labels=True, high_mi_heads=iou_high_mi_heads, no_pbar=False))\n",
    "    # print('result (elasticnet, equal_contribution):', infer_by_attn_features(tok_llama, model_llama.to(MODEL_DEVICE), dl_val_llama, {unmasked_label_idx: dep_parser_llama.labelmap[label_name] for unmasked_label_idx, label_name in enumerate(unmasked_label_names)}, x, [*zip(conditional_probab_pos, conditional_probab_neg)], arc_mi, conditional_probab_label, torch.ones_like(elasticnet_matrix), -1, with_labels=True, high_mi_heads=elasticnet_high_mi_heads, no_pbar=False))\n",
    "    # # elasticnet: UAS: 16.3, GTLAS=13.5, elasticnet (euqal): UAS=33.5, GTLAS=49.8\n",
    "    # print('result (random):', infer_by_attn_features(tok_llama, model_llama.to(MODEL_DEVICE), dl_val_llama, {unmasked_label_idx: dep_parser_llama.labelmap[label_name] for unmasked_label_idx, label_name in enumerate(unmasked_label_names)}, x, [*zip(conditional_probab_pos, conditional_probab_neg)], arc_mi, conditional_probab_label, torch.ones_like(elasticnet_matrix), -1, with_labels=True, high_mi_heads=random_high_mi_heads, no_pbar=False))\n",
    "elif _INFER_MODE == 'baseline_grid':\n",
    "    def baseline_grid_test(baseline_name: str, baseline_matrix: Tensor, baseline_thresholds: List[float], baseline_head_sets: List[List[int]], baseline_num_heads: List[int]):\n",
    "        for threshold, head_set, num_heads in zip(baseline_thresholds, baseline_head_sets, baseline_num_heads):\n",
    "            print(f'testing on method `{baseline_name}`, threshold: {threshold} ({num_heads} heads)')\n",
    "            print('result:', infer_by_attn_features(tok_llama, model_llama.to(MODEL_DEVICE), dl_val_llama, {unmasked_label_idx: dep_parser_llama.labelmap[label_name] for unmasked_label_idx, label_name in enumerate(unmasked_label_names)}, x, [*zip(conditional_probab_pos, conditional_probab_neg)], arc_mi, conditional_probab_label, baseline_matrix, -1, with_labels=True, high_mi_heads=head_set, no_pbar=False))\n",
    "    # threshold 0.01 UAS 36.1\n",
    "    # threshold 0.02 UAS 39.4\n",
    "    # threshold 0.025 UAS 36.5\n",
    "    baseline_grid_test('pl', pl_matrix, pl_thresholds, pl_head_sets, pl_num_heads)\n",
    "    baseline_grid_test('iou', iou_matrix, iou_thresholds, iou_head_sets, iou_num_heads)\n",
    "    baseline_grid_test('elasticnet', elasticnet_matrix, elasticnet_thresholds, elasticnet_head_sets, elasticnet_num_heads)\n",
    "\n",
    "# 3bv2 labels (baseline): UAS: 26.6, GTLAS: 59.5\n",
    "# 3bv2 labels (conditional_probab_label includes no_arc, binary_label_mi includes no_arc): UAS 47.8 GTLAS 53.1\n",
    "# 7b: UAS=48.5, GTLAS=52.1\n",
    "# 13b: UAS=48.6, GTLAS=59.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([len(each) for each in conditional_probab_label])\n",
    "# binary_label_mi.append(\n",
    "#     estimate_mi(\n",
    "#         x, [label_probabilities[label_idx], 1 - label_probabilities[label_idx]], marginal_probab_attn_from_label, \n",
    "#         torch.cat([\n",
    "#             joint_probab_label_stacked[:, label_idx, :].view(n_attn_features, 1, n_x), \n",
    "#             joint_probab_label_stacked[:, torch.arange(n_labels) != label_idx, :].sum(1).view(n_attn_features, 1, n_x)\n",
    "#         ], dim=1), # Tensor[n_heads, 2, n_x]\n",
    "#         KDE_DEVICE\n",
    "#     )\n",
    "# )\n",
    "# print(len(label_probabilities), joint_probab_label_stacked.shape, marginal_probab_attn_from_label.shape)\n",
    "\n",
    "# def estimate_mi_at_labelset(selected_abel_probabilities, ):\n",
    "#     estimate_mi(\n",
    "#         x, [label_probabilities[label_idx], 1 - label_probabilities[label_idx]], marginal_probab_attn_from_label, \n",
    "#         torch.cat([\n",
    "#             joint_probab_label_stacked[:, label_idx, :].view(n_attn_features, 1, n_x), \n",
    "#             joint_probab_label_stacked[:, torch.arange(n_labels) != label_idx, :].sum(1).view(n_attn_features, 1, n_x)\n",
    "#         ], dim=1), # Tensor[n_heads, 2, n_x]\n",
    "#         KDE_DEVICE\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.arange(1, 10).view(3, 3)\n",
    "# a[a != 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 result and mi analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=SET_DATASET_PRIORS\n",
    "with open('./data/dataset_priors.pkl', 'rb') as f:\n",
    "    dataset_priors = pkl.load(f)\n",
    "\n",
    "label2avgdist, label2direction, label2direction_binary = dataset_priors['label2avgdist'], dataset_priors['label2direction'], dataset_priors['label2direction_binary']\n",
    "populations = []\n",
    "for idx, label_samples in enumerate(label_features):\n",
    "    n_samples = label_samples.shape[0]\n",
    "    if n_samples != 0:\n",
    "        populations.append(n_samples)\n",
    "\n",
    "sorted_avgdist = sorted(label2avgdist.values(), reverse=True)\n",
    "label2distrank = {label_name: sorted_avgdist.index(label2avgdist[label_name]) for label_name in label2avgdist}\n",
    "\n",
    "print(populations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39832, 0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#papermill_description=POPULATIONS_AND_DIRECTIONS\n",
    "for label_idx, population in sorted([*enumerate(populations)], key=lambda x: x[1], reverse=True):\n",
    "    label_name = unmasked_label_names[label_idx]\n",
    "    print(f\"{label_name}({label2direction_binary[label_name]}): {population}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.6.1 Correlation between MI and Acc, accopanied with number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=ANALYZE_MI_RESULT_CORR\n",
    "dataset_labelid_to_kde_labelid = {dep_parser_llama.labelmap[unmasked_label_name]: unmasked_label_idx for unmasked_label_idx, unmasked_label_name in enumerate(unmasked_label_names)}\n",
    "kde_labelid_to_dataset_labelid = {unmasked_label_idx: dep_parser_llama.labelmap[unmasked_label_name]  for unmasked_label_idx, unmasked_label_name in enumerate(unmasked_label_names)}\n",
    "\n",
    "def analyze_mi_result_correlationship(result_entry: Dict[str, Any], mi: List[List[int]], entropies: List[int]):\n",
    "    accs, avg_mis, n_samples = [], [], []\n",
    "    for dataset_labelid, kde_labelid in dataset_labelid_to_kde_labelid.items():\n",
    "        accs.append(result_entry['label_corrects'][dataset_labelid] / result_entry['label_totals'][dataset_labelid] * 100)\n",
    "        avg_mis.append(sum(mi[kde_labelid]) / len(mi[kde_labelid]) / entropies[kde_labelid])\n",
    "        n_samples.append(result_entry['label_totals'][dataset_labelid])\n",
    "    \n",
    "    # print(unmasked_label_names[avg_mis.index(max(avg_mis))])\n",
    "    accs, avg_mis, n_samples, label_names = [*zip(*sorted(zip(accs, avg_mis, n_samples, unmasked_label_names), key=lambda x: x[1]))]\n",
    "\n",
    "    return accs, avg_mis, n_samples, label_names\n",
    "\n",
    "accs_by_mi, avg_mis_by_mi, n_samples_by_mi, label_names_by_mi = analyze_mi_result_correlationship(results[1], binary_label_mi, label_binary_entropies)\n",
    "plt.figure(figsize=(32, 32))\n",
    "plt.scatter(avg_mis_by_mi[:-1], accs_by_mi[:-1])\n",
    "for acc, avg_mi, n_sample, label_name in zip(accs_by_mi, avg_mis_by_mi, n_samples_by_mi, label_names_by_mi):\n",
    "    plt.annotate(f'{label_name}\\n({n_sample})', (avg_mi, acc))\n",
    "plt.savefig('./plot.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=VISUALIZE_MI_STACKPLOT\n",
    "accs_by_acc, label_names_by_acc = [*zip(*sorted(zip(accs_by_mi, label_names_by_mi), key=lambda x: x[0], reverse=True))]\n",
    "high_acc_label_names = label_names_by_acc[:16]\n",
    "for label_rank, label_name in enumerate(tqdm(high_acc_label_names, desc=f'drawing & saving stackplots')):\n",
    "    direction = label2direction_binary[label_name]\n",
    "    distrank = label2distrank[label_name]\n",
    "    label_idx = unmasked_label_names.index(label_name)\n",
    "    mi = binary_label_mi[label_idx]\n",
    "    mi_pos = binary_label_mi_pos[label_idx]\n",
    "    L, H = model_llama.config.num_hidden_layers, model_llama.config.num_attention_heads\n",
    "    # fig = plt.figure(figsize=(8, 8))\n",
    "    # plt.stackplot(range(L), [mi[i::H] for i in range(H)])\n",
    "    # plt.title(f'mi stack plot for label {label_name} (accrank: {label_rank}, direction: {direction}, distrank: {distrank})')\n",
    "    # plt.savefig(f\"./visualization_results/mi_stackplot{'_transpose' if TRANSPOSE else ''}/{label_rank:02d}_{label_name}_{direction}_d{distrank:02d}.png\", dpi=400)\n",
    "    # plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.stackplot(range(L), [mi_pos[i::H] for i in range(H)])\n",
    "    plt.title(f'mi (pos) stack plot for label {label_name} (accrank: {label_rank}, direction: {direction}, distrank: {distrank})')\n",
    "    plt.savefig(f\"./visualization_results/mi_pos_stackplot{'_transpose' if TRANSPOSE else ''}/{label_rank:02d}_{label_name}_{direction}_d{distrank:02d}.png\", dpi=400)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=VISUALIZE_PROBAB_DISTRIBUTIONS\n",
    "for label_rank, label_name in enumerate(tqdm(high_acc_label_names, desc='drawing & saving probab distributions')):\n",
    "    direction = label2direction_binary[label_name]\n",
    "    label_idx = unmasked_label_names.index(label_name)\n",
    "    mi = binary_label_mi[label_idx]\n",
    "    high_mi_heads = [each[0] for each in sorted([*enumerate(mi)], key=lambda x: x[1], reverse=True)[:5]]\n",
    "    for head_rank, head_idx in enumerate(high_mi_heads):\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        plt.subplot(2, 1, 1).plot(x.cpu(), joint_probab_label_stacked[head_idx][label_idx].cpu(), label='label')\n",
    "        plt.subplot(2, 1, 2).plot(x.cpu(), marginal_probab_attn_from_label[head_idx].cpu(), label='marginal')\n",
    "        plt.title(f'probab distribution for label {label_name} and head {head_idx} (No. {head_rank})')\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"./visualization_results/probab_distribution{'_transpose' if TRANSPOSE else ''}/{label_rank:02d}_{label_name}_{direction}_{head_rank:02d}_head{head_idx}.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(osp.join('inference_results', 'original_0.2.json'), 'r') as f_original, open(osp.join('inference_results', 'transpose_0.5.json'), 'r') as f_transpose:\n",
    "    result_original = json.load(f_original)\n",
    "    result_transpose = json.load(f_transpose)\n",
    "with open(osp.join(kde_save_pth, 'mi_1.json')) as f_original, open(osp.join(kde_save_pth, 'mi_1_transpose.json')) as f_transpose:\n",
    "    mi_original = json.load(f_original)\n",
    "    mi_transpose = json.load(f_transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_macro_acc(result):\n",
    "    label_corrects, label_totals = [*result['label_corrects'].values()], [*result['label_totals'].values()]\n",
    "    # label_totals is possibly zero\n",
    "    individual_results = [label_correct / label_total for (label_correct, label_total) in zip(label_corrects, label_totals) if label_total > 0]\n",
    "    return sum(individual_results) / len(individual_results)\n",
    "\n",
    "print(get_macro_acc(result_original))\n",
    "print(get_macro_acc(result_transpose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_listlike([*zip([max(each) for each in mi_original['binary_label_mi']], [max(each) for each in mi_transpose['binary_label_mi']])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.008509306237101555, 0.011922112666070461) (0.005480445921421051, 0.01151051465421915) (0.0038365633226931095, 0.00990423932671547) (0.0012449548812583089, 0.00282877404242754) (0.006173050031065941, 0.012894314713776112) (0.004695800598710775, 0.013605937361717224) (0.001123006222769618, 0.002053096890449524) (0.0014259052695706487, 0.0018166368827223778) (0.005726659670472145, 0.008281094953417778) (0.00371841574087739, 0.00848476029932499)\n",
      "(0.00032986191217787564, 0.0008982069557532668) (0.0035700106527656317, 0.004983129911124706) (0.0003002947196364403, 0.0015047428896650672) (0.00217073573730886, 0.002882377477362752) (0.00163302943110466, 0.0029146289452910423) (0.00040211749728769064, 0.0009975847788155079) (0.00038957095239311457, 0.0007821473991498351) (0.0008883021655492485, 0.001717525301501155) (0.0012408897746354342, 0.0030396226793527603) (0.005120383109897375, 0.007830372080206871)\n",
      "(0.0008684846106916666, 0.0017866557464003563) (0.003090638667345047, 0.004927610047161579) (0.0006299521774053574, 0.0012377448147162795) (0.0007534925825893879, 0.0015438147820532322) (0.00017766703967936337, 0.000650808506179601) (0.00022260613332036883, 0.00042925457819364965) (0.0005413563922047615, 0.0006820096168667078) (0.00017213911633007228, 0.0004010225529782474) (0.0004218074318487197, 0.001041446696035564) (0.00038910406874492764, 0.0006904701003804803)\n",
      "(0.0004926404217258096, 0.0013195454375818372) (0.0001903606316773221, 0.0007636048831045628) (2.2737094695912674e-05, 4.754740439238958e-05) (1.7615006072446704e-05, 5.371710722101852e-05) (9.592642891220748e-05, 0.00023520170361734927) (3.752478369278833e-05, 0.00017649895744398236) (0.00013844271597918123, 0.00031320887501351535) (0.0008176161791197956, 0.0014379114145413041) (5.265280560706742e-05, 9.725991549203172e-05) (0.0003262209065724164, 0.0005583890597335994)\n",
      "(7.229312177514657e-05, 8.874681225279346e-05) (2.021445288846735e-05, 8.239836461143568e-05) (8.180856093531474e-05, 0.00017599755665287375) (4.728794920083601e-06, 1.326938763668295e-05) (1.199484358949121e-06, 1.9171409348928137e-06) (0.02792215347290039, 0.10397017747163773)\n",
      "(0.003538423450663686, 0.09654909372329712) (0.005230465903878212, 0.09930947422981262) (0.00298764044418931, 0.10022991895675659) (0.0017552977660670877, 0.09837523102760315) (0.0003950856043957174, 0.09820210188627243) (0.0036543463356792927, 0.10229430347681046) (0.002104786690324545, 0.10029257088899612) (0.0005779524799436331, 0.09685541689395905) (0.000559798558242619, 0.09780830144882202) (0.002719311509281397, 0.09615157544612885)\n",
      "(0.000775539199821651, 0.09470006823539734) (0.004721942823380232, 0.10394040495157242) (0.0018686753464862704, 0.09612993896007538) (0.0022754173260182142, 0.10081900656223297) (0.002744500758126378, 0.10183010995388031) (0.0024910559877753258, 0.09597383439540863) (0.003864030819386244, 0.09129296988248825) (0.0028296776581555605, 0.1003391370177269) (0.0022124508395791054, 0.09982605278491974) (0.0012656375765800476, 0.09461067616939545)\n",
      "(0.0010135099291801453, 0.1007528305053711) (0.0014310934348031878, 0.0977499857544899) (0.0008170060464181006, 0.08581780642271042) (0.0028000499587506056, 0.10116851329803467) (0.00431143818423152, 0.08308853209018707) (0.0031493729911744595, 0.09787991642951965) (0.003410052042454481, 0.08985605835914612) (0.0005383268580771983, 0.09061265736818314) (0.0030584444757550955, 0.10014204680919647) (0.0011059385724365711, 0.09994509816169739)\n",
      "(0.0034676091745495796, 0.09689827263355255) (0.0029892660677433014, 0.0858207494020462) (0.0021781898103654385, 0.09620308130979538) (0.0007053419831208885, 0.07141280174255371) (0.0004885889939032495, 0.07803846150636673) (0.0006251857848837972, 0.09660587459802628) (0.0036440598778426647, 0.08087445050477982) (0.0007423280039802194, 0.09942762553691864) (0.0042703235521912575, 0.10367786139249802) (0.0011169607751071453, 0.0794285386800766)\n",
      "(0.0009513851255178452, 0.09938749670982361) (0.0011091344058513641, 0.07141361385583878) (0.0012024729512631893, 0.10180211067199707) (0.0036953995004296303, 0.10171431303024292) (0.004994899034500122, 0.09192556887865067) (0.0005735846934840083, 0.10293376445770264) (0.002033098367974162, 0.08320410549640656) (0.0011613457463681698, 0.0747300237417221) (0.0015596409793943167, 0.09145932644605637) (0.0033757640048861504, 0.08838936686515808)\n",
      "(0.002824694849550724, 0.0859801322221756) (0.0014617869164794683, 0.06612419337034225) (0.0019008195959031582, 0.09014333039522171) (0.002749057486653328, 0.07348426431417465) (0.002308052498847246, 0.09395502507686615) (0.00047484668903052807, 0.10146933048963547) (0.0006396367098204792, 0.1032126322388649) (0.0030418795067816973, 0.08456768840551376) (0.0055458261631429195, 0.09617383033037186) (0.005449195392429829, 0.09140415489673615)\n",
      "(0.0010120822116732597, 0.0986442118883133) (0.006652199663221836, 0.09561967849731445) (0.012357131578028202, 0.09551138430833817) (0.00478027667850256, 0.10111004114151001) (0.00289359362795949, 0.08255456387996674) (0.0006812833016738296, 0.08527390658855438) (0.0007143736002035439, 0.07042033225297928) (0.0037561224307864904, 0.08098212629556656) (0.0017892041942104697, 0.09026279300451279) (0.0033484655432403088, 0.07564334571361542)\n",
      "(0.002329564420506358, 0.07082056254148483) (0.0035044657997787, 0.08329334110021591) (0.0015091325622051954, 0.08209148794412613) (0.0026287913788110018, 0.07862455397844315) (0.001605004770681262, 0.0837293192744255) (0.0008892512414604425, 0.08440781384706497) (0.002116499003022909, 0.1032349243760109) (0.0019911611452698708, 0.0863451436161995) (0.006079092621803284, 0.09043745696544647) (0.0012043367605656385, 0.08102182298898697)\n",
      "(0.006829528138041496, 0.06668747216463089) (0.002616978483274579, 0.08417288959026337) (0.003050382947549224, 0.07557651400566101) (0.002130358712747693, 0.07785594463348389) (0.0013833249686285853, 0.08120817691087723) (0.00624353950843215, 0.07804524898529053) (0.0021339678205549717, 0.09894666075706482) (0.0008346559479832649, 0.08420773595571518) (0.0008060901891440153, 0.08620499819517136) (0.00397516880184412, 0.09149312973022461)\n",
      "(0.0017623184248805046, 0.07234926521778107) (0.0012357837986201048, 0.07700920850038528) (0.003157820086926222, 0.08335420489311218) (0.0013179833767935634, 0.07851720601320267) (0.0012572959531098604, 0.08107301592826843) (0.0026978293899446726, 0.0799381360411644) (0.0005909988540224731, 0.10098032653331757) (0.0009880070574581623, 0.09945032000541687) (0.0008387358975596726, 0.10038833320140839) (0.009954411536455154, 0.08844179660081863)\n",
      "(0.0022189165465533733, 0.07846574485301971) (0.008406427688896656, 0.09448565542697906) (0.0038724762853235006, 0.09284304082393646) (0.003451521508395672, 0.08564137667417526) (0.012744387611746788, 0.09031596034765244) (0.00823616236448288, 0.09184794872999191) (0.008060827851295471, 0.09149032831192017) (0.002647778019309044, 0.09444565325975418) (0.005269243847578764, 0.09420152753591537) (0.0026657278649508953, 0.09801393002271652)\n",
      "(0.0007045163656584918, 0.0991656482219696) (0.002693778369575739, 0.1000940352678299) (0.00701238214969635, 0.09234137088060379) (0.00854237750172615, 0.07813861221075058) (0.0036876618396490812, 0.09495218098163605) (0.003606476355344057, 0.0957106351852417) (0.004028730560094118, 0.09770217537879944) (0.0060328166000545025, 0.09463941305875778) (0.0012789086904376745, 0.10192076861858368) (0.003987171221524477, 0.09766773134469986)\n",
      "(0.0007881120545789599, 0.0963008850812912) (0.005205586552619934, 0.09010390937328339) (0.0017581103602424264, 0.09387332201004028) (0.0008809371502138674, 0.09216530621051788) (0.005652072373777628, 0.09044855833053589) (0.006350558251142502, 0.09285178780555725) (0.006012083031237125, 0.08348887413740158) (0.001419279957190156, 0.1001393049955368) (0.00719337398186326, 0.09391285479068756) (0.003398515284061432, 0.09509076178073883)\n",
      "(0.00373157043941319, 0.09139255434274673) (0.008549234829843044, 0.08865842968225479) (0.00044956564670428634, 0.09996841847896576) (0.0010140967788174748, 0.09936481714248657) (0.0009712208993732929, 0.0992942601442337) (0.0013623125851154327, 0.10051358491182327) (0.00042191240936517715, 0.10063789784908295) (0.006613501813262701, 0.09316255152225494) (0.0008757588220760226, 0.10056697577238083) (0.00184353138320148, 0.0966816395521164)\n",
      "(0.008898756466805935, 0.09225993603467941) (0.006493098102509975, 0.06865230947732925) (0.012293001636862755, 0.09279428422451019) (0.012775108218193054, 0.09020783007144928) (0.00468836072832346, 0.07542058825492859) (0.0007853522547520697, 0.09969358146190643) (0.004350611008703709, 0.09259482473134995) (0.0014790694694966078, 0.09988579154014587) (0.00180212059058249, 0.10086517035961151) (0.006510079838335514, 0.09330010414123535)\n",
      "(0.004475949797779322, 0.08567435294389725) (0.0013618434313684702, 0.07680749148130417) (0.0038625053130090237, 0.08731311559677124) (0.003049912629649043, 0.0976703017950058) (0.004908286966383457, 0.09105256199836731) (0.01352180726826191, 0.08878349512815475) (0.008707608096301556, 0.09078611433506012) (0.010531073436141014, 0.09449908137321472) (0.00026212306693196297, 0.0924082100391388) (0.004695587791502476, 0.0955466628074646)\n",
      "(0.008348317816853523, 0.0967005267739296) (0.0040847803466022015, 0.0913144052028656) (0.0014896576758474112, 0.08957012742757797) (0.009573910385370255, 0.09639353305101395) (0.002650311915203929, 0.10153350234031677) (0.0028426244389265776, 0.07792763411998749) (0.005387724377214909, 0.09464335441589355) (0.0065495665185153484, 0.0714249387383461) (0.00901241134852171, 0.09149134159088135) (0.0106765516102314, 0.09287300705909729)\n",
      "(0.003067743731662631, 0.09333249181509018) (0.008120762184262276, 0.0906262993812561) (0.011573482304811478, 0.07421654462814331) (0.006602110806852579, 0.09398353844881058) (0.006094660144299269, 0.09650205820798874) (0.0022227561566978693, 0.09061750769615173) (0.0063570234924554825, 0.09689895063638687) (0.005806311033666134, 0.09959869086742401) (0.014112893491983414, 0.09004823118448257) (0.0007306495681405067, 0.0942670926451683)\n",
      "(0.0038460928481072187, 0.09219576418399811) (0.007697959430515766, 0.09408187866210938) (0.0014334791339933872, 0.09051947295665741) (0.019245047122240067, 0.09326281398534775) (0.009910918772220612, 0.09847855567932129) (0.006462684366852045, 0.09531450271606445) (0.005397583823651075, 0.08379253000020981) (0.004727233666926622, 0.09795117378234863) (0.006754571571946144, 0.08769334852695465) (0.003361633513122797, 0.10040570795536041)\n",
      "(0.0016868086531758308, 0.08107184618711472) (0.002929933834820986, 0.09866616129875183) (0.004812275990843773, 0.07230014353990555) (0.004146854393184185, 0.0797051265835762) (0.0011699943570420146, 0.0993599072098732) (0.011397005058825016, 0.09593725204467773) (0.003785846522077918, 0.09329535067081451) (0.001927293953485787, 0.09656058996915817) (0.010425037704408169, 0.09040136635303497) (0.014975709840655327, 0.0977754220366478)\n",
      "(0.0005387926357798278, 0.10195451974868774) (0.007732093334197998, 0.09268675744533539) (0.0025158640928566456, 0.09039007127285004) (0.007923582568764687, 0.08470819890499115) (0.0023354487493634224, 0.09067057818174362) (0.006269556935876608, 0.09369394928216934) (0.003908977378159761, 0.0797761008143425) (0.010405546054244041, 0.0966329425573349) (0.009849049150943756, 0.09057861566543579) (0.006710696965456009, 0.085242360830307)\n",
      "(0.007954631000757217, 0.09541915357112885) (0.012321194633841515, 0.08336766064167023) (0.002995163667947054, 0.09220221638679504) (0.007597539573907852, 0.0851520374417305) (0.011303868144750595, 0.08789287507534027) (0.0074305483140051365, 0.09568588435649872) (0.00530354306101799, 0.09576133638620377) (0.013950093649327755, 0.0958728939294815) (0.006519750691950321, 0.09520941972732544) (0.002769160084426403, 0.08547544479370117)\n",
      "(0.012150113470852375, 0.0903436690568924) (0.003955803345888853, 0.08972654491662979) (0.0018148235976696014, 0.08852607011795044) (0.003730545751750469, 0.09859233349561691) (0.020638760179281235, 0.08971956372261047) (0.00544152082875371, 0.09478535503149033) (0.0028854277916252613, 0.08922650665044785) (0.015442891977727413, 0.07889382541179657) (0.001809685374610126, 0.09771489351987839) (0.011866007000207901, 0.09683690965175629)\n",
      "(0.008526117540895939, 0.09186053276062012) (0.004785505589097738, 0.09638605266809464) (0.005047207698225975, 0.08056436479091644) (0.012311490252614021, 0.09631960093975067) (0.013583429157733917, 0.09037947654724121) (0.020639104768633842, 0.09733949601650238) (0.012402174063026905, 0.09830301254987717) (0.017745187506079674, 0.0971517264842987) (0.0017002243548631668, 0.08449907600879669) (0.0065129417926073074, 0.08731243014335632)\n",
      "(0.005504217930138111, 0.09838788211345673) (0.00944489985704422, 0.08790242671966553) (0.0002694054855965078, 0.10088656842708588) (0.0022679357789456844, 0.10125377774238586) (0.006460641045123339, 0.09862717986106873) (0.013893386349081993, 0.09411383420228958) (0.0058175926096737385, 0.07485263049602509) (0.009086567908525467, 0.09930776804685593) (0.0015313985059037805, 0.08979891240596771) (0.00879058986902237, 0.09553361684083939)\n",
      "(0.011931532993912697, 0.09049688279628754) (0.012864826247096062, 0.09187096357345581) (0.015886416658759117, 0.09794919937849045) (0.013661375269293785, 0.09136827290058136) (0.01206850353628397, 0.09485160559415817) (0.00435693608596921, 0.09390785545110703) (0.005832463502883911, 0.07802502810955048) (0.006127861328423023, 0.09199178963899612) (0.022296687588095665, 0.09165076911449432) (0.011796863749623299, 0.09090027958154678)\n",
      "(0.0070177405141294, 0.09281978011131287) (0.0032748128287494183, 0.08884463459253311) (0.0025299065746366978, 0.1012154147028923) (0.014902453869581223, 0.09497540444135666) (0.018150102347135544, 0.0921565443277359) (0.01124508772045374, 0.09203219413757324) (0.006325880531221628, 0.08428216725587845) (0.010553453117609024, 0.08961323648691177) (0.012234905734658241, 0.09348763525485992) (0.01327758003026247, 0.09581736475229263)\n",
      "(0.008486904203891754, 0.09501132369041443) (0.001698957639746368, 0.088590607047081) (0.010104615241289139, 0.092919260263443) (0.011731501668691635, 0.08906807005405426) (0.0043495730496943, 0.09762085229158401) (0.006137463729828596, 0.09193647652864456) (0.014836871065199375, 0.09214981645345688) (0.005469339434057474, 0.07825646549463272) (0.0029154662042856216, 0.08196880668401718) (0.005209995899349451, 0.08897824585437775)\n",
      "(0.0032994134817272425, 0.09239019453525543) (0.009008211083710194, 0.09355038404464722) (0.007223828695714474, 0.098723866045475) (0.014202569611370564, 0.09251277148723602) (0.01207238994538784, 0.08994266390800476) (0.0058500864543020725, 0.09548838436603546) (0.002701782388612628, 0.09659833461046219) (0.008291458711028099, 0.09183517098426819) (0.016102444380521774, 0.09928224980831146) (0.0030522882007062435, 0.09859272092580795)\n",
      "(0.0020170763600617647, 0.10160152614116669) (0.012945911847054958, 0.10337750613689423) (0.002658191369846463, 0.10501523315906525) (0.001384550821967423, 0.09678473323583603) (0.002125718165189028, 0.10293511301279068) (0.006187799386680126, 0.10003996640443802) (0.011312361806631088, 0.09587593376636505) (0.000563190144021064, 0.09092529118061066) (0.01462815422564745, 0.09428852051496506) (0.00319862668402493, 0.0953781008720398)\n",
      "(0.019962023943662643, 0.09953442960977554) (0.006045129150152206, 0.09275554120540619) (0.011195771396160126, 0.09474413096904755) (0.003285613376647234, 0.09670010209083557) (0.003124213544651866, 0.1039353609085083) (0.005452087614685297, 0.08944925665855408) (0.00313049228861928, 0.10241726040840149) (0.007839723490178585, 0.10114835947751999) (0.013009234331548214, 0.09327584505081177) (0.010158526711165905, 0.09912225604057312)\n",
      "(0.004024852998554707, 0.10326385498046875) (0.01891675963997841, 0.09705530107021332) (0.010482482612133026, 0.10108127444982529) (0.012864943593740463, 0.09672315418720245) (0.019231528043746948, 0.09686475247144699) (0.008007899858057499, 0.0775851458311081) (0.011559396982192993, 0.09845144301652908) (0.012989099137485027, 0.09851907193660736) (0.004904498346149921, 0.0898134708404541) (0.003341283882036805, 0.10276135802268982)\n",
      "(0.012875054962933064, 0.09788816422224045) (0.006643777247518301, 0.09321301430463791) (0.01781308278441429, 0.0954798087477684) (0.019134173169732094, 0.09598399698734283) (0.012103283777832985, 0.10024712234735489) (0.010011507198214531, 0.10082022100687027) (0.012363863177597523, 0.09866348654031754) (0.01253003440797329, 0.09687883406877518) (0.005899067502468824, 0.09526337683200836) (0.00435820734128356, 0.09701430052518845)\n",
      "(0.00782222393900156, 0.09706263244152069) (0.006164699327200651, 0.09315861761569977) (0.011429564096033573, 0.10176265239715576) (0.013629531487822533, 0.09617826342582703) (0.003492107382044196, 0.093463234603405) (0.02105925977230072, 0.09908008575439453) (0.006403855513781309, 0.09598858654499054) (0.008325932547450066, 0.09935788065195084) (0.008388432674109936, 0.09848452359437943) (0.009082168340682983, 0.08950327336788177)\n",
      "(0.014954079873859882, 0.0966406837105751) (0.009050313383340836, 0.0971863716840744) (0.006239466834813356, 0.07085642218589783) (0.018290536478161812, 0.09466131031513214) (0.013312160968780518, 0.10377073287963867) (0.01809898018836975, 0.0958063155412674) (0.018673496320843697, 0.10166607052087784) (0.012287609279155731, 0.09960015118122101) (0.0034599974751472473, 0.09347130358219147) (0.003451216034591198, 0.1012471467256546)\n",
      "(0.011967523023486137, 0.08325422555208206) (0.01239645853638649, 0.09718979895114899) (0.013890898786485195, 0.10354036092758179) (0.007687052711844444, 0.09866553544998169) (0.013704527169466019, 0.09626574069261551) (0.009641803801059723, 0.09707313030958176) (0.008503459393978119, 0.09542831033468246) (0.012467565014958382, 0.0926666110754013) (0.008692662231624126, 0.09287112206220627) (0.013826378621160984, 0.09298799932003021)\n",
      "(0.02571934461593628, 0.09967772662639618) (0.009270410984754562, 0.09240967035293579) (0.014576306566596031, 0.09024173021316528) (0.019124221056699753, 0.09325423091650009) (0.019161313772201538, 0.09808118641376495) (0.0020843439269810915, 0.09735888242721558) (0.002542235190048814, 0.10331421345472336) (0.011233452707529068, 0.09791344404220581) (0.022426588460803032, 0.1023518443107605) (0.011294101364910603, 0.09807667136192322)\n",
      "(0.01708923652768135, 0.09271189570426941) (0.009470542892813683, 0.09665758907794952) (0.011875761672854424, 0.09546004235744476) (0.01358485221862793, 0.0985470712184906) (0.00501530896872282, 0.09884089231491089) (0.01571510173380375, 0.09510816633701324) (0.004852959420531988, 0.09858700633049011) (0.0033788704313337803, 0.10259716212749481) (0.011001725681126118, 0.09651509672403336) (0.0043654670007526875, 0.08641047775745392)\n",
      "(0.011203653179109097, 0.08935071527957916) (0.011143891140818596, 0.09715452045202255) (0.017028266564011574, 0.09239056706428528) (0.00681100832298398, 0.07540352642536163) (0.004054199438542128, 0.09649854153394699) (0.0023911590687930584, 0.1017746850848198) (0.019974712282419205, 0.09620288014411926) (0.008669418282806873, 0.09073413163423538) (0.014479970559477806, 0.09465152025222778) (0.008429499343037605, 0.07759146392345428)\n",
      "(0.006330389995127916, 0.10268991440534592) (0.007875794544816017, 0.10370174795389175) (0.0082200076431036, 0.10426061600446701) (0.006647847592830658, 0.08976902812719345) (0.015835514292120934, 0.09493666887283325) (0.008839026093482971, 0.09697174280881882) (0.01042898092418909, 0.09730450809001923) (0.011324774473905563, 0.09543521702289581) (0.014118866994976997, 0.09759105741977692) (0.01245160587131977, 0.09495478868484497)\n",
      "(0.011857066303491592, 0.08982637524604797) (0.012426420114934444, 0.0944608598947525) (0.008988449349999428, 0.08689679205417633) (0.01367587223649025, 0.09108158946037292) (0.022270793095231056, 0.09808754920959473) (0.009374559856951237, 0.09164781868457794) (0.003933980129659176, 0.09029901027679443) (0.006706364452838898, 0.10232960432767868) (0.0030240975320339203, 0.09104684740304947) (0.008224143646657467, 0.10184070467948914)\n",
      "(0.009230195544660091, 0.09804895520210266) (0.01676562801003456, 0.0949673280119896) (0.01184998918324709, 0.08339070528745651) (0.011155606247484684, 0.09167733043432236) (0.0027000706177204847, 0.09701767563819885) (0.011972641572356224, 0.08661489933729172) (0.015101904049515724, 0.0919736698269844) (0.014180240221321583, 0.09430479258298874) (0.004802963696420193, 0.08793437480926514) (0.008440764620900154, 0.07910427451133728)\n",
      "(0.009915027767419815, 0.08463052660226822) (0.008382606320083141, 0.09115449339151382) (0.013226684182882309, 0.09670662879943848) (0.007910837419331074, 0.10365992784500122) (0.015899207442998886, 0.1022033765912056) (0.0037351949140429497, 0.08518622815608978) (0.009736450389027596, 0.09451087564229965) (0.01565534807741642, 0.09486900269985199) (0.014195606112480164, 0.09689031541347504) (0.0004834143619518727, 0.09796947240829468)\n",
      "(0.0014142778236418962, 0.09907115250825882) (0.020164934918284416, 0.09555346518754959) (0.013895073905587196, 0.08925259858369827) (0.005551177076995373, 0.08623859286308289) (0.017751509323716164, 0.09391635656356812) (0.018927685916423798, 0.09188239276409149) (0.008589988574385643, 0.08827383816242218) (0.012944404035806656, 0.08249807357788086) (0.005638486240059137, 0.09769301116466522) (0.015584444627165794, 0.09918690472841263)\n",
      "(0.005272476468235254, 0.10325267910957336) (0.008603179827332497, 0.08987440913915634) (0.016501933336257935, 0.0991545245051384) (0.019791947677731514, 0.09122133255004883) (0.011761338450014591, 0.10149694979190826) (0.016170859336853027, 0.09042275696992874) (0.02600705996155739, 0.08764274418354034) (0.026932071894407272, 0.09378121793270111) (0.007643935736268759, 0.07250579446554184) (0.011736785992980003, 0.08242790400981903)\n",
      "(0.0062076980248093605, 0.08404349535703659) (0.0013148043071851134, 0.09084869176149368) (0.017032276839017868, 0.10151251405477524) (0.004197330679744482, 0.09493694454431534) (0.010530226863920689, 0.09611521661281586) (0.007903813384473324, 0.09151342511177063) (0.010688225738704205, 0.09828108549118042) (0.014207442291080952, 0.095549575984478) (0.01966325007379055, 0.08930642157793045) (0.00983043760061264, 0.09171504527330399)\n",
      "(0.010699938982725143, 0.0919242650270462) (0.005594177171587944, 0.08781411498785019) (0.006066116504371166, 0.08449555188417435) (0.009469453245401382, 0.08564680069684982) (0.008281391113996506, 0.08735613524913788) (0.007976364344358444, 0.09384164214134216) (0.006367110647261143, 0.08237787336111069) (0.009708963334560394, 0.09389644861221313) (0.013281738385558128, 0.09420829266309738) (0.014034491032361984, 0.09372835606336594)\n",
      "(0.0131086939945817, 0.08122026920318604) (0.008373389951884747, 0.08814728260040283) (0.0036430733744055033, 0.08541634678840637) (0.007490950636565685, 0.08411981165409088) (0.003220955142751336, 0.09153426438570023) (0.004972946364432573, 0.09120790660381317) (0.010671338066458702, 0.07850778847932816) (0.005352659150958061, 0.08361611515283585) (0.0040854234248399734, 0.0856558084487915) (0.011112269945442677, 0.08878607302904129)\n",
      "(0.010411204770207405, 0.09224222600460052) (0.012117404490709305, 0.10172850638628006) (0.0018546057399362326, 0.09219442307949066) (0.012023141607642174, 0.09868556261062622) (0.014351701363921165, 0.08999155461788177) (0.008262814953923225, 0.09381825476884842) (0.01948012039065361, 0.0939236655831337) (0.008147591724991798, 0.09917604178190231) (0.010607641190290451, 0.09633907675743103) (0.015316307544708252, 0.08795444667339325)\n",
      "(0.007689495570957661, 0.08884982764720917) (0.011843010783195496, 0.08808668702840805) (0.014352699741721153, 0.09821277856826782) (0.0008272875566035509, 0.10273963212966919) (0.019804080948233604, 0.09610023349523544) (0.008939962834119797, 0.0821637511253357) (0.013166163116693497, 0.07894483208656311) (0.010109511204063892, 0.07354255765676498) (0.010908550582826138, 0.09607847779989243) (0.0028076611924916506, 0.0895223394036293)\n",
      "(0.015435395762324333, 0.09023971110582352) (0.00645797373726964, 0.08697601407766342) (0.02031426690518856, 0.08730915933847427) (0.011265626177191734, 0.08716966211795807) (0.008058365434408188, 0.09561432152986526) (0.005733063444495201, 0.09006849676370621) (0.00949431024491787, 0.07858817279338837) (0.016118310391902924, 0.08905266970396042) (0.012283879332244396, 0.08170532435178757) (0.003987794276326895, 0.08830160647630692)\n",
      "(0.009080440737307072, 0.08157328516244888) (0.005554630421102047, 0.08286906778812408) (0.0140937315300107, 0.08311976492404938) (0.0123886214569211, 0.08227982372045517) (0.01287925522774458, 0.08147884160280228) (0.0016965765971690416, 0.09895570576190948) (0.00798557698726654, 0.08828059583902359) (0.006053981836885214, 0.09828688204288483) (0.021923989057540894, 0.08913694322109222) (0.0054098693653941154, 0.08923692256212234)\n",
      "(0.00309458514675498, 0.08450198173522949) (0.010426532477140427, 0.0960787832736969) (0.004229182377457619, 0.09109965711832047) (0.008865480311214924, 0.08767470717430115) (0.009784613735973835, 0.08799635618925095) (0.006597310770303011, 0.09189854562282562) (0.0072442092932760715, 0.08795259147882462) (0.005433559883385897, 0.08185307681560516) (0.006326326169073582, 0.07799729704856873) (0.023844074457883835, 0.08901695162057877)\n",
      "(0.013388258405029774, 0.09378457814455032) (0.01244749128818512, 0.08095405995845795) (0.004884355701506138, 0.08306585252285004) (0.025241916999220848, 0.08246225118637085) (0.01953553594648838, 0.08845281600952148) (0.015702221542596817, 0.0909036248922348) (0.0055746347643435, 0.09228217601776123) (0.004111666232347488, 0.09199397265911102) (0.016175590455532074, 0.09078459441661835) (0.014161651954054832, 0.08400304615497589)\n",
      "(0.007278790231794119, 0.08809573203325272) (0.007349180988967419, 0.08423228561878204) (0.01524020079523325, 0.07964375615119934) (0.008258027955889702, 0.08330132812261581) (0.020464971661567688, 0.08916466683149338) (0.005640771239995956, 0.08029544353485107) (0.010339498519897461, 0.08658166974782944) (0.013341614045202732, 0.08761341869831085) (0.014526774175465107, 0.09068472683429718) (0.01730348728597164, 0.09722089767456055)\n",
      "(0.005634491331875324, 0.09083757549524307) (0.015825070440769196, 0.07329044491052628) (0.00942273996770382, 0.07725506275892258) (0.012868344783782959, 0.09763479232788086) (0.009838324040174484, 0.08314387500286102) (0.013534629717469215, 0.09186888486146927) (0.02047826163470745, 0.07834090292453766) (0.01264970749616623, 0.08640557527542114) (0.018047694116830826, 0.08994293212890625) (0.01474057324230671, 0.09692513942718506)\n",
      "(0.00923106912523508, 0.0878852978348732) (0.008141008205711842, 0.0882074385881424) (0.010014107450842857, 0.10415548831224442) (0.012566990219056606, 0.08329485356807709) (0.004554386250674725, 0.09611478447914124) (0.005864831153303385, 0.08358381688594818) (0.013076499104499817, 0.0904163047671318) (0.01326935924589634, 0.08547026664018631) (0.00824199803173542, 0.08248191326856613) (0.0049516428261995316, 0.09345732629299164)\n",
      "(0.009017545729875565, 0.0732363909482956) (0.018743915483355522, 0.09085343033075333) (0.012367038987576962, 0.08957947790622711) (0.013889042660593987, 0.09737711399793625) (0.025932861492037773, 0.09267923980951309) (0.014345812611281872, 0.07978850603103638) (0.02366894856095314, 0.08830045908689499) (0.0109182707965374, 0.07955291122198105) (0.0089306915178895, 0.08828823268413544) (0.017524808645248413, 0.09325218945741653)\n",
      "(0.018401052802801132, 0.09392049163579941) (0.0023051449097692966, 0.08169111609458923) (0.011702235788106918, 0.08319670706987381) (0.019011229276657104, 0.08650178462266922) (0.0206571314483881, 0.09260290861129761) (0.0056708804331719875, 0.09186334162950516) (0.020887652412056923, 0.0867532342672348) (0.008325143717229366, 0.08844240754842758) (0.008084003813564777, 0.08187005668878555) (0.007193964906036854, 0.08515866100788116)\n",
      "(0.01832440122961998, 0.08541200309991837) (0.007598709315061569, 0.0865258201956749) (0.009514356963336468, 0.09282055497169495) (0.016852382570505142, 0.09123054146766663) (0.016250893473625183, 0.09390590339899063) (0.012760374695062637, 0.09648790210485458) (0.009561590850353241, 0.08511706441640854) (0.00707116536796093, 0.09051156044006348) (0.004764144774526358, 0.089708611369133) (0.00989575032144785, 0.07988391816616058)\n",
      "(0.007279001176357269, 0.09358825534582138) (0.009595480747520924, 0.08766042441129684) (0.014808906242251396, 0.08215012401342392) (0.007917898707091808, 0.07611483335494995) (0.011226372793316841, 0.09044764190912247) (0.012901504524052143, 0.08389712125062943) (0.020442228764295578, 0.08968392759561539) (0.013544922694563866, 0.0974007397890091) (0.006919763050973415, 0.08622036874294281) (0.009247113019227982, 0.09505874663591385)\n",
      "(0.010511031374335289, 0.08775714039802551) (0.020891793072223663, 0.09035293757915497) (0.027849143370985985, 0.09222550690174103) (0.011239014565944672, 0.07988995313644409) (0.00857567135244608, 0.07669830322265625) (0.01974198967218399, 0.08587973564863205) (0.015456105582416058, 0.09388703107833862) (0.011864098720252514, 0.09806956350803375) (0.020485518500208855, 0.08526761829853058) (0.02089492231607437, 0.08608217537403107)\n",
      "(0.0008385850815102458, 0.10007846355438232) (0.010192498564720154, 0.08910223096609116) (0.009997459128499031, 0.07683208584785461) (0.013823497109115124, 0.08315233886241913) (0.017404740676283836, 0.08670161664485931) (0.008117301389575005, 0.08548855781555176) (0.01091801282018423, 0.08680928498506546) (0.011765765026211739, 0.0863567441701889) (0.013472368009388447, 0.08038166910409927) (0.018734697252511978, 0.08420374989509583)\n",
      "(0.00796539057046175, 0.08394001424312592) (0.015240572392940521, 0.0881103128194809) (0.01657947152853012, 0.09089823067188263) (0.009386113844811916, 0.08640962839126587) (0.006711956579238176, 0.08587407320737839) (0.016478974372148514, 0.0859234407544136) (0.009177399799227715, 0.08286982774734497) (0.006177034229040146, 0.08621367812156677) (0.016063513234257698, 0.0896887555718422) (0.013010799884796143, 0.09524793922901154)\n",
      "(0.004585853777825832, 0.08507951349020004) (0.00639338418841362, 0.0871373638510704) (0.019259022548794746, 0.09314101189374924) (0.015328891575336456, 0.0791357159614563) (0.015579862520098686, 0.0841798186302185) (0.019914641976356506, 0.09009916335344315) (0.006986473686993122, 0.08840261399745941) (0.018325962126255035, 0.08195514976978302) (0.014007601886987686, 0.08398356288671494) (0.018477484583854675, 0.08418435603380203)\n",
      "(0.010841459035873413, 0.08505306392908096) (0.020919429138302803, 0.08732078224420547) (0.005011740606278181, 0.09156511723995209) (0.0051145534962415695, 0.0786915123462677) (0.011684919707477093, 0.07988279312849045) (0.013033857569098473, 0.09567063301801682) (0.027087543159723282, 0.08902028948068619) (0.018733009696006775, 0.09466065466403961) (0.018855730071663857, 0.0948980301618576) (0.011555438861250877, 0.08329015225172043)\n",
      "(0.009729409590363503, 0.09831923246383667) (0.013583861291408539, 0.08523187041282654) (0.008743606507778168, 0.07911522686481476) (0.01775939390063286, 0.08935079723596573) (0.007563345599919558, 0.08336839824914932) (0.013558076694607735, 0.08443906903266907) (0.004044321831315756, 0.09632821381092072) (0.012235712260007858, 0.09239412844181061) (0.0038129400927573442, 0.07500158250331879) (0.01073681190609932, 0.09427385032176971)\n",
      "(0.004761449061334133, 0.08856121450662613) (0.014415576122701168, 0.08534877002239227) (0.010842956602573395, 0.08884205669164658) (0.016054941341280937, 0.08440543711185455) (0.012504514306783676, 0.09136341512203217) (0.01459026150405407, 0.09264139831066132) (0.01879817619919777, 0.09408697485923767) (0.010668477043509483, 0.08271556347608566) (0.010379759594798088, 0.09583409875631332) (0.019401922821998596, 0.0936327800154686)\n",
      "(0.018002262338995934, 0.08958365768194199) (0.008572325110435486, 0.09366628527641296) (0.014983193948864937, 0.08761647343635559) (0.014177381992340088, 0.09554824978113174) (0.017569448798894882, 0.08927540481090546) (0.008633911609649658, 0.08695713430643082) (0.012904328294098377, 0.09057304263114929) (0.010186564177274704, 0.0948345810174942) (0.019771631807088852, 0.0957891196012497) (0.013667871244251728, 0.08377627283334732)\n",
      "(0.018747370690107346, 0.08562130481004715) (0.01771060936152935, 0.09455220401287079) (0.0036050945054739714, 0.08694806694984436) (0.009595353156328201, 0.08278734236955643) (0.023800039663910866, 0.09507343918085098) (0.02070322073996067, 0.09558692574501038) (0.011243964545428753, 0.08890382945537567) (0.01540181040763855, 0.08972981572151184) (0.004428293090313673, 0.10079210251569748) (0.012302287854254246, 0.09022793173789978)\n",
      "(0.008092012256383896, 0.09436684101819992) (0.004317235201597214, 0.08713097870349884) (0.016602298244833946, 0.09051217883825302) (0.01910167746245861, 0.08707533776760101) (0.022866372019052505, 0.09381666779518127) (0.01255637314170599, 0.08779443055391312) (0.02200351096689701, 0.09314588457345963) (0.00979579333215952, 0.08992289006710052) (0.013810484670102596, 0.08816403150558472) (0.010138356126844883, 0.0886356458067894)\n",
      "(0.015618911944329739, 0.08549393713474274) (0.008118972182273865, 0.09319431334733963) (0.006723219528794289, 0.08943717926740646) (0.012610042467713356, 0.08683668076992035) (0.010790344327688217, 0.09201797097921371) (0.026020871475338936, 0.09403640031814575) (0.0102992607280612, 0.09068572521209717) (0.012890761718153954, 0.09594143182039261) (0.007610823027789593, 0.09514418989419937) (0.02096405252814293, 0.0884939432144165)\n",
      "(0.014524241909384727, 0.08202046155929565) (0.011075378395617008, 0.09519810229539871) (0.010950994677841663, 0.08986169844865799) (0.011994251050055027, 0.09425467252731323) (0.005084805190563202, 0.08944681286811829) (0.011278714053332806, 0.09682759642601013) (0.014483161270618439, 0.08703363686800003) (0.0030646363738924265, 0.09499502182006836) (0.011983700096607208, 0.08323817700147629) (0.018531829118728638, 0.08700547367334366)\n",
      "(0.011234988458454609, 0.09491954743862152) (0.008033400401473045, 0.09555687010288239) (0.01551486924290657, 0.09002115577459335) (0.014574341475963593, 0.08893096446990967) (0.014440440572798252, 0.09594742953777313) (0.01441690232604742, 0.08572997152805328) (0.013824502006173134, 0.09099831432104111) (0.012484380044043064, 0.0910254642367363) (0.015305707231163979, 0.09743804484605789) (0.013470202684402466, 0.09794151782989502)\n",
      "(0.009868673980236053, 0.08173561096191406) (0.015628844499588013, 0.08253270387649536) (0.009819312021136284, 0.09032142907381058) (0.008023110218346119, 0.07631616294384003) (0.0062727355398237705, 0.08015234023332596) (0.010333908721804619, 0.09653611481189728) (0.010540242306888103, 0.09464286267757416) (0.011894425377249718, 0.09460664540529251) (0.010898363776504993, 0.09616641700267792) (0.0019032284617424011, 0.09350451081991196)\n",
      "(0.012619600631296635, 0.08831215649843216) (0.025250989943742752, 0.08883629739284515) (0.008364755660295486, 0.085820272564888) (0.01663736253976822, 0.09569232165813446) (0.009286046028137207, 0.08669506758451462) (0.020032178610563278, 0.08799798786640167) (0.011598153971135616, 0.08608061075210571) (0.006619810126721859, 0.08795808255672455) (0.007462089415639639, 0.08991654962301254) (0.016989463940262794, 0.09098498523235321)\n",
      "(0.005467996466904879, 0.09470859915018082) (0.022594504058361053, 0.0892869159579277) (0.00530738802626729, 0.08114606887102127) (0.010459019802510738, 0.09203733503818512) (0.015492365695536137, 0.08882809430360794) (0.019899871200323105, 0.0914417952299118) (0.004822688642889261, 0.09270285069942474) (0.007287153508514166, 0.09167739003896713) (0.01358705386519432, 0.09412623941898346) (0.012503687292337418, 0.09698308259248734)\n",
      "(0.004806194920092821, 0.09155067801475525) (0.016593128442764282, 0.09174443036317825) (0.013018857687711716, 0.08327285945415497) (0.00997796282172203, 0.08329328894615173) (0.0030761705711483955, 0.09288661181926727) (0.01859213225543499, 0.08453585207462311) (0.00763513520359993, 0.08349797874689102) (0.009172838181257248, 0.08322750777006149) (0.019093722105026245, 0.09050155431032181) (0.014419804327189922, 0.09169050306081772)\n",
      "(0.007291440386325121, 0.09804323315620422) (0.015153300017118454, 0.08840340375900269) (0.012217272073030472, 0.08925288170576096) (0.008766749873757362, 0.08813010901212692) (0.01077738031744957, 0.09318424016237259) (0.016290966421365738, 0.08516739308834076) (0.013066214509308338, 0.08996004611253738) (0.015473569743335247, 0.09301634132862091) (0.010084892623126507, 0.08144323527812958) (0.011395828798413277, 0.09560082107782364)\n",
      "(0.009805124253034592, 0.08841129392385483) (0.0105497557669878, 0.08245671540498734) (0.011985382996499538, 0.08419875800609589) (0.004072356037795544, 0.08623623847961426) (0.008643180131912231, 0.08437511324882507) (0.007872121408581734, 0.09796184301376343) (0.014179371297359467, 0.08769410848617554) (0.009910250082612038, 0.09382792562246323) (0.012415344826877117, 0.09460694342851639) (0.00605780677869916, 0.08740854263305664)\n",
      "(0.014124326407909393, 0.08584520220756531) (0.009114488027989864, 0.0906776413321495) (0.006532940082252026, 0.09141389280557632) (0.019195236265659332, 0.0877789705991745) (0.01181219331920147, 0.084842748939991) (0.00756294559687376, 0.09148356318473816) (0.012502355501055717, 0.08911135047674179) (0.013222984969615936, 0.08967441320419312) (0.010428639128804207, 0.09439964592456818) (0.01357100997120142, 0.09153491258621216)\n",
      "(0.0037782355211675167, 0.09233623743057251) (0.013343075290322304, 0.08539644628763199) (0.007054027169942856, 0.09207946062088013) (0.012845966964960098, 0.08433549851179123) (0.01109321042895317, 0.09222029894590378) (0.0056713661178946495, 0.09300829470157623) (0.013720137998461723, 0.09014062583446503) (0.016485681757330894, 0.08971711248159409) (0.00830860249698162, 0.08769530802965164) (0.014203434810042381, 0.08369612693786621)\n",
      "(0.00456642173230648, 0.09575410187244415) (0.015685366466641426, 0.08858847618103027) (0.008610626682639122, 0.08465786278247833) (0.011659733019769192, 0.09516841173171997) (0.012531153857707977, 0.08411617577075958) (0.010996561497449875, 0.082619309425354) (0.00640754122287035, 0.09386493265628815) (0.020267784595489502, 0.08561897277832031) (0.004520862363278866, 0.08201101422309875) (0.005319410003721714, 0.09046053886413574)\n",
      "(0.009815026074647903, 0.08871257305145264) (0.004368488676846027, 0.09280030429363251) (0.011420808732509613, 0.09400784969329834) (0.01032131165266037, 0.08513116836547852) (0.014278795570135117, 0.0975809246301651) (0.01181410625576973, 0.0938183143734932) (0.005996903870254755, 0.09237554669380188) (0.007051956374198198, 0.09337780624628067) (0.007685465272516012, 0.09080955386161804) (0.01588103175163269, 0.08497990667819977)\n",
      "(0.008401630446314812, 0.08686676621437073) (0.011335483752191067, 0.08804433047771454) (0.00537800882011652, 0.08647629618644714) (0.0035965917631983757, 0.09211252629756927) (0.00600624643266201, 0.08929771929979324) (0.007810657378286123, 0.10075458139181137) (0.00525896018370986, 0.09211137890815735) (0.000686870829667896, 0.09258906543254852) (0.003774690441787243, 0.08683116734027863) (0.00232979585416615, 0.08903229236602783)\n",
      "(0.007656657136976719, 0.08983396738767624) (0.010571219027042389, 0.09262489527463913) (0.007570826914161444, 0.0944652110338211) (0.005400864407420158, 0.09341384470462799) (0.015256043523550034, 0.08869127929210663) (0.021694274619221687, 0.09610016644001007) (0.01296529546380043, 0.09350772202014923) (0.01749286986887455, 0.08950003981590271) (0.01723683625459671, 0.09261380136013031) (0.013064045459032059, 0.08871959149837494)\n",
      "(0.01030812505632639, 0.08791912347078323) (0.010205695405602455, 0.08368658274412155) (0.017151547595858574, 0.08933764696121216) (0.013959506526589394, 0.09240943193435669) (0.01320914551615715, 0.08401869982481003) (0.0025862972252070904, 0.09410588443279266) (0.00379156693816185, 0.08741237968206406) (0.013607095927000046, 0.08954647183418274) (0.009634417481720448, 0.08189164102077484) (0.014981217682361603, 0.08944950252771378)\n",
      "(0.008195716887712479, 0.07936528325080872) (0.009794513694941998, 0.08445480465888977) (0.010966118425130844, 0.08630567789077759) (0.014723120257258415, 0.08577536046504974) (0.008265450596809387, 0.08751007169485092) (0.013320856727659702, 0.08769463002681732) (0.012520438060164452, 0.09514419734477997) (0.014891858212649822, 0.08763322979211807) (0.002953782444819808, 0.09584838896989822) (0.006105381064116955, 0.08470616489648819)\n",
      "(0.009494688361883163, 0.08607496321201324) (0.010581882670521736, 0.09639929980039597) (0.008894057944417, 0.09060560166835785) (0.013383457437157631, 0.09699445962905884) (0.011347613297402859, 0.08687485754489899) (0.004502785392105579, 0.08561746031045914) (0.014335274696350098, 0.0892520621418953) (0.001976965693756938, 0.09474070370197296) (0.016146507114171982, 0.08945219963788986) (0.005204259417951107, 0.0925731360912323)\n",
      "(0.006833834573626518, 0.0908818393945694) (0.008395501412451267, 0.09542223066091537) (0.01811172254383564, 0.08542944490909576) (0.0077281370759010315, 0.08580069243907928) (0.00705094076693058, 0.09219126403331757) (0.004885022062808275, 0.08757532387971878) (0.0041077518835663795, 0.09866863489151001) (0.0048829298466444016, 0.08815427124500275) (0.004109707195311785, 0.08638221025466919) (0.011478705331683159, 0.08894338458776474)\n",
      "(0.007568232249468565, 0.0865195095539093) (0.007992114871740341, 0.08163449168205261) (0.008275900967419147, 0.0841168686747551) (0.004317348822951317, 0.08456378430128098) (0.007116048131138086, 0.08830712735652924) (0.005242706276476383, 0.07914713770151138) (0.006867819465696812, 0.08386413753032684) (0.009604588150978088, 0.07837112247943878) (0.01042578648775816, 0.08895426988601685) (0.01197889819741249, 0.08494235575199127)\n",
      "(0.005974982865154743, 0.07289374619722366) (0.002030646661296487, 0.08665220439434052) (0.010105373337864876, 0.08354661613702774) (0.012488240376114845, 0.08934840559959412) (0.014531686902046204, 0.08982904255390167) (0.011282894760370255, 0.0938410684466362) (0.008837269619107246, 0.08006581664085388) (0.012250007130205631, 0.08019764721393585) (0.009988745674490929, 0.08585094660520554) (0.01165817491710186, 0.09646954387426376)\n",
      "(0.008044294081628323, 0.07769471406936646) (0.015846701338887215, 0.08087100088596344) (0.010756558738648891, 0.08642704784870148) (0.00733941700309515, 0.09317756444215775) (0.014588676393032074, 0.08632956445217133) (0.006960732862353325, 0.08958035707473755) (0.0045210132375359535, 0.08715717494487762) (0.007989617995917797, 0.08759965002536774) (0.0018339627422392368, 0.09211525321006775) (0.005739254876971245, 0.08630306273698807)\n",
      "(0.010727357119321823, 0.08973558992147446) (0.011589049361646175, 0.08367723226547241) (0.006266810465604067, 0.09118695557117462) (0.0156993567943573, 0.08861114084720612) (0.007129493169486523, 0.08771595358848572) (0.012723550200462341, 0.08242709934711456) (0.006895810831338167, 0.08896960318088531) (0.007633592002093792, 0.08938106894493103) (0.005576879717409611, 0.0960272029042244) (0.007683916017413139, 0.0845281183719635)\n",
      "(0.009232910349965096, 0.09415961056947708) (0.010843805968761444, 0.08158893883228302) (0.0062626563012599945, 0.08939222991466522) (0.006535165943205357, 0.09180942177772522) (0.012924996204674244, 0.0930488109588623) (0.016838638111948967, 0.08372025936841965) (0.005451858509331942, 0.09136883169412613) (0.005129911936819553, 0.07968592643737793) (0.016264159232378006, 0.09259076416492462) (0.003814060939475894, 0.08699110895395279)\n",
      "(0.0021647769026458263, 0.09649236500263214) (0.005585885606706142, 0.09953729808330536) (0.005201959051191807, 0.0911700502038002) (0.003984331153333187, 0.09058912098407745) (0.0085983294993639, 0.08034489303827286) (0.011119145900011063, 0.08765494078397751) (0.007817115634679794, 0.08865606784820557) (0.006957569625228643, 0.09287098795175552) (0.010703225620090961, 0.09751268476247787) (0.011208931915462017, 0.09533203393220901)\n",
      "(0.006479495670646429, 0.09351996332406998) (0.005811498500406742, 0.09116895496845245) (0.008107676170766354, 0.07578733563423157) (0.005044352728873491, 0.09399645030498505) (0.008949300274252892, 0.08206984400749207) (0.007684002164751291, 0.08614940941333771) (0.0034573960583657026, 0.07627515494823456) (0.0063780914060771465, 0.07846899330615997) (0.010954350233078003, 0.09646781533956528) (0.0021113429684191942, 0.09653954207897186)\n",
      "(0.002191956387832761, 0.09678836911916733) (0.003835405455902219, 0.08182717114686966) (0.012860409915447235, 0.08694605529308319) (0.003686802927404642, 0.09228290617465973) (0.005911390762776136, 0.08633379638195038) (0.00480684032663703, 0.09199105203151703) (0.006403839681297541, 0.08914197981357574) (0.011036030948162079, 0.07948870211839676) (0.0047202808782458305, 0.0843464806675911) (0.003172054886817932, 0.095677949488163)\n",
      "(0.005540712736546993, 0.09292077273130417) (0.011082439683377743, 0.07785611599683762) (0.009316534735262394, 0.08961565792560577) (0.008367829024791718, 0.08103260397911072) (0.015637315809726715, 0.08138319104909897) (0.004064962267875671, 0.08852166682481766) (0.015271453186869621, 0.08696762472391129) (0.003349942620843649, 0.08833096921443939) (0.01168215274810791, 0.09130364656448364) (0.009663667529821396, 0.08893708139657974)\n",
      "(0.009806069545447826, 0.08983659744262695) (0.0015116637805476785, 0.0901852548122406) (0.009983126074075699, 0.09058623015880585) (0.013254685327410698, 0.0892641693353653) (0.00974560808390379, 0.07982183247804642) (0.005166927352547646, 0.08015404641628265) (0.0036797448992729187, 0.0886879563331604) (0.014661191031336784, 0.08068704605102539) (0.01019004825502634, 0.0774565115571022) (0.013539159670472145, 0.0929669663310051)\n",
      "(0.016722869127988815, 0.09232068061828613) (0.004403449594974518, 0.09312069416046143) (0.006105228792876005, 0.08259975910186768) (0.014927818439900875, 0.08310356736183167) (0.01513619627803564, 0.0904618352651596) (0.01522389892488718, 0.08886749297380447) (0.007598553318530321, 0.07349077612161636) (0.008253642357885838, 0.07718469947576523) (0.006473181769251823, 0.08053160458803177) (0.004189224913716316, 0.06844191998243332)\n",
      "(0.004919766914099455, 0.08429227024316788) (0.02393817901611328, 0.08340850472450256) (0.003719818079844117, 0.08066260069608688) (0.00800718367099762, 0.08766540884971619) (0.005047286860644817, 0.08528152108192444) (0.012646671384572983, 0.08330635726451874) (0.008491043001413345, 0.07600848376750946) (0.003921967931091785, 0.08634059131145477) (0.01288975216448307, 0.07856561243534088) (0.011106479912996292, 0.09081664681434631)\n",
      "(0.005056110210716724, 0.08313526213169098) (0.009916907176375389, 0.10006991773843765) (0.007042715325951576, 0.07420234382152557) (0.012099611572921276, 0.10304203629493713)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print_listlike([*zip([max(each) for each in mi_original['binary_label_mi']], [max(each) for each in mi_transpose['binary_label_mi']])])\n",
    "print_listlike([*zip(mi_original['arc_mi'], mi_transpose['arc_mi'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008965459026784438\n",
      "0.08830101225334051\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def get_weighted_average_mi(num_samples_for_each_label, binary_mi):\n",
    "    total_weighted = 0\n",
    "    for num_samples_for_label, binary_mi_for_label in zip(num_samples_for_each_label, binary_mi):\n",
    "        total_weighted += (num_samples_for_label / sum(num_samples_for_each_label)) * (sum(binary_mi_for_label) / len(binary_mi_for_label))\n",
    "    \n",
    "    return total_weighted\n",
    "\n",
    "label_num_samples = [v for k, v in result_original['label_totals'].items() if v != 0]\n",
    "# print(len(mi_original['binary_label_mi']), len(label_num_samples))\n",
    "# print(label_num_samples)\n",
    "num_samples_for_each_label = [each.shape[0] for each in label_features if each.shape[0]] + [n_neg_samples]\n",
    "\n",
    "print(get_weighted_average_mi(num_samples_for_each_label, mi_original['binary_label_mi']))\n",
    "print(get_weighted_average_mi(num_samples_for_each_label, mi_transpose['binary_label_mi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s>', 0, 1700, 0.0)\n",
      "('num', 1, 1087, 0.09)\n",
      "('dep', 3, 756, 0.4)\n",
      "('cop', 2, 359, 0.56)\n",
      "('dobj', 14, 1620, 0.86)\n",
      "('mark', 4, 422, 0.95)\n",
      "('number', 8, 493, 1.62)\n",
      "('poss', 29, 706, 4.11)\n",
      "('advmod', 94, 1257, 7.48)\n",
      "('amod', 316, 2477, 12.76)\n",
      "('aux', 240, 1243, 19.31)\n",
      "('cc', 314, 1001, 31.37)\n",
      "('possessive', 140, 434, 32.26)\n",
      "('xcomp', 142, 434, 32.72)\n",
      "('conj', 572, 1004, 56.97)\n",
      "('ccomp', 380, 560, 67.86)\n",
      "('det', 2283, 3341, 68.33)\n",
      "('prep', 3129, 3783, 82.71)\n",
      "('nn', 2726, 3240, 84.14)\n",
      "('nsubj', 2438, 2836, 85.97)\n",
      "('punct', 4397, 4731, 92.94)\n",
      "('pobj', 3563, 3744, 95.17)\n",
      "('root', 1700, 1700, 100.0)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "inverse_labelmap = {v: k for k, v in dep_parser_llama.labelmap.items()}\n",
    "label_entries = []\n",
    "for label_idx in results['label_corrects']:\n",
    "    if label_idx in inverse_labelmap:\n",
    "        if results['label_totals'][label_idx] == 0:\n",
    "            label_entries.append((inverse_labelmap[label_idx], results['label_corrects'][label_idx], results['label_totals'][label_idx], 0))\n",
    "        else:\n",
    "            label_entries.append((inverse_labelmap[label_idx], results['label_corrects'][label_idx], results['label_totals'][label_idx], round(results['label_corrects'][label_idx] / results['label_totals'][label_idx] * 100, 2)))\n",
    "\n",
    "large_quantity_label_entries = sorted(label_entries, key=lambda x: x[-2], reverse=True)[:len(label_entries) // 2]\n",
    "print(*sorted(large_quantity_label_entries, key=lambda x: x[-1]), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with open('./label_entries.json', 'w') as f:\n",
    "    json.dump(label_entries, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 47)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "len(unmasked_label_names), len(dep_parser_llama.labelmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1024(A), 2(A), [168](T)]\n",
      "[1024(A), 46(A), [168](T)]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(get_shape([\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mzip\u001b[39m(conditional_probab_pos, conditional_probab_neg)]))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(get_shape(conditional_probab_label))\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(get_shape([*zip(conditional_probab_pos, conditional_probab_neg)]))\n",
    "print(get_shape(conditional_probab_label))\n",
    "1 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(torch.arange(-3.8, 2.9, 0.05), pos_kde_estims[1].cpu())\n",
    "plt.plot(torch.arange(-3.8, 2.9, 0.05), neg_kde_estims[1].cpu())\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2[Optional]. Calculate Corr Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m attn_feature_cov \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcov(arc_attn_features\u001b[39m.\u001b[39;49mT)\n",
      "\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'T'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "attn_feature_cov = torch.cov(arc_attn_features.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# print(arc_attn_features.shape)\n",
    "attn_feature_cov_naiive = torch.matmul(arc_attn_features.T, arc_attn_features) / arc_attn_features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc8940eebe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(32, 32))\n",
    "plt.imshow(attn_feature_cov_naiive.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3[Optional]. Estimate spearsman-corr of each dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "for i in trange(len(arc_feature_with_y)):\n",
    "    pkl.dump(arc_feature_with_y, open(f'/tmp/pickles/arc_feature_with_y_dim{i}.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def estimate_mi_for_dimension(features2y: List[Tensor]):\n",
    "    pass\n",
    "\n",
    "def estimate_spearmanr_for_dimension(features2y: List[Tensor], num_features: int = -1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        features2y: List[Tensor[total_num_features, num_rel_categories] ...(total `featurn_dim_size` Tensors)]\n",
    "        num_features: the first n features to process\n",
    "    returns:\n",
    "        corrs: List[float], list of spearman relations\n",
    "        pvalues: List[float], list of p-values on null hypothesis of `feature_i and y has no relationship`\n",
    "    \"\"\"\n",
    "    if num_features == -1:\n",
    "        num_features = len(features2y)\n",
    "    corrs, pvalues = [], []\n",
    "    for feature_idx, (feature_values, ys) in enumerate(tqdm(features2y, desc='calculating for each dimension...')):\n",
    "        # print(f\"feature_idx: {feature_idx}, spearman_r: {spearmanr(feature_values.numpy(), ys.numpy())}\")\n",
    "        corr, pvalue = spearmanr(feature_values.numpy(), ys.numpy())\n",
    "        corrs.append(corr), pvalues.append(pvalue)\n",
    "\n",
    "    # results = Pool(processes=20).imap(spearmanr, features2y)\n",
    "    # corrs, pvalues = zip(*results)\n",
    "    \n",
    "    return corrs, pvalues\n",
    "\n",
    "corrs, pvalues = estimate_spearmanr_for_dimension(arc_feature_with_y, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "max_idx = np.argmax(np.abs(corrs))\n",
    "def plot_single_head(arc_feature_with_y):\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    plt.scatter(*arc_feature_with_y, s=0.2)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "plot_single_head(arc_feature_with_y[max_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(spearmanr(arc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.bar([*range(len(corrs))], sorted(corrs))\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6917530078326845e-25, 4.324582566083494e-256, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(sorted(pvalues[:10], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.bar([*range(len(pvalues))], sorted(pvalues))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcbc41c6c40>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot([*range(len(corrs))], corrs)\n",
    "# print(corrs, pvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import dblquad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhx-OpenPrompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
